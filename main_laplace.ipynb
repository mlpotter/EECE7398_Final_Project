{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:37.188521Z",
     "start_time": "2023-11-07T20:21:28.626293Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.models import Exponential_Model\n",
    "from src.criterion import right_censored,RightCensorWrapper\n",
    "from src.load_data import load_datasets,load_dataframe\n",
    "from src.utils import train_robust,lower_bound\n",
    "from src.visualizations import visualize_population_curves_attacked,visualize_individual_curves_attacked\n",
    "\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "from auto_LiRPA import BoundedModule, BoundedTensor\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS(object):\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ARGS()\n",
    "args.verify=False\n",
    "args.device=\"cpu\"\n",
    "\n",
    "args.seed = 123\n",
    "\n",
    "args.eps=0.5\n",
    "args.norm=np.inf\n",
    "args.bound_type = \"CROWN-IBP\"\n",
    "args.num_epochs=50\n",
    "args.lr = 1e-3\n",
    "args.batch_size= 128\n",
    "args.scheduler_name = \"SmoothedScheduler\"\n",
    "args.scheduler_opts = \"start=10,length=20\"\n",
    "args.hidden_dims = [15,15]\n",
    "args.save_model = \"\"\n",
    "args.dataset = \"TRACE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:37.272860Z",
     "start_time": "2023-11-07T20:21:37.188521Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# GOOD DATASETS\n",
    "# 1. TRACE\n",
    "# 2. divorce \n",
    "# 3. Dialysis\n",
    "dataset_train,dataset_test = load_datasets(args.dataset,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:37.295094Z",
     "start_time": "2023-11-07T20:21:37.272860Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input_dims = dataset_train.tensors[0].shape[1]\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:37.321189Z",
     "start_time": "2023-11-07T20:21:37.297057Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1502, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_train = DataLoader(dataset_train,batch_size=args.batch_size,shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test,batch_size=args.batch_size,shuffle=False)\n",
    "\n",
    "dataloader_train.mean = dataloader_test.mean = dataset_train.mean\n",
    "dataloader_train.std = dataloader_test.std = dataset_train.std\n",
    "\n",
    "\n",
    "dataset_train.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:37.986364Z",
     "start_time": "2023-11-07T20:21:37.318893Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "clf_robust = Exponential_Model(input_dim=input_dims,hidden_layers=args.hidden_dims)\n",
    "clf_fragile = Exponential_Model(input_dim=input_dims,hidden_layers=args.hidden_dims)\n",
    "clf_fragile.load_state_dict(deepcopy(clf_robust.state_dict()))\n",
    "\n",
    "\n",
    "# model = BoundedModule(clf, X_train)\n",
    "model_robust_wrap = BoundedModule(RightCensorWrapper(clf_robust),dataset_train.tensors)\n",
    "model_fragile_wrap = BoundedModule(RightCensorWrapper(clf_fragile),dataset_train.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpott\\anaconda3\\envs\\survival\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, learning rate [0.001]\n",
      "[ 1:   0]: eps=0.00000000 Loss=665.7952 Time=0.0090\n",
      "[ 1:  10]: eps=0.00000000 Loss=624.1345 Time=0.0087\n",
      "[ 1:  11]: eps=0.00000000 Loss=613.9793 Time=0.0086\n",
      "Epoch time: 0.1544, Total time: 0.1544\n",
      "Evaluating...\n",
      "[ 1:   2]: eps=0.00000000 Loss=576.3916 Time=0.0040\n",
      "Epoch 2, learning rate [0.001]\n",
      "[ 2:   0]: eps=0.00000000 Loss=657.6672 Time=0.0060\n",
      "[ 2:  10]: eps=0.00000000 Loss=581.3987 Time=0.0056\n",
      "[ 2:  11]: eps=0.00000000 Loss=573.6357 Time=0.0056\n",
      "Epoch time: 0.0916, Total time: 0.2461\n",
      "Evaluating...\n",
      "[ 2:   2]: eps=0.00000000 Loss=538.8378 Time=0.0030\n",
      "Epoch 3, learning rate [0.001]\n",
      "[ 3:   0]: eps=0.00000000 Loss=588.1846 Time=0.0050\n",
      "[ 3:  10]: eps=0.00000000 Loss=545.3104 Time=0.0052\n",
      "[ 3:  11]: eps=0.00000000 Loss=535.4977 Time=0.0051\n",
      "Epoch time: 0.0846, Total time: 0.3307\n",
      "Evaluating...\n",
      "[ 3:   2]: eps=0.00000000 Loss=499.1743 Time=0.0033\n",
      "Epoch 4, learning rate [0.001]\n",
      "[ 4:   0]: eps=0.00000000 Loss=463.7708 Time=0.0050\n",
      "[ 4:  10]: eps=0.00000000 Loss=502.2614 Time=0.0053\n",
      "[ 4:  11]: eps=0.00000000 Loss=492.7763 Time=0.0053\n",
      "Epoch time: 0.0877, Total time: 0.4184\n",
      "Evaluating...\n",
      "[ 4:   2]: eps=0.00000000 Loss=455.9896 Time=0.0037\n",
      "Epoch 5, learning rate [0.001]\n",
      "[ 5:   0]: eps=0.00000000 Loss=482.2530 Time=0.0060\n",
      "[ 5:  10]: eps=0.00000000 Loss=457.6184 Time=0.0056\n",
      "[ 5:  11]: eps=0.00000000 Loss=447.5899 Time=0.0056\n",
      "Epoch time: 0.0867, Total time: 0.5051\n",
      "Evaluating...\n",
      "[ 5:   2]: eps=0.00000000 Loss=408.2640 Time=0.0033\n",
      "Epoch 6, learning rate [0.001]\n",
      "[ 6:   0]: eps=0.00000000 Loss=448.6642 Time=0.0060\n",
      "[ 6:  10]: eps=0.00000000 Loss=404.8912 Time=0.0064\n",
      "[ 6:  11]: eps=0.00000000 Loss=396.1009 Time=0.0063\n",
      "Epoch time: 0.0987, Total time: 0.6039\n",
      "Evaluating...\n",
      "[ 6:   2]: eps=0.00000000 Loss=355.9036 Time=0.0037\n",
      "Epoch 7, learning rate [0.001]\n",
      "[ 7:   0]: eps=0.00000000 Loss=350.2580 Time=0.0050\n",
      "[ 7:  10]: eps=0.00000000 Loss=348.7209 Time=0.0052\n",
      "[ 7:  11]: eps=0.00000000 Loss=341.9142 Time=0.0051\n",
      "Epoch time: 0.0837, Total time: 0.6875\n",
      "Evaluating...\n",
      "[ 7:   2]: eps=0.00000000 Loss=305.2736 Time=0.0033\n",
      "Epoch 8, learning rate [0.001]\n",
      "[ 8:   0]: eps=0.00000000 Loss=313.5024 Time=0.0050\n",
      "[ 8:  10]: eps=0.00000000 Loss=300.4591 Time=0.0054\n",
      "[ 8:  11]: eps=0.00000000 Loss=293.8539 Time=0.0054\n",
      "Epoch time: 0.0847, Total time: 0.7722\n",
      "Evaluating...\n",
      "[ 8:   2]: eps=0.00000000 Loss=262.7550 Time=0.0043\n",
      "Epoch 9, learning rate [0.001]\n",
      "[ 9:   0]: eps=0.00000000 Loss=278.7311 Time=0.0050\n",
      "[ 9:  10]: eps=0.00000000 Loss=260.1719 Time=0.0053\n",
      "[ 9:  11]: eps=0.00000000 Loss=254.3557 Time=0.0052\n",
      "Epoch time: 0.0836, Total time: 0.8558\n",
      "Evaluating...\n",
      "[ 9:   2]: eps=0.00000000 Loss=229.1178 Time=0.0034\n",
      "Epoch 10, learning rate [0.001]\n",
      "[10:   0]: eps=0.00000000 Loss=234.9610 Time=0.0050\n",
      "[10:  10]: eps=0.00002968 Loss=229.4129 Time=0.1090 Robust_Loss=228.8589\n",
      "[10:  11]: eps=0.00004345 Loss=224.8443 Time=0.1090 Robust_Loss=223.9029\n",
      "Epoch time: 1.3321, Total time: 2.1879\n",
      "Evaluating...\n",
      "[10:   2]: eps=0.00004345 Loss=207.9192 Robust_Loss=207.9241 Time=0.0598\n",
      "Epoch 11, learning rate [0.001]\n",
      "[11:   0]: eps=0.00006154 Loss=218.0496 Robust_Loss=218.0569 Time=0.1158\n",
      "[11:  10]: eps=0.00069520 Loss=211.6410 Robust_Loss=211.6736 Time=0.1372\n",
      "[11:  11]: eps=0.00083049 Loss=207.4225 Robust_Loss=207.4576 Time=0.1344\n",
      "Epoch time: 1.6620, Total time: 3.8499\n",
      "Evaluating...\n",
      "[11:   2]: eps=0.00083049 Loss=195.9675 Robust_Loss=196.0612 Time=0.0615\n",
      "Epoch 12, learning rate [0.001]\n",
      "[12:   0]: eps=0.00098462 Loss=196.9523 Robust_Loss=197.0665 Time=0.1186\n",
      "[12:  10]: eps=0.00396586 Loss=200.2836 Robust_Loss=200.5331 Time=0.1268\n",
      "[12:  11]: eps=0.00445342 Loss=197.0968 Robust_Loss=197.3544 Time=0.1266\n",
      "Epoch time: 1.5453, Total time: 5.3952\n",
      "Evaluating...\n",
      "[12:   2]: eps=0.00445342 Loss=189.9265 Robust_Loss=190.4354 Time=0.0665\n",
      "Epoch 13, learning rate [0.001]\n",
      "[13:   0]: eps=0.00498462 Loss=191.9706 Robust_Loss=192.5581 Time=0.1934\n",
      "[13:  10]: eps=0.01328780 Loss=195.6244 Robust_Loss=196.6077 Time=0.1130\n",
      "[13:  11]: eps=0.01448148 Loss=192.5024 Robust_Loss=193.4955 Time=0.1129\n",
      "Epoch time: 1.3805, Total time: 6.7757\n",
      "Evaluating...\n",
      "[13:   2]: eps=0.01448148 Loss=187.4709 Robust_Loss=189.1735 Time=0.0564\n",
      "Epoch 14, learning rate [0.001]\n",
      "[14:   0]: eps=0.01575385 Loss=195.9104 Robust_Loss=197.4943 Time=0.1107\n",
      "[14:  10]: eps=0.03358409 Loss=192.8415 Robust_Loss=195.6652 Time=0.1057\n",
      "[14:  11]: eps=0.03596083 Loss=190.8434 Robust_Loss=193.7101 Time=0.1051\n",
      "Epoch time: 1.2858, Total time: 8.0615\n",
      "Evaluating...\n",
      "[14:   2]: eps=0.03596083 Loss=188.1185 Robust_Loss=192.6293 Time=0.0738\n",
      "Epoch 15, learning rate [0.001]\n",
      "[15:   0]: eps=0.03846154 Loss=204.5997 Robust_Loss=209.1354 Time=0.1147\n",
      "[15:  10]: eps=0.06410256 Loss=197.0491 Robust_Loss=203.6592 Time=0.1076\n",
      "[15:  11]: eps=0.06666667 Loss=193.1615 Robust_Loss=199.7254 Time=0.1075\n",
      "Epoch time: 1.3133, Total time: 9.3748\n",
      "Evaluating...\n",
      "[15:   2]: eps=0.06666667 Loss=191.4100 Robust_Loss=200.5523 Time=0.0532\n",
      "Epoch 16, learning rate [0.001]\n",
      "[16:   0]: eps=0.06923077 Loss=197.8132 Robust_Loss=207.3446 Time=0.1038\n",
      "[16:  10]: eps=0.09487179 Loss=198.5996 Robust_Loss=210.0750 Time=0.1037\n",
      "[16:  11]: eps=0.09743590 Loss=196.2740 Robust_Loss=207.7265 Time=0.1033\n",
      "Epoch time: 1.2646, Total time: 10.6394\n",
      "Evaluating...\n",
      "[16:   2]: eps=0.09743590 Loss=195.9776 Robust_Loss=210.6994 Time=0.0545\n",
      "Epoch 17, learning rate [0.001]\n",
      "[17:   0]: eps=0.10000000 Loss=173.9355 Robust_Loss=188.0440 Time=0.0998\n",
      "[17:  10]: eps=0.12564103 Loss=204.6433 Robust_Loss=222.0078 Time=0.1118\n",
      "[17:  11]: eps=0.12820513 Loss=201.4162 Robust_Loss=218.6498 Time=0.1111\n",
      "Epoch time: 1.3569, Total time: 11.9963\n",
      "Evaluating...\n",
      "[17:   2]: eps=0.12820513 Loss=201.9682 Robust_Loss=223.4260 Time=0.0558\n",
      "Epoch 18, learning rate [0.001]\n",
      "[18:   0]: eps=0.13076923 Loss=207.0933 Robust_Loss=228.3961 Time=0.1119\n",
      "[18:  10]: eps=0.15641026 Loss=212.1246 Robust_Loss=236.6880 Time=0.1170\n",
      "[18:  11]: eps=0.15897436 Loss=207.9825 Robust_Loss=232.2282 Time=0.1175\n",
      "Epoch time: 1.4342, Total time: 13.4305\n",
      "Evaluating...\n",
      "[18:   2]: eps=0.15897436 Loss=209.4787 Robust_Loss=239.0403 Time=0.0546\n",
      "Epoch 19, learning rate [0.001]\n",
      "[19:   0]: eps=0.16153846 Loss=212.3797 Robust_Loss=240.0312 Time=0.1505\n",
      "[19:  10]: eps=0.18717949 Loss=218.6815 Robust_Loss=251.7723 Time=0.1256\n",
      "[19:  11]: eps=0.18974359 Loss=215.6519 Robust_Loss=248.4852 Time=0.1280\n",
      "Epoch time: 1.5584, Total time: 14.9889\n",
      "Evaluating...\n",
      "[19:   2]: eps=0.18974359 Loss=219.1913 Robust_Loss=258.8869 Time=0.0764\n",
      "Epoch 20, learning rate [0.001]\n",
      "[20:   0]: eps=0.19230769 Loss=227.3870 Robust_Loss=266.6539 Time=0.1277\n",
      "[20:  10]: eps=0.21794872 Loss=230.1131 Robust_Loss=274.2903 Time=0.1223\n",
      "[20:  11]: eps=0.22051282 Loss=226.1622 Robust_Loss=269.7223 Time=0.1223\n",
      "Epoch time: 1.4903, Total time: 16.4792\n",
      "Evaluating...\n",
      "[20:   2]: eps=0.22051282 Loss=231.6748 Robust_Loss=284.1686 Time=0.0580\n",
      "Epoch 21, learning rate [0.001]\n",
      "[21:   0]: eps=0.22307692 Loss=229.3859 Robust_Loss=276.9427 Time=0.1167\n",
      "[21:  10]: eps=0.24871795 Loss=242.7462 Robust_Loss=301.1358 Time=0.1157\n",
      "[21:  11]: eps=0.25128205 Loss=240.0755 Robust_Loss=298.0976 Time=0.1153\n",
      "Epoch time: 1.4077, Total time: 17.8869\n",
      "Evaluating...\n",
      "[21:   2]: eps=0.25128205 Loss=248.8247 Robust_Loss=318.6811 Time=0.0714\n",
      "Epoch 22, learning rate [0.001]\n",
      "[22:   0]: eps=0.25384615 Loss=276.1275 Robust_Loss=351.1379 Time=0.1774\n",
      "[22:  10]: eps=0.27948718 Loss=261.3902 Robust_Loss=337.9140 Time=0.1345\n",
      "[22:  11]: eps=0.28205128 Loss=257.7434 Robust_Loss=333.2360 Time=0.1323\n",
      "Epoch time: 1.6321, Total time: 19.5190\n",
      "Evaluating...\n",
      "[22:   2]: eps=0.28205128 Loss=263.6975 Robust_Loss=347.9697 Time=0.0578\n",
      "Epoch 23, learning rate [0.001]\n",
      "[23:   0]: eps=0.28461538 Loss=238.8918 Robust_Loss=313.3169 Time=0.1036\n",
      "[23:  10]: eps=0.31025641 Loss=272.5282 Robust_Loss=359.0912 Time=0.1228\n",
      "[23:  11]: eps=0.31282051 Loss=268.4371 Robust_Loss=353.6695 Time=0.1294\n",
      "Epoch time: 1.5790, Total time: 21.0981\n",
      "Evaluating...\n",
      "[23:   2]: eps=0.31282051 Loss=267.2640 Robust_Loss=353.2931 Time=0.0830\n",
      "Epoch 24, learning rate [0.001]\n",
      "[24:   0]: eps=0.31538462 Loss=278.8282 Robust_Loss=365.9569 Time=0.1467\n",
      "[24:  10]: eps=0.34102564 Loss=273.9026 Robust_Loss=359.9021 Time=0.1191\n",
      "[24:  11]: eps=0.34358974 Loss=269.9032 Robust_Loss=354.6824 Time=0.1204\n",
      "Epoch time: 1.4708, Total time: 22.5688\n",
      "Evaluating...\n",
      "[24:   2]: eps=0.34358974 Loss=271.4779 Robust_Loss=359.5290 Time=0.0573\n",
      "Epoch 25, learning rate [0.001]\n",
      "[25:   0]: eps=0.34615385 Loss=240.2419 Robust_Loss=316.5768 Time=0.1127\n",
      "[25:  10]: eps=0.37179487 Loss=278.7400 Robust_Loss=368.3520 Time=0.1777\n",
      "[25:  11]: eps=0.37435897 Loss=275.2033 Robust_Loss=363.7311 Time=0.1770\n",
      "Epoch time: 2.1576, Total time: 24.7264\n",
      "Evaluating...\n",
      "[25:   2]: eps=0.37435897 Loss=277.4101 Robust_Loss=369.6219 Time=0.0603\n",
      "Epoch 26, learning rate [0.001]\n",
      "[26:   0]: eps=0.37692308 Loss=270.1275 Robust_Loss=358.4465 Time=0.1603\n",
      "[26:  10]: eps=0.40256410 Loss=285.3460 Robust_Loss=378.7319 Time=0.1486\n",
      "[26:  11]: eps=0.40512821 Loss=280.3843 Robust_Loss=372.1738 Time=0.1485\n",
      "Epoch time: 1.8097, Total time: 26.5361\n",
      "Evaluating...\n",
      "[26:   2]: eps=0.40512821 Loss=282.4563 Robust_Loss=377.8949 Time=0.0781\n",
      "Epoch 27, learning rate [0.001]\n",
      "[27:   0]: eps=0.40769231 Loss=290.6377 Robust_Loss=388.1646 Time=0.1507\n",
      "[27:  10]: eps=0.43333333 Loss=290.9920 Robust_Loss=388.2337 Time=0.1761\n",
      "[27:  11]: eps=0.43589744 Loss=285.8810 Robust_Loss=381.7999 Time=0.1903\n",
      "Epoch time: 2.3127, Total time: 28.8489\n",
      "Evaluating...\n",
      "[27:   2]: eps=0.43589744 Loss=288.2235 Robust_Loss=388.2730 Time=0.0755\n",
      "Epoch 28, learning rate [0.001]\n",
      "[28:   0]: eps=0.43846154 Loss=273.3914 Robust_Loss=368.8564 Time=0.1592\n",
      "[28:  10]: eps=0.46410256 Loss=296.1568 Robust_Loss=398.9951 Time=0.1665\n",
      "[28:  11]: eps=0.46666667 Loss=291.3463 Robust_Loss=392.1174 Time=0.1646\n",
      "Epoch time: 2.0083, Total time: 30.8572\n",
      "Evaluating...\n",
      "[28:   2]: eps=0.46666667 Loss=290.6877 Robust_Loss=391.8555 Time=0.1158\n",
      "Epoch 29, learning rate [0.001]\n",
      "[29:   0]: eps=0.46923077 Loss=284.1874 Robust_Loss=381.3205 Time=0.1494\n",
      "[29:  10]: eps=0.49487179 Loss=297.6093 Robust_Loss=399.4083 Time=0.2101\n",
      "[29:  11]: eps=0.49743590 Loss=292.0219 Robust_Loss=391.8448 Time=0.2057\n",
      "Epoch time: 2.5032, Total time: 33.3604\n",
      "Evaluating...\n",
      "[29:   2]: eps=0.49743590 Loss=290.8736 Robust_Loss=391.0658 Time=0.0809\n",
      "Epoch 30, learning rate [0.001]\n",
      "[30:   0]: eps=0.50000000 Loss=290.4007 Robust_Loss=394.6216 Time=0.1542\n",
      "[30:  10]: eps=0.50000000 Loss=289.2740 Robust_Loss=381.6095 Time=0.1803\n",
      "[30:  11]: eps=0.50000000 Loss=283.3238 Robust_Loss=373.5077 Time=0.1867\n",
      "Epoch time: 2.2736, Total time: 35.6340\n",
      "Evaluating...\n",
      "[30:   2]: eps=0.50000000 Loss=275.0506 Robust_Loss=359.0164 Time=0.0768\n",
      "Epoch 31, learning rate [0.001]\n",
      "[31:   0]: eps=0.50000000 Loss=273.6517 Robust_Loss=356.1023 Time=0.1198\n",
      "[31:  10]: eps=0.50000000 Loss=276.6355 Robust_Loss=356.6482 Time=0.1368\n",
      "[31:  11]: eps=0.50000000 Loss=271.3070 Robust_Loss=349.8806 Time=0.1347\n",
      "Epoch time: 1.6445, Total time: 37.2785\n",
      "Evaluating...\n",
      "[31:   2]: eps=0.50000000 Loss=267.9509 Robust_Loss=346.1424 Time=0.0636\n",
      "Epoch 32, learning rate [0.001]\n",
      "[32:   0]: eps=0.50000000 Loss=270.1374 Robust_Loss=346.9358 Time=0.2180\n",
      "[32:  10]: eps=0.50000000 Loss=271.7068 Robust_Loss=348.4484 Time=0.1482\n",
      "[32:  11]: eps=0.50000000 Loss=266.2582 Robust_Loss=341.3562 Time=0.1460\n",
      "Epoch time: 1.7770, Total time: 39.0555\n",
      "Evaluating...\n",
      "[32:   2]: eps=0.50000000 Loss=264.0951 Robust_Loss=339.8172 Time=0.0536\n",
      "Epoch 33, learning rate [0.001]\n",
      "[33:   0]: eps=0.50000000 Loss=273.9266 Robust_Loss=350.1490 Time=0.0986\n",
      "[33:  10]: eps=0.50000000 Loss=268.3045 Robust_Loss=342.2631 Time=0.1168\n",
      "[33:  11]: eps=0.50000000 Loss=262.5442 Robust_Loss=334.9714 Time=0.1160\n",
      "Epoch time: 1.4144, Total time: 40.4699\n",
      "Evaluating...\n",
      "[33:   2]: eps=0.50000000 Loss=260.1022 Robust_Loss=332.7435 Time=0.0536\n",
      "Epoch 34, learning rate [0.001]\n",
      "[34:   0]: eps=0.50000000 Loss=261.3363 Robust_Loss=335.8355 Time=0.1157\n",
      "[34:  10]: eps=0.50000000 Loss=263.2499 Robust_Loss=333.8589 Time=0.1146\n",
      "[34:  11]: eps=0.50000000 Loss=258.4613 Robust_Loss=327.7985 Time=0.1148\n",
      "Epoch time: 1.3982, Total time: 41.8680\n",
      "Evaluating...\n",
      "[34:   2]: eps=0.50000000 Loss=256.3519 Robust_Loss=325.9635 Time=0.0561\n",
      "Epoch 35, learning rate [0.001]\n",
      "[35:   0]: eps=0.50000000 Loss=265.2531 Robust_Loss=337.9176 Time=0.1087\n",
      "[35:  10]: eps=0.50000000 Loss=259.4424 Robust_Loss=327.4097 Time=0.1345\n",
      "[35:  11]: eps=0.50000000 Loss=255.1672 Robust_Loss=321.9495 Time=0.1318\n",
      "Epoch time: 1.6061, Total time: 43.4741\n",
      "Evaluating...\n",
      "[35:   2]: eps=0.50000000 Loss=253.0708 Robust_Loss=319.8248 Time=0.0630\n",
      "Epoch 36, learning rate [0.001]\n",
      "[36:   0]: eps=0.50000000 Loss=260.8600 Robust_Loss=323.7007 Time=0.1196\n",
      "[36:  10]: eps=0.50000000 Loss=256.1823 Robust_Loss=321.5664 Time=0.1120\n",
      "[36:  11]: eps=0.50000000 Loss=252.1070 Robust_Loss=316.3456 Time=0.1129\n",
      "Epoch time: 1.3768, Total time: 44.8509\n",
      "Evaluating...\n",
      "[36:   2]: eps=0.50000000 Loss=250.1223 Robust_Loss=314.2444 Time=0.0545\n",
      "Epoch 37, learning rate [0.001]\n",
      "[37:   0]: eps=0.50000000 Loss=269.1120 Robust_Loss=336.9085 Time=0.1127\n",
      "[37:  10]: eps=0.50000000 Loss=254.3228 Robust_Loss=317.4894 Time=0.1201\n",
      "[37:  11]: eps=0.50000000 Loss=249.6611 Robust_Loss=311.6318 Time=0.1200\n",
      "Epoch time: 1.4632, Total time: 46.3141\n",
      "Evaluating...\n",
      "[37:   2]: eps=0.50000000 Loss=247.7861 Robust_Loss=309.9260 Time=0.0695\n",
      "Epoch 38, learning rate [0.001]\n",
      "[38:   0]: eps=0.50000000 Loss=258.5260 Robust_Loss=319.1715 Time=0.1076\n",
      "[38:  10]: eps=0.50000000 Loss=252.1504 Robust_Loss=313.4283 Time=0.1552\n",
      "[38:  11]: eps=0.50000000 Loss=247.5103 Robust_Loss=307.6926 Time=0.1520\n",
      "Epoch time: 1.8516, Total time: 48.1657\n",
      "Evaluating...\n",
      "[38:   2]: eps=0.50000000 Loss=245.5793 Robust_Loss=305.8793 Time=0.0553\n",
      "Epoch 39, learning rate [0.001]\n",
      "[39:   0]: eps=0.50000000 Loss=251.9668 Robust_Loss=312.1977 Time=0.0987\n",
      "[39:  10]: eps=0.50000000 Loss=249.3271 Robust_Loss=308.7464 Time=0.1071\n",
      "[39:  11]: eps=0.50000000 Loss=245.4675 Robust_Loss=304.1562 Time=0.1095\n",
      "Epoch time: 1.3367, Total time: 49.5024\n",
      "Evaluating...\n",
      "[39:   2]: eps=0.50000000 Loss=244.0083 Robust_Loss=303.3111 Time=0.0563\n",
      "Epoch 40, learning rate [0.00025]\n",
      "[40:   0]: eps=0.50000000 Loss=263.1837 Robust_Loss=326.6429 Time=0.1734\n",
      "[40:  10]: eps=0.50000000 Loss=247.5014 Robust_Loss=306.8036 Time=0.1275\n",
      "[40:  11]: eps=0.50000000 Loss=244.2574 Robust_Loss=302.4454 Time=0.1277\n",
      "Epoch time: 1.5526, Total time: 51.0549\n",
      "Evaluating...\n",
      "[40:   2]: eps=0.50000000 Loss=243.0667 Robust_Loss=301.6376 Time=0.0509\n",
      "Epoch 41, learning rate [0.0005]\n",
      "[41:   0]: eps=0.50000000 Loss=251.2491 Robust_Loss=307.0222 Time=0.1423\n",
      "[41:  10]: eps=0.50000000 Loss=248.6585 Robust_Loss=307.0148 Time=0.1592\n",
      "[41:  11]: eps=0.50000000 Loss=243.7151 Robust_Loss=300.9335 Time=0.1557\n",
      "Epoch time: 1.8915, Total time: 52.9465\n",
      "Evaluating...\n",
      "[41:   2]: eps=0.50000000 Loss=242.0161 Robust_Loss=299.5905 Time=0.0575\n",
      "Epoch 42, learning rate [0.0005]\n",
      "[42:   0]: eps=0.50000000 Loss=237.1505 Robust_Loss=295.5620 Time=0.1098\n",
      "[42:  10]: eps=0.50000000 Loss=246.5918 Robust_Loss=303.6962 Time=0.1391\n",
      "[42:  11]: eps=0.50000000 Loss=242.4730 Robust_Loss=298.7338 Time=0.1403\n",
      "Epoch time: 1.7129, Total time: 54.6593\n",
      "Evaluating...\n",
      "[42:   2]: eps=0.50000000 Loss=241.1321 Robust_Loss=297.9542 Time=0.0661\n",
      "Epoch 43, learning rate [0.0005]\n",
      "[43:   0]: eps=0.50000000 Loss=253.0591 Robust_Loss=308.0602 Time=0.1227\n",
      "[43:  10]: eps=0.50000000 Loss=246.8802 Robust_Loss=303.8653 Time=0.1252\n",
      "[43:  11]: eps=0.50000000 Loss=241.9694 Robust_Loss=297.7451 Time=0.1253\n",
      "Epoch time: 1.5308, Total time: 56.1901\n",
      "Evaluating...\n",
      "[43:   2]: eps=0.50000000 Loss=240.3874 Robust_Loss=296.6282 Time=0.0591\n",
      "Epoch 44, learning rate [0.0005]\n",
      "[44:   0]: eps=0.50000000 Loss=248.0722 Robust_Loss=302.6103 Time=0.1313\n",
      "[44:  10]: eps=0.50000000 Loss=245.9515 Robust_Loss=302.2781 Time=0.1240\n",
      "[44:  11]: eps=0.50000000 Loss=241.1987 Robust_Loss=296.3980 Time=0.1240\n",
      "Epoch time: 1.5094, Total time: 57.6995\n",
      "Evaluating...\n",
      "[44:   2]: eps=0.50000000 Loss=239.5629 Robust_Loss=295.1414 Time=0.0581\n",
      "Epoch 45, learning rate [0.0005]\n",
      "[45:   0]: eps=0.50000000 Loss=254.2668 Robust_Loss=309.6408 Time=0.1188\n",
      "[45:  10]: eps=0.50000000 Loss=243.2206 Robust_Loss=298.4254 Time=0.1300\n",
      "[45:  11]: eps=0.50000000 Loss=239.9871 Robust_Loss=294.4852 Time=0.1288\n",
      "Epoch time: 1.5681, Total time: 59.2677\n",
      "Evaluating...\n",
      "[45:   2]: eps=0.50000000 Loss=238.8731 Robust_Loss=293.9640 Time=0.0571\n",
      "Epoch 46, learning rate [0.0005]\n",
      "[46:   0]: eps=0.50000000 Loss=252.3529 Robust_Loss=307.8546 Time=0.1217\n",
      "[46:  10]: eps=0.50000000 Loss=243.5614 Robust_Loss=298.4695 Time=0.1259\n",
      "[46:  11]: eps=0.50000000 Loss=239.5403 Robust_Loss=293.5390 Time=0.1249\n",
      "Epoch time: 1.5229, Total time: 60.7906\n",
      "Evaluating...\n",
      "[46:   2]: eps=0.50000000 Loss=238.1541 Robust_Loss=292.6630 Time=0.0553\n",
      "Epoch 47, learning rate [0.0005]\n",
      "[47:   0]: eps=0.50000000 Loss=254.3378 Robust_Loss=310.1063 Time=0.1124\n",
      "[47:  10]: eps=0.50000000 Loss=242.9960 Robust_Loss=297.4022 Time=0.1198\n",
      "[47:  11]: eps=0.50000000 Loss=238.9450 Robust_Loss=292.4997 Time=0.1183\n",
      "Epoch time: 1.4440, Total time: 62.2346\n",
      "Evaluating...\n",
      "[47:   2]: eps=0.50000000 Loss=237.6031 Robust_Loss=291.7628 Time=0.0598\n",
      "Epoch 48, learning rate [0.0005]\n",
      "[48:   0]: eps=0.50000000 Loss=230.7583 Robust_Loss=280.5822 Time=0.1107\n",
      "[48:  10]: eps=0.50000000 Loss=241.9167 Robust_Loss=295.8936 Time=0.1142\n",
      "[48:  11]: eps=0.50000000 Loss=238.2671 Robust_Loss=291.4312 Time=0.1136\n",
      "Epoch time: 1.3842, Total time: 63.6188\n",
      "Evaluating...\n",
      "[48:   2]: eps=0.50000000 Loss=236.8650 Robust_Loss=290.4182 Time=0.0539\n",
      "Epoch 49, learning rate [0.0005]\n",
      "[49:   0]: eps=0.50000000 Loss=249.5393 Robust_Loss=303.1715 Time=0.1088\n",
      "[49:  10]: eps=0.50000000 Loss=241.5537 Robust_Loss=295.1149 Time=0.1205\n",
      "[49:  11]: eps=0.50000000 Loss=237.6621 Robust_Loss=290.3015 Time=0.1195\n",
      "Epoch time: 1.4576, Total time: 65.0764\n",
      "Evaluating...\n",
      "[49:   2]: eps=0.50000000 Loss=236.0178 Robust_Loss=288.8422 Time=0.0499\n",
      "Epoch 50, learning rate [0.000125]\n",
      "[50:   0]: eps=0.50000000 Loss=231.3981 Robust_Loss=280.3407 Time=0.0976\n",
      "[50:  10]: eps=0.50000000 Loss=238.4324 Robust_Loss=291.0696 Time=0.1172\n",
      "[50:  11]: eps=0.50000000 Loss=236.4501 Robust_Loss=288.4944 Time=0.1164\n",
      "Epoch time: 1.4223, Total time: 66.4986\n",
      "Evaluating...\n",
      "[50:   2]: eps=0.50000000 Loss=235.6657 Robust_Loss=288.2407 Time=0.0552\n"
     ]
    }
   ],
   "source": [
    "train_robust(model_robust_wrap,dataloader_train,dataloader_test,method=\"robust\",args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_noise_wrapper(t,e):\n",
    "    def input_noise(f):\n",
    "    \n",
    "        log_exact = e * torch.log(f) + e * -(t * f)\n",
    "        log_right = (1 - e) * -(f * t)\n",
    "    \n",
    "        return (log_exact + log_right).sum()\n",
    "\n",
    "    return input_noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_jacobian_wrapper(model):\n",
    "    def feature_jacobian(x):\n",
    "        return model(x).sum()\n",
    "\n",
    "    return feature_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,T_train,E_train = dataloader_train.dataset.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = clf_robust(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "input_noise = input_noise_wrapper(T_train,E_train)\n",
    "feature_jacobian = feature_jacobian_wrapper(clf_robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_noise_matrix = torch.autograd.functional.hessian(input_noise,f).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'vmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m(\u001b[38;5;28;01mlambda\u001b[39;00m x: torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mjacobian(feature_jacobian,x))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'vmap'"
     ]
    }
   ],
   "source": [
    "g = torch.vmap(lambda x: torch.autograd.functional.jacobian(feature_jacobian,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_jacobian_matrix = torch.autograd.functional.jacobian(feature_jacobian,X_train,vectorize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1502, 10])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_jacobian_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "GGN = torch.einsum(\"ij,jk,kl->il\",feature_jacobian_matrix.T,input_noise_matrix,feature_jacobian_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-4.0050e-02, -1.5325e-02,  3.8261e-01, -4.6629e-02,  8.1042e-03,\n",
       "         7.2691e-02, -2.6946e-02, -2.5298e-01, -5.7435e-02,  6.3444e-02,\n",
       "        -1.0338e-02, -7.8418e-03, -3.1645e-01,  4.2134e-01,  1.9483e-02,\n",
       "        -1.0950e-02, -6.5861e-02, -8.8244e-02,  6.8074e-02, -3.2696e-02,\n",
       "        -1.2317e-01, -2.0445e-01,  1.1611e-01,  1.7681e-01,  3.6831e-01,\n",
       "        -4.2261e-01, -4.1311e-05,  3.6252e-02, -6.4882e-02,  3.0270e-04,\n",
       "         1.1336e-03, -2.8301e-01, -5.3418e-02,  1.5328e-01,  1.4185e-01,\n",
       "        -1.3245e-01, -1.0019e-01, -1.1118e-01,  2.8019e-01,  6.2318e-03,\n",
       "         8.6517e-03, -2.3194e-02, -1.5506e-01, -1.6453e-01, -3.8632e-01,\n",
       "         3.0604e-01, -2.7372e-02,  1.2627e-01,  1.9315e-01, -5.6199e-03,\n",
       "         8.8263e-02, -1.6328e-01,  3.1899e-01, -2.1560e-01,  1.3379e-01,\n",
       "         8.2785e-02, -8.3005e-02,  7.5767e-02,  8.3884e-02, -1.9764e-03,\n",
       "         3.9146e-01,  1.9229e-01, -2.4575e-01,  1.4082e-01,  1.7515e-01,\n",
       "         6.5948e-02,  2.1431e-01,  9.4760e-02,  1.8434e-01, -2.5089e-01,\n",
       "         2.7224e-02, -5.6791e-02,  9.4129e-02,  1.6424e-01,  2.7139e-02,\n",
       "         2.0547e-02,  2.1746e-02,  8.1355e-04, -2.2950e-01, -3.0555e-02,\n",
       "         7.7674e-02,  1.4018e-02,  1.2797e-01,  1.5705e-02, -3.8435e-01,\n",
       "         3.4997e-01, -9.4251e-02, -2.0735e-01,  9.2350e-03, -2.4418e-02,\n",
       "         1.5729e-01, -1.6266e-01, -3.8390e-03, -1.1014e-01,  2.5601e-01,\n",
       "        -2.9472e-01, -1.2897e-01, -2.5067e-05, -6.5993e-02, -1.2334e-01,\n",
       "         2.1689e-01, -3.1229e-01, -1.1890e-01,  7.7219e-02, -9.6757e-04,\n",
       "        -1.8271e-02,  2.5820e-01, -6.4662e-02,  1.8069e-01,  1.5240e-01,\n",
       "         5.7527e-02, -4.2099e-02, -3.0511e-01,  3.5080e-01, -9.3012e-02,\n",
       "        -1.6243e-01, -1.0990e-01,  1.3224e-01,  7.4718e-03,  1.2127e-01,\n",
       "         1.3424e-01, -9.4925e-02,  3.1573e-01, -3.6432e-01, -1.2171e-01,\n",
       "        -3.4153e-01,  1.5785e-01,  1.3886e-01,  2.5916e-02, -2.7229e-03,\n",
       "        -3.9697e-01, -9.2529e-02,  3.5788e-02, -1.2865e-01, -1.6362e-01,\n",
       "         2.3817e-01, -4.1665e-05, -1.4507e-02,  1.2357e-01, -1.8073e-01,\n",
       "         1.3089e-01,  3.3772e-01, -1.9940e-01, -2.2206e-01, -2.5375e-03,\n",
       "        -1.7130e-01,  4.6616e-02, -1.7334e-01, -2.5805e-01,  1.0273e-01,\n",
       "         2.7439e-01, -1.1791e-01,  3.7040e-01,  4.0988e-01,  1.8348e-01,\n",
       "         5.3425e-01,  9.2152e-02,  5.2234e-01, -8.7645e-02,  5.3029e-01,\n",
       "         5.8013e-01, -1.3993e-01,  2.2789e-01,  5.8135e-02,  2.6374e-01,\n",
       "        -1.2563e-01,  3.0963e-01,  1.5371e-01, -1.2361e-01,  1.3763e-01,\n",
       "        -2.6489e-01,  1.0695e-02, -1.6843e-01, -1.0458e-01, -2.9456e-01,\n",
       "        -1.5255e-01, -4.5018e-02, -1.9178e-01, -1.3932e-01,  7.3163e-02,\n",
       "        -1.1065e-01, -3.2360e-01, -3.0493e-01,  4.8611e-02, -3.1723e-01,\n",
       "        -2.2708e-01, -2.4019e-01,  5.5489e-02,  1.1887e-01, -1.1242e-01,\n",
       "        -2.2705e-01, -7.8319e-02, -2.9883e-02,  6.8054e-02, -7.5221e-02,\n",
       "        -4.4897e-02,  7.2890e-02,  2.1535e-01,  2.1180e-01,  1.9165e-01,\n",
       "         1.0945e-01, -2.5694e-04,  2.0914e-01,  1.5388e-01,  5.2350e-02,\n",
       "        -1.3167e-02, -1.8220e-01,  2.3229e-01,  2.5196e-01,  3.6968e-01,\n",
       "         1.5721e-01, -2.3773e-02,  2.8518e-01, -4.0078e-02,  8.2780e-02,\n",
       "         1.6374e-01, -1.2697e-01,  4.1378e-01, -6.3269e-02,  5.5667e-02,\n",
       "         2.0747e-01, -1.0753e-01, -8.5735e-03, -6.9672e-02, -3.5049e-02,\n",
       "         1.5140e-01, -1.8748e-02, -1.7213e-01,  1.2280e-01, -7.2522e-03,\n",
       "         8.7158e-02,  1.6298e-01,  1.8933e-01, -1.1248e-01, -4.5136e-02,\n",
       "        -1.9186e-01,  2.0398e-01,  3.1919e-02, -7.7384e-02,  1.0561e-02,\n",
       "         3.2958e-01,  1.7523e-01,  2.8486e-01,  3.7328e-01,  7.6573e-02,\n",
       "         1.4513e-01,  7.6961e-02, -8.9540e-02,  1.3888e-01,  1.4209e-01,\n",
       "         1.0862e-01,  5.2611e-02,  3.4161e-01,  1.1559e-01, -1.3044e-01,\n",
       "         1.7151e-01,  1.0738e-01,  1.1326e-01,  8.4871e-02, -1.8221e-02,\n",
       "        -1.6420e-01,  8.3284e-02, -2.1631e-01, -2.1423e-01, -2.2077e-02,\n",
       "        -2.2960e-01,  1.8262e-01,  2.0187e-01,  6.3757e-02,  1.1769e-01,\n",
       "        -1.2144e-01,  2.9444e-01,  2.5809e-01,  2.1105e-02, -6.2794e-02,\n",
       "         2.7234e-01,  1.2212e-01,  2.5833e-01,  7.7184e-03,  1.3792e-01,\n",
       "        -5.2625e-02,  2.0228e-01,  1.0357e-01, -1.0609e-01, -3.8147e-02,\n",
       "        -1.2381e-02, -3.6302e-01,  1.8631e-01, -8.9833e-02, -5.5866e-02,\n",
       "         6.9612e-02, -1.6885e-01, -1.9213e-01, -1.0208e-01,  5.3594e-02,\n",
       "        -3.3926e-02,  3.1333e-02,  9.7135e-02,  5.8497e-03,  1.9898e-01,\n",
       "        -1.7768e-01, -2.9272e-01,  1.5685e-02, -7.6705e-03,  1.9803e-01,\n",
       "         2.1067e-01,  1.0476e-02,  1.2615e-01,  7.0262e-02, -6.5244e-02,\n",
       "        -2.7359e-01, -1.5672e-01, -2.7173e-01,  1.1788e-01, -1.7070e-01,\n",
       "        -2.7155e-01, -1.1048e-01,  9.1875e-02,  1.7012e-02,  1.3501e-02,\n",
       "        -8.6254e-02, -2.8003e-01, -2.0359e-01, -2.4685e-01, -1.5165e-01,\n",
       "         9.9220e-02, -2.2423e-01, -1.4535e-01,  1.1344e-01,  1.1663e-01,\n",
       "         1.7014e-01,  2.3483e-01,  1.5147e-01,  2.0433e-02,  2.0148e-01,\n",
       "         2.4960e-01, -8.5092e-02,  3.8313e-01, -1.3935e-01,  1.7443e-01,\n",
       "         3.4895e-01,  1.1499e-01, -8.0047e-02,  8.1776e-02,  7.7656e-02,\n",
       "         1.8765e-01,  2.1938e-01,  4.2723e-02,  1.0173e-01,  2.8863e-01,\n",
       "         2.5361e-02, -8.3834e-02,  1.8683e-01, -2.7471e-02,  1.7164e-01,\n",
       "         3.1797e-01,  1.9747e-01,  4.1076e-02,  6.5341e-02, -1.0347e-01,\n",
       "        -1.6793e-02,  4.5061e-02,  1.5454e-01,  1.8679e-01,  8.2464e-02,\n",
       "         7.2388e-02,  1.9261e-01,  7.4306e-02,  2.5426e-01,  2.9715e-01,\n",
       "         1.1821e-01, -6.5225e-02,  2.7161e-01,  4.8696e-02, -4.1668e-02,\n",
       "         4.1320e-02, -5.1438e-02,  1.6627e-01, -4.1879e-01, -1.4970e-02,\n",
       "        -3.5661e-01, -6.1143e-02, -1.2286e-01,  1.1707e-01, -2.3640e-01,\n",
       "        -2.6388e-01,  4.0218e-03, -8.1068e-02, -2.1205e-01, -7.5145e-02,\n",
       "         6.7002e-02, -2.5184e-01,  4.5180e-02,  1.9596e-01,  5.8753e-02,\n",
       "         1.4225e-01,  1.6745e-01,  2.6404e-01,  8.5165e-05,  1.7939e-01,\n",
       "         1.2254e-02,  3.0479e-02,  3.1076e-01,  1.5687e-01, -2.9995e-01,\n",
       "         1.1884e-01,  2.4223e-01, -2.5262e-01, -4.2867e-01,  3.0580e-02,\n",
       "        -3.6702e-01, -8.1587e-03, -3.5158e-01, -1.1227e-02, -1.6098e-02,\n",
       "         5.6418e-02, -2.4682e-01, -1.7626e-01, -2.8796e-01,  1.1470e-01,\n",
       "        -1.0309e-01])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.utils.parameters_to_vector(clf_robust.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-157.3515,   -0.0000,   -0.0000,  ...,   -0.0000,   -0.0000,\n",
       "           -0.0000],\n",
       "        [  -0.0000,  -33.0515,   -0.0000,  ...,   -0.0000,   -0.0000,\n",
       "           -0.0000],\n",
       "        [  -0.0000,   -0.0000,   -0.0000,  ...,   -0.0000,   -0.0000,\n",
       "           -0.0000],\n",
       "        ...,\n",
       "        [  -0.0000,   -0.0000,   -0.0000,  ...,   -0.0000,   -0.0000,\n",
       "           -0.0000],\n",
       "        [  -0.0000,   -0.0000,   -0.0000,  ...,   -0.0000,   -0.0000,\n",
       "           -0.0000],\n",
       "        [  -0.0000,   -0.0000,   -0.0000,  ...,   -0.0000,   -0.0000,\n",
       "          -11.7302]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_noise_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "survival"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
