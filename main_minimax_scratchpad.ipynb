{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:37.188521Z",
     "start_time": "2023-11-07T20:21:28.626293Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpott\\anaconda3\\envs\\survival\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\lpott\\anaconda3\\envs\\survival\\lib\\site-packages\\torch\\torch_version.py:21: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from src.criterion import right_censored\n",
    "from src.load_data import load_datasets,load_dataframe\n",
    "from src.utils import train\n",
    "\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "args = ARGS()\n",
    "args.verify=False\n",
    "args.device=\"cpu\"\n",
    "\n",
    "args.seed = 123\n",
    "\n",
    "args.eps=0.5\n",
    "args.norm=np.inf\n",
    "args.bound_type = \"CROWN-IBP\"\n",
    "args.num_epochs=100\n",
    "args.lr = 5e-4\n",
    "args.batch_size= 512\n",
    "args.scheduler_name = \"SmoothedScheduler\"\n",
    "args.scheduler_opts = \"start=1000,length=20\"\n",
    "args.hidden_dims = [15,15]\n",
    "args.save_model = \"\"\n",
    "args.dataset = \"Dialysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:37.272860Z",
     "start_time": "2023-11-07T20:21:37.188521Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# GOOD DATASETS\n",
    "# 1. TRACE\n",
    "# 2. divorce \n",
    "# 3. Dialysis\n",
    "dataset_train,dataset_test = load_datasets(args.dataset,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:37.321189Z",
     "start_time": "2023-11-07T20:21:37.297057Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train,batch_size=args.batch_size,shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test,batch_size=args.batch_size,shuffle=False)\n",
    "\n",
    "dataloader_train.mean = dataloader_test.mean = dataset_train.mean\n",
    "dataloader_train.std = dataloader_test.std = dataset_train.std\n",
    "\n",
    "\n",
    "_,args.input_dims = dataset_train.tensors[0].shape\n",
    "args.output_dims = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Weibull_Model(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dim,10)\n",
    "        self.linear2 = nn.Linear(10,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return torch.exp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Weibull_Model(input_dim=args.input_dims,output_dim=args.output_dims)\n",
    "\n",
    "optimizer = Adam(clf.parameters(),lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RightCensorWrapper(nn.Module):\n",
    "#     def __init__(self,model):\n",
    "#         super(RightCensorWrapper,self).__init__()\n",
    "#         self.model = model\n",
    "\n",
    "#     def forward(self,x,t,e):\n",
    "#         parameters = self.model(x)\n",
    "        \n",
    "#         # rate = torch.matmul(parameters,torch.FloatTensor([[1],[0]]))\n",
    "#         # k = torch.matmul(parameters,torch.FloatTensor([[0],[1]]))\n",
    "\n",
    "\n",
    "#         # print(k.shape)\n",
    "#         # print(rate.shape)\n",
    "\n",
    "#         # log_exact = e * torch.log(rate) + e*torch.log(k) + e * (k-1)*torch.log(rate) + e*(k-1)*torch.log(t) + e * -(torch.pow(t,k) * torch.pow(rate,k))\n",
    "#         # log_right = (1 - e) * -(torch.pow(rate,k) * torch.pow(t,k))\n",
    "#         # log_right = (1 - e) * parameters #-(torch.pow(parameters,2.0) * torch.pow(t,2.0))\n",
    "#         parameters = parameters*e+t\n",
    "#         return torch.mean(parameters,axis=1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RightCensorWrapper(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(RightCensorWrapper,self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self,x,t,e):\n",
    "        rate = self.model(x)\n",
    "\n",
    "        log_exact = e * torch.log(rate) + e * -(t * rate)\n",
    "        log_right = (1 - e) * -(rate * t)\n",
    "\n",
    "        return -log_exact + -log_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def right_censored(rate,k, t,e):\n",
    "    log_exact = e * torch.log(rate) * e*torch.log(k) + e * (k-1)*torch.log(rate) + e*(k-1)*torch.log(t) + e * -(torch.pow(t,k) * torch.pow(rate,k))\n",
    "    log_right = (1 - e) * - (torch.pow(rate,k) * torch.pow(t,k))\n",
    "\n",
    "    return (-log_exact + -log_right).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_LiRPA import BoundedModule, BoundedTensor\n",
    "from auto_LiRPA.perturbations import *\n",
    "from auto_LiRPA.utils import MultiAverageMeter\n",
    "from auto_LiRPA.eps_scheduler import LinearScheduler, AdaptiveScheduler, SmoothedScheduler, FixedScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,T_train,E_train = dataset_train.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9068],\n",
       "        [0.9417],\n",
       "        [1.0010],\n",
       "        ...,\n",
       "        [1.0702],\n",
       "        [0.9447],\n",
       "        [1.1344]], grad_fn=<PowBackward1>)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(clf(X_train),clf(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[184], line 44\u001b[0m\n\u001b[0;32m     40\u001b[0m my_input \u001b[38;5;241m=\u001b[39m BoundedTensor(X_train, ptb)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# # Regular forward propagation using BoundedTensor works as usual.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# prediction = model(my_input)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Compute LiRPA bounds using the backward mode bound propagation (CROWN).\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m lb, ub \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_bounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmy_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43mT_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mE_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mIBP\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m ub\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\survival\\lib\\site-packages\\auto_LiRPA\\bound_general.py:1245\u001b[0m, in \u001b[0;36mBoundedModule.compute_bounds\u001b[1;34m(self, x, aux, C, method, IBP, forward, bound_lower, bound_upper, reuse_ibp, reuse_alpha, return_A, needed_A_dict, final_node_name, average_A, intermediate_layer_bounds, reference_bounds, intermediate_constr, alpha_idx, aux_reference_bounds, need_A_only, cutter, decision_thresh, update_mask)\u001b[0m\n\u001b[0;32m   1242\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal node \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IBP:\n\u001b[1;32m-> 1245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mibp_lower, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mibp_upper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIBP_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mibp_lower, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mibp_upper\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\survival\\lib\\site-packages\\auto_LiRPA\\interval_bound.py:30\u001b[0m, in \u001b[0;36mIBP_general\u001b[1;34m(self, node, C, delete_bounds_after_use)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39minputs:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(n, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterval\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;66;03m# Node n does not have interval bounds; we must compute it.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIBP_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelete_bounds_after_use\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelete_bounds_after_use\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m         to_be_deleted_bounds\u001b[38;5;241m.\u001b[39mappend(n)\n\u001b[0;32m     33\u001b[0m inp \u001b[38;5;241m=\u001b[39m [n_pre\u001b[38;5;241m.\u001b[39minterval \u001b[38;5;28;01mfor\u001b[39;00m n_pre \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39minputs]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\survival\\lib\\site-packages\\auto_LiRPA\\interval_bound.py:30\u001b[0m, in \u001b[0;36mIBP_general\u001b[1;34m(self, node, C, delete_bounds_after_use)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39minputs:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(n, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minterval\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;66;03m# Node n does not have interval bounds; we must compute it.\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIBP_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelete_bounds_after_use\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelete_bounds_after_use\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m         to_be_deleted_bounds\u001b[38;5;241m.\u001b[39mappend(n)\n\u001b[0;32m     33\u001b[0m inp \u001b[38;5;241m=\u001b[39m [n_pre\u001b[38;5;241m.\u001b[39minterval \u001b[38;5;28;01mfor\u001b[39;00m n_pre \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39minputs]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\survival\\lib\\site-packages\\auto_LiRPA\\interval_bound.py:42\u001b[0m, in \u001b[0;36mIBP_general\u001b[1;34m(self, node, C, delete_bounds_after_use)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     node\u001b[38;5;241m.\u001b[39minterval \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterval_propagate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m node\u001b[38;5;241m.\u001b[39mlower, node\u001b[38;5;241m.\u001b[39mupper \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39minterval\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node\u001b[38;5;241m.\u001b[39mlower, torch\u001b[38;5;241m.\u001b[39mSize):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\survival\\lib\\site-packages\\auto_LiRPA\\operators\\nonlinear.py:559\u001b[0m, in \u001b[0;36mBoundPow.interval_propagate\u001b[1;34m(self, *v)\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minterval_propagate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mv):\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_input_perturbed(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    560\u001b[0m     exp \u001b[38;5;241m=\u001b[39m v[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exp \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mint\u001b[39m(exp)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class RightCensorWrapper(nn.Module):\n",
    "    def __init__(self,model):\n",
    "        super(RightCensorWrapper,self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self,x,t,e):\n",
    "        rate = self.model(x)[:,[0]]\n",
    "        k = self.model(x)[:,[1]]\n",
    "\n",
    "        log_exact = e * torch.log(rate) + e * -(t * rate)\n",
    "        log_right = torch.pow(rate,k)\n",
    "\n",
    "        return -log_exact + -log_right\n",
    "        \n",
    "class Weibull_Model(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dim,10)\n",
    "        self.linear2 = nn.Linear(10,2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return torch.exp(x)\n",
    "epsilon = 0.1 \n",
    "from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm\n",
    "model = Weibull_Model(X_train.shape[1],1)\n",
    "\n",
    "\n",
    "my_input = X_train\n",
    "# Wrap the model with auto_LiRPA.\n",
    "model = BoundedModule(RightCensorWrapper(model), dataloader_train.dataset.tensors)\n",
    "# Define perturbation. Here we add Linf perturbation to input data.\n",
    "ptb = PerturbationLpNorm(norm=np.inf, eps=0.1)\n",
    "# Make the input a BoundedTensor with the pre-defined perturbation.\n",
    "my_input = BoundedTensor(X_train, ptb)\n",
    "# # Regular forward propagation using BoundedTensor works as usual.\n",
    "# prediction = model(my_input)\n",
    "# Compute LiRPA bounds using the backward mode bound propagation (CROWN).\n",
    "lb, ub = model.compute_bounds(x=(my_input,T_train,E_train), method=\"backward\",IBP=True)\n",
    "ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0168],\n",
       "        [0.9859],\n",
       "        [1.0457],\n",
       "        ...,\n",
       "        [0.9438],\n",
       "        [0.9845],\n",
       "        [1.0355]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm\n",
    "\n",
    "# Define computation as a nn.Module.\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self,input_dim,output_dim=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dim,10)\n",
    "        self.linear2 = nn.Linear(10,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return torch.exp(x)\n",
    "model = MyModel(X_train.shape[1],1)\n",
    "\n",
    "my_input = X_train\n",
    "# Wrap the model with auto_LiRPA.\n",
    "model = BoundedModule(model, my_input)\n",
    "# Define perturbation. Here we add Linf perturbation to input data.\n",
    "ptb = PerturbationLpNorm(norm=np.inf, eps=0.1)\n",
    "# Make the input a BoundedTensor with the pre-defined perturbation.\n",
    "my_input = BoundedTensor(X_train, ptb)\n",
    "# # Regular forward propagation using BoundedTensor works as usual.\n",
    "# prediction = model(my_input)\n",
    "# Compute LiRPA bounds using the backward mode bound propagation (CROWN).\n",
    "lb, ub = model.compute_bounds(x=(my_input,), method=\"backward\",IBP=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_robust = BoundedModule(RightWeibullCensorWrapper(clf), (X_train,T_train,E_train))\n",
    "epsilon = 1e-4\n",
    "# Define perturbation. Here we add Linf perturbation to input data.\n",
    "ptb = PerturbationLpNorm(norm=np.inf, eps=torch.Tensor([epsilon]))\n",
    "# Make the input a BoundedTensor with the pre-defined perturbation.\n",
    "my_input = BoundedTensor(torch.Tensor(X_train), ptb)\n",
    "# Regular forward propagation using BoundedTensor works as usual.\n",
    "# prediction = model(my_input,torch.rand_like(T_train),torch.rand_like(E_train))\n",
    "# Compute LiRPA bounds using CROWN\n",
    "lb, ub = model_robust.compute_bounds(method=\"backward\",x=(my_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_robust_step(model_loss, t, loader, eps_scheduler, norm, train, opt, bound_type, method='robust'):\n",
    "    meter = MultiAverageMeter()\n",
    "    if train:\n",
    "        model_loss.train()\n",
    "        eps_scheduler.train()\n",
    "        eps_scheduler.step_epoch()\n",
    "        eps_scheduler.set_epoch_length(int((len(loader.dataset) + loader.batch_size - 1) / loader.batch_size))\n",
    "    else:\n",
    "        model_loss.eval()\n",
    "        eps_scheduler.eval()\n",
    "\n",
    "    for i, data in enumerate(loader):\n",
    "        start = time.time()\n",
    "        eps_scheduler.step_batch()\n",
    "        eps = eps_scheduler.get_eps()\n",
    "        xi, ti, yi = data\n",
    "\n",
    "        # For small eps just use natural training, no need to compute LiRPA bounds\n",
    "        batch_method = method\n",
    "        if eps < 1e-20:\n",
    "            batch_method = \"natural\"\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "\n",
    "        # Specify Lp norm perturbation.\n",
    "        # When using Linf perturbation, we manually set element-wise bound x_L and x_U. eps is not used for Linf norm.\n",
    "        if norm > 0:\n",
    "            ptb = PerturbationLpNorm(norm=norm, eps=eps)\n",
    "        elif norm == 0:\n",
    "            ptb = PerturbationL0Norm(eps=eps_scheduler.get_max_eps(),\n",
    "                                     ratio=eps_scheduler.get_eps() / eps_scheduler.get_max_eps())\n",
    "\n",
    "        # Make the input a BoundedTensor with the pre-defined perturbation.\n",
    "        x_bounded = BoundedTensor(xi, ptb)\n",
    "\n",
    "        regular_loss = model_loss(xi, ti, yi).sum()  # regular Right Censoring\n",
    "        meter.update('Loss', regular_loss.item(), xi.size(0))\n",
    "\n",
    "        if batch_method == \"robust\":\n",
    "            # Compute LiRPA bounds using CROWN\n",
    "            lb, ub = model_loss.compute_bounds(x=(x_bounded, ti, yi), IBP=False, method=\"backward\", bound_upper=True,\n",
    "                                               bound_lower=False)\n",
    "            robust_loss = ub.sum()\n",
    "            loss = robust_loss\n",
    "        elif batch_method == \"natural\":\n",
    "            loss = regular_loss\n",
    "\n",
    "        if train:\n",
    "            (.1 * loss + 0.9 * regular_loss).backward()\n",
    "            eps_scheduler.update_loss(loss.item() - regular_loss.item())\n",
    "            opt.step()\n",
    "        meter.update('Loss', loss.item(), xi.size(0))\n",
    "        if batch_method != \"natural\":\n",
    "            meter.update('Robust_Loss', robust_loss.item(), xi.size(0))\n",
    "\n",
    "        meter.update('Time', time.time() - start)\n",
    "        if i % 10 == 0 and train:\n",
    "            print('[{:2d}:{:4d}]: eps={:.8f} {}'.format(t, i, eps, meter))\n",
    "    print('[{:2d}:{:4d}]: eps={:.8f} {}'.format(t, i, eps, meter))\n",
    "\n",
    "def train_robust(model,dataloader_train,dataloader_test,method,args):\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    # model = BoundedModule(clf, X_train)\n",
    "\n",
    "    ## Step 4 prepare optimizer, epsilon scheduler and learning rate scheduler\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    norm = float(args.norm)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    eps_scheduler = eval(args.scheduler_name)(args.eps, args.scheduler_opts)\n",
    "\n",
    "    if method == \"robust\":\n",
    "        scheduler_opts = args.scheduler_opts\n",
    "    elif method == \"natural\":\n",
    "        scheduler_opts = f\"start={args.num_epochs+1},length={args.num_epochs+1}\"\n",
    "\n",
    "    eps_scheduler = eval(args.scheduler_name)(args.eps, scheduler_opts)\n",
    "\n",
    "\n",
    "\n",
    "    timer = 0.0\n",
    "    for t in range(1, args.num_epochs+1):\n",
    "        if eps_scheduler.reached_max_eps():\n",
    "            # Only decay learning rate after reaching the maximum eps\n",
    "            lr_scheduler.step()\n",
    "        print(\"Epoch {}, learning rate {}\".format(t, lr_scheduler.get_lr()))\n",
    "        start_time = time.time()\n",
    "        train_robust_step(model, t, dataloader_train, eps_scheduler, args.norm, True, optimizer, args.bound_type)\n",
    "        epoch_time = time.time() - start_time\n",
    "        timer += epoch_time\n",
    "        print('Epoch time: {:.4f}, Total time: {:.4f}'.format(epoch_time, timer))\n",
    "        print(\"Evaluating...\")\n",
    "        with torch.no_grad():\n",
    "            train_robust_step(model, t, dataloader_test, eps_scheduler, norm, False, None, args.bound_type)\n",
    "\n",
    "        if args.save_model != \"\":\n",
    "            torch.save({'state_dict': model.state_dict(), 'epoch': t}, args.save_model)\n",
    "\n",
    "def lower_bound(clf, nominal_input, epsilon):\n",
    "    # Wrap the model with auto_LiRPA.\n",
    "    model = BoundedModule(clf, nominal_input)\n",
    "    # Define perturbation. Here we add Linf perturbation to input data.\n",
    "    ptb = PerturbationLpNorm(norm=np.inf, eps=torch.Tensor([epsilon]))\n",
    "    # Make the input a BoundedTensor with the pre-defined perturbation.\n",
    "    my_input = BoundedTensor(torch.Tensor(nominal_input), ptb)\n",
    "    # Regular forward propagation using BoundedTensor works as usual.\n",
    "    prediction = model(my_input)\n",
    "    # Compute LiRPA bounds using CROWN\n",
    "    lb, ub = model.compute_bounds(x=(my_input,), method=\"backward\")\n",
    "\n",
    "    return lb, ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_robust(model_robust,dataloader_train,dataloader_test,method=\"robust\",args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,param in clf.named_parameters():\n",
    "    print(name,param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.exp(clf.k_logit(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,param in model_robust.named_parameters():\n",
    "    print(name,param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import WeibullFitter,ExponentialFitter\n",
    "clf_weib = WeibullFitter()\n",
    "clf_weib.fit(durations=T_train.ravel(),event_observed=E_train.ravel())\n",
    "clf_weib.plot_survival_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import WeibullFitter,ExponentialFitter,CoxPHFitter\n",
    "clf_weib = WeibullFitter()\n",
    "clf_weib.fit(durations=T_train.ravel(),event_observed=E_train.ravel())\n",
    "clf_weib.plot_survival_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:57.396625Z",
     "start_time": "2023-11-07T20:21:56.638091Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X_train,T_train,E_train = dataloader_train.dataset.tensors\n",
    "t = torch.linspace(0,T_train.max(),10000)\n",
    "\n",
    "St_given_x = clf.survival_qdf(X_train,t).detach()\n",
    "\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(durations=T_train,event_observed=E_train)\n",
    "St_kmf  = kmf.predict(times=t.ravel().numpy())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(t,St_kmf)\n",
    "clf_weib.plot_survival_function(label=\"_nolegend_\")\n",
    "plt.plot(t,St_given_x.mean(0))\n",
    "plt.ylabel(\"S(t)\"); plt.xlabel(\"Time\")\n",
    "plt.legend([\"Kaplan Meier Numerical\",\"Weibull Fit\",\"Neural Network\"])\n",
    "plt.title(\"Train Population Survival Curves\")\n",
    "plt.ylim([0,1.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.exp(clf.k_logit),clf.k_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = clf(X_train)\n",
    "lam,k = parameters[:,[0]],parameters[:,[1]]\n",
    "concordance_index(T_train,predicted_scores= -lam**k,event_observed=E_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,T_test,E_test = dataset_test.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = clf(X_test)\n",
    "lam,k = parameters[:,[0]],parameters[:,[1]]\n",
    "concordance_index(T_test,predicted_scores= - lam**k,event_observed=E_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_weib.predict(T_train.ravel()).values\n",
    "\n",
    "lambda_,rho_ = clf_weib.params_\n",
    "lambda_ = 1/lambda_\n",
    "\n",
    "F = 1.0 - torch.exp(-(T_train*lambda_)**rho_)\n",
    "\n",
    "concordance_index(T_train,predicted_scores=F,event_observed=E_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_weib.predict(T_test.ravel()).values\n",
    "\n",
    "lambda_,rho_ = clf_weib.params_\n",
    "lambda_ = 1/lambda_\n",
    "\n",
    "F = 1.0 - torch.exp(-(T_test*lambda_)**rho_)\n",
    "\n",
    "concordance_index(T_test,predicted_scores=F,event_observed=E_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTO LIRPA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "from auto_LiRPA import BoundedModule, BoundedTensor\n",
    "from auto_LiRPA.perturbations import *\n",
    "from auto_LiRPA.utils import MultiAverageMeter\n",
    "from auto_LiRPA.eps_scheduler import LinearScheduler, AdaptiveScheduler, SmoothedScheduler, FixedScheduler\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from auto_LiRPA import BoundedModule\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = nn.Sequential(nn.Dropout(.5),nn.Linear(input_dims,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(input_dims,1)\n",
    "\n",
    "    def forward(self,x,t,e):\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,T_train,E_train = dataset_train.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# class FNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(FNN, self).__init__()\n",
    "#         self.fc1 = nn.Linear(2, 4)\n",
    "#         self.fc2 = nn.Linear(4, 2)\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.float()\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# model_ori = FNN()\n",
    "# batch_size = 100\n",
    "# x = torch.normal(0, 0.2, (batch_size, 2))\n",
    "# y = torch.ones((batch_size,))\n",
    "\n",
    "# dummy_input = x[:2]\n",
    "# model = BoundedModule(model_ori, dummy_input, bound_opts={\n",
    "#     'relu': \"same-slope\", 'sparse_intermediate_bounds': False,\n",
    "#     'sparse_conv_intermediate_bounds': False, 'sparse_intermediate_bounds_with_ibp': False})\n",
    "# final_name1 = model.final_name\n",
    "\n",
    "# model.train()\n",
    "# c = torch.tensor([[-1, 1], [-1, 1]]).type_as(y)\n",
    "# dummy_out = model(dummy_input)\n",
    "# lb, ub = model(method_opt=\"compute_bounds\", C=c, method=\"IBP\", final_node_name=None,\n",
    "#                no_replicas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BoundedModule(classifier(), (X_train,T_train,E_train))\n",
    "epsilon = 1e-4\n",
    "# Define perturbation. Here we add Linf perturbation to input data.\n",
    "ptb = PerturbationLpNorm(norm=np.inf, eps=torch.Tensor([epsilon]))\n",
    "# Make the input a BoundedTensor with the pre-defined perturbation.\n",
    "my_input = BoundedTensor(torch.Tensor(X_train), ptb)\n",
    "# Regular forward propagation using BoundedTensor works as usual.\n",
    "# prediction = model(my_input,torch.rand_like(T_train),torch.rand_like(E_train))\n",
    "# Compute LiRPA bounds using CROWN\n",
    "lb, ub = model.compute_bounds( method=\"backward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb, ub = model.compute_bounds(x=(my_input,T_train,E_train), method=\"backward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BoundedModule(loss_clf, (X_train,T_train,E_train))\n",
    "# # Define perturbation. Here we add Linf perturbation to input data.\n",
    "# ptb = PerturbationLpNorm(norm=np.inf, eps=torch.Tensor([epsilon]))\n",
    "# # Make the input a BoundedTensor with the pre-defined perturbation.\n",
    "# my_input = BoundedTensor(torch.Tensor(X_train), ptb)\n",
    "# # Regular forward propagation using BoundedTensor works as usual.\n",
    "# # prediction = model(my_input,T_train,E_train)\n",
    "# # Compute LiRPA bounds using CROWN\n",
    "# lb, ub = model.compute_bounds(x=(my_input,T_train,E_train), method=\"backward\",bound_upper=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A simple script to train certified defense using the auto_LiRPA library.\n",
    "\n",
    "We compute output bounds under input perturbations using auto_LiRPA, and use\n",
    "them to form a \"robust loss\" for certified defense.  Several different bound\n",
    "options are supported, such as IBP, CROWN, and CROWN-IBP. This is a basic\n",
    "example on MNIST and CIFAR-10 datasets with Lp (p>=0) norm perturbation. For\n",
    "faster training, please see our examples with loss fusion such as\n",
    "cifar_training.py and tinyimagenet_training.py\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "import argparse\n",
    "import torch.optim as optim\n",
    "from auto_LiRPA import BoundedModule, BoundedTensor\n",
    "from auto_LiRPA.perturbations import *\n",
    "from auto_LiRPA.utils import MultiAverageMeter\n",
    "from auto_LiRPA.eps_scheduler import LinearScheduler, AdaptiveScheduler, SmoothedScheduler, FixedScheduler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "def Train(model_loss, t, loader, eps_scheduler, norm, train, opt, bound_type, method='robust'):\n",
    "    meter = MultiAverageMeter()\n",
    "    if train:\n",
    "        model_loss.train()\n",
    "        eps_scheduler.train()\n",
    "        eps_scheduler.step_epoch()\n",
    "        eps_scheduler.set_epoch_length(int((len(loader.dataset) + loader.batch_size - 1) / loader.batch_size))\n",
    "    else:\n",
    "        model_loss.eval()\n",
    "        eps_scheduler.eval()\n",
    "\n",
    "    for i, data in enumerate(loader):\n",
    "        start = time.time()\n",
    "        eps_scheduler.step_batch()\n",
    "        eps = eps_scheduler.get_eps()\n",
    "        xi,ti,yi = data\n",
    "\n",
    "        # For small eps just use natural training, no need to compute LiRPA bounds\n",
    "        batch_method = method\n",
    "        if eps < 1e-20:\n",
    "            batch_method = \"natural\"\n",
    "        if train:\n",
    "            opt.zero_grad()\n",
    "\n",
    "        # Specify Lp norm perturbation.\n",
    "        # When using Linf perturbation, we manually set element-wise bound x_L and x_U. eps is not used for Linf norm.\n",
    "        if norm > 0:\n",
    "            ptb = PerturbationLpNorm(norm=norm, eps=eps)\n",
    "        elif norm == 0:\n",
    "            ptb = PerturbationL0Norm(eps = eps_scheduler.get_max_eps(), ratio = eps_scheduler.get_eps()/eps_scheduler.get_max_eps())\n",
    "        \n",
    "        # Make the input a BoundedTensor with the pre-defined perturbation.\n",
    "        x_bounded = BoundedTensor(xi, ptb)\n",
    "\n",
    "        regular_loss = model_loss(xi,ti,yi).sum() # regular Right Censoring\n",
    "        meter.update('Loss', regular_loss.item(), xi.size(0))\n",
    "        if batch_method == \"robust\":\n",
    "            # Compute LiRPA bounds using CROWN\n",
    "            lb, ub = model_loss.compute_bounds(x=(x_bounded,ti,yi),IBP=False, method=\"backward\",bound_upper=True,bound_lower=False)\n",
    "            robust_loss = ub.sum()\n",
    "            loss = robust_loss\n",
    "        elif batch_method == \"natural\":\n",
    "            loss =  regular_loss\n",
    "        if train:\n",
    "            (.1*loss+0.9*regular_loss).backward()\n",
    "            eps_scheduler.update_loss(loss.item() - regular_loss.item())\n",
    "            opt.step()\n",
    "        meter.update('Loss', loss.item(), xi.size(0))\n",
    "        if batch_method != \"natural\":\n",
    "            meter.update('Robust_Loss', robust_loss.item(), xi.size(0))\n",
    "\n",
    "        meter.update('Time', time.time() - start)\n",
    "        if i % 10 == 0 and train:\n",
    "            print('[{:2d}:{:4d}]: eps={:.8f} {}'.format(t, i, eps, meter))\n",
    "    print('[{:2d}:{:4d}]: eps={:.8f} {}'.format(t, i, eps, meter))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS(object):\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ARGS()\n",
    "args.verify=False\n",
    "args.device=\"cpu\"\n",
    "\n",
    "args.seed = 123\n",
    "\n",
    "args.eps=0.5\n",
    "args.norm=np.inf\n",
    "args.bound_type = \"CROWN-IBP\"\n",
    "args.num_epochs=num_epochs\n",
    "args.lr = lr\n",
    "args.scheduler_name = \"SmoothedScheduler\"\n",
    "# args.scheduler_opts = \"start=5,length=10\"\n",
    "args.scheduler_opts = \"start=5,length=10\"\n",
    "\n",
    "args.save_model = \"apple.pth\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_fragile = Exponential_Model(input_dim=input_dims,hidden_layers=hidden_dim,output_dim=output_dim)\n",
    "clf_fragile.load_state_dict(clf.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BoundedModule(clf, X_train)\n",
    "loss_clf = RightCensorWrapper(clf)\n",
    "X_train,T_train,E_train = dataloader_train.dataset.tensors\n",
    "model_loss = BoundedModule(loss_clf, (X_train,T_train,E_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "## Step 4 prepare optimizer, epsilon scheduler and learning rate scheduler\n",
    "opt = optim.Adam(model_loss.parameters(), lr=args.lr)\n",
    "norm = float(args.norm)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.5)\n",
    "eps_scheduler = eval(args.scheduler_name)(args.eps, args.scheduler_opts)\n",
    "print(\"Model structure: \\n\", str(clf))\n",
    "\n",
    "## Step 5: start training\n",
    "if args.verify:\n",
    "    eps_scheduler = FixedScheduler(args.eps)\n",
    "    with torch.no_grad():\n",
    "        Train(model, 1, test_data, eps_scheduler, norm, False, None, args.bound_type)\n",
    "else:\n",
    "    timer = 0.0\n",
    "    for t in range(1, args.num_epochs+1):\n",
    "        if eps_scheduler.reached_max_eps():\n",
    "            # Only decay learning rate after reaching the maximum eps\n",
    "            lr_scheduler.step()\n",
    "        print(\"Epoch {}, learning rate {}\".format(t, lr_scheduler.get_lr()))\n",
    "        start_time = time.time()\n",
    "        Train(model_loss, t, dataloader_train, eps_scheduler, norm, True, opt, args.bound_type)\n",
    "        epoch_time = time.time() - start_time\n",
    "        timer += epoch_time\n",
    "        print('Epoch time: {:.4f}, Total time: {:.4f}'.format(epoch_time, timer))\n",
    "        print(\"Evaluating...\")\n",
    "        with torch.no_grad():\n",
    "            Train(model_loss, t, dataloader_test, eps_scheduler, norm, False, None, args.bound_type)\n",
    "        torch.save({'state_dict': loss_clf.state_dict(), 'epoch': t}, args.save_model if args.save_model != \"\" else args.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_robust = Exponential_Model(input_dim=input_dims,hidden_layers=hidden_dim,output_dim=output_dim)\n",
    "clf_robust.load_state_dict({re.sub(\"model.\",\"\",key):value for key,value in model_loss.state_dict().items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_bound(clf,nominal_input,epsilon):\n",
    "    # Wrap the model with auto_LiRPA.\n",
    "    model = BoundedModule(clf, nominal_input)\n",
    "    # Define perturbation. Here we add Linf perturbation to input data.\n",
    "    ptb = PerturbationLpNorm(norm=np.inf, eps=torch.Tensor([epsilon]))\n",
    "    # Make the input a BoundedTensor with the pre-defined perturbation.\n",
    "    my_input = BoundedTensor(torch.Tensor(nominal_input), ptb)\n",
    "    # Regular forward propagation using BoundedTensor works as usual.\n",
    "    prediction = model(my_input)\n",
    "    # Compute LiRPA bounds using CROWN\n",
    "    lb, ub = model.compute_bounds(x=(my_input,), method=\"backward\")\n",
    "    \n",
    "    return lb,ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "X_train,T_train,E_train = dataloader_train.dataset.tensors\n",
    "t = torch.linspace(0,2*T_train.max(),10000)\n",
    "\n",
    "St_fragile_given_x = clf_fragile.survival_qdf(X_train,t).detach()\n",
    "St_given_x = clf.survival_qdf(X_train,t).detach()\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(durations=T_train,event_observed=E_train)\n",
    "St_kmf = kmf.predict(times=t.ravel().numpy())\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(1,2,figsize=(20,10))\n",
    "axes[0].plot(t,St_kmf,linewidth=3)\n",
    "axes[0].plot(t,St_fragile_given_x.mean(0),'k-',linewidth=3)\n",
    "axes[0].plot(t,St_given_x.mean(0),linewidth=3)\n",
    "\n",
    "epsilons = [0.5,0.1,0.07,0.05,0.03,0.01,0.001]\n",
    "for epsilon in epsilons:\n",
    "    lb,ub = lower_bound(clf_robust,X_train,epsilon)\n",
    "    St_lb = torch.exp(-ub*t).mean(0)\n",
    "    print(\"=\"*10 + \"@ eps={}\".format(epsilon) + \"=\"*10)\n",
    "    print(\"Train CI Unperturbed\",concordance_index(event_times=T_train,predicted_scores=-clf_robust.rate_logit(X_train).detach(),event_observed=E_train))\n",
    "    print(\"Train CI Pertubed\",concordance_index(event_times=T_train,predicted_scores=-ub.detach(),event_observed=E_train))\n",
    "    axes[0].plot(t,St_lb.detach(),'--')\n",
    "\n",
    "\n",
    "\n",
    "axes[0].set_ylabel(\"S(t)\"); axes[0].set_xlabel(\"Time\")\n",
    "axes[0].legend([\"Kaplan Meier Numerical\",\"Neural Network Nonrobust\",\"Neural Network Robust\"]+[f\"LB@{epsilon}\" for epsilon in epsilons])\n",
    "axes[0].set_title(\"Robust Train Population Survival Curves\")\n",
    "axes[0].set_ylim([0,1])\n",
    "\n",
    "axes[1].plot(t,St_kmf,linewidth=3)\n",
    "axes[1].plot(t,St_fragile_given_x.mean(0),'k-',linewidth=3)\n",
    "axes[1].plot(t,St_given_x.mean(0),linewidth=3)\n",
    "\n",
    "print(\"NON ROBUST\")\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    lb,ub = lower_bound(clf_fragile,X_train,epsilon)\n",
    "    St_lb = torch.exp(-ub*t).mean(0)\n",
    "    print(\"=\"*10 + \"@ eps={}\".format(epsilon) + \"=\"*10)\n",
    "    print(\"Train CI Unperturbed\",concordance_index(event_times=T_train,predicted_scores=-clf_fragile.rate_logit(X_train).detach(),event_observed=E_train))\n",
    "    print(\"Train CI Pertubed\",concordance_index(event_times=T_train,predicted_scores=-ub.detach(),event_observed=E_train))\n",
    "    axes[1].plot(t,St_lb.detach(),'--')\n",
    "\n",
    "\n",
    "\n",
    "axes[1].set_ylabel(\"S(t)\"); axes[1].set_xlabel(\"Time\")\n",
    "axes[1].legend([\"Kaplan Meier Numerical\",\"Neural Network Nonrobust\",\"Neural Network Robust\"]+[f\"LB@{epsilon}\" for epsilon in epsilons])\n",
    "axes[1].set_title(\"Nonrobust Train Population Survival Curves\")\n",
    "axes[1].set_ylim([0,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "X_test,T_test,E_test = dataloader_train.dataset.tensors\n",
    "t = torch.linspace(0,2*T_test.max(),10000)\n",
    "\n",
    "St_fragile_given_x = clf_fragile.survival_qdf(X_test,t).detach()\n",
    "St_given_x = clf.survival_qdf(X_test,t).detach()\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(durations=T_test,event_observed=E_test)\n",
    "St_kmf = kmf.predict(times=t.ravel().numpy())\n",
    "\n",
    "\n",
    "fig,axes = plt.subplots(1,2,figsize=(20,10))\n",
    "axes[0].plot(t,St_kmf,linewidth=3)\n",
    "axes[0].plot(t,St_fragile_given_x.mean(0),'k-',linewidth=3)\n",
    "axes[0].plot(t,St_given_x.mean(0),linewidth=3)\n",
    "\n",
    "epsilons = [0.5,0.1,0.07,0.05,0.03,0.01,0.001]\n",
    "for epsilon in epsilons:\n",
    "    lb,ub = lower_bound(clf_robust,X_test,epsilon)\n",
    "    St_lb = torch.exp(-ub*t).mean(0)\n",
    "    print(\"=\"*10 + \"@ eps={}\".format(epsilon) + \"=\"*10)\n",
    "    print(\"Test CI Unperturbed\",concordance_index(event_times=T_test,predicted_scores=-clf_robust.rate_logit(X_test).detach(),event_observed=E_test))\n",
    "    print(\"Test CI Pertubed\",concordance_index(event_times=T_test,predicted_scores=-ub.detach(),event_observed=E_test))\n",
    "    axes[0].plot(t,St_lb.detach(),'--')\n",
    "\n",
    "\n",
    "\n",
    "axes[0].set_ylabel(\"S(t)\"); axes[0].set_xlabel(\"Time\")\n",
    "axes[0].legend([\"Kaplan Meier Numerical\",\"Neural Network Nonrobust\",\"Neural Network Robust\"]+[f\"LB@{epsilon}\" for epsilon in epsilons])\n",
    "axes[0].set_title(\"Robust Test Population Survival Curves\")\n",
    "axes[0].set_ylim([0,1])\n",
    "\n",
    "axes[1].plot(t,St_kmf,linewidth=3)\n",
    "axes[1].plot(t,St_fragile_given_x.mean(0),'k-',linewidth=3)\n",
    "axes[1].plot(t,St_given_x.mean(0),linewidth=3)\n",
    "\n",
    "print(\"NON ROBUST\")\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    lb,ub = lower_bound(clf_fragile,X_test,epsilon)\n",
    "    St_lb = torch.exp(-ub*t).mean(0)\n",
    "    print(\"=\"*10 + \"@ eps={}\".format(epsilon) + \"=\"*10)\n",
    "    print(\"Test CI Unperturbed\",concordance_index(event_times=T_test,predicted_scores=-clf_fragile.rate_logit(X_test).detach(),event_observed=E_test))\n",
    "    print(\"Test CI Pertubed\",concordance_index(event_times=T_test,predicted_scores=-ub.detach(),event_observed=E_test))\n",
    "    axes[1].plot(t,St_lb.detach(),'--')\n",
    "\n",
    "\n",
    "\n",
    "axes[1].set_ylabel(\"S(t)\"); axes[1].set_xlabel(\"Time\")\n",
    "axes[1].legend([\"Kaplan Meier Numerical\",\"Neural Network Nonrobust\",\"Neural Network Robust\"]+[f\"LB@{epsilon}\" for epsilon in epsilons])\n",
    "axes[1].set_title(\"Nonrobust Test Population Survival Curves\")\n",
    "axes[1].set_ylim([0,1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb,ub = lower_bound(clf_robust,X_test,0.1)\n",
    "St_lb = torch.exp(-ub*t).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "    \n",
    "test_cases = 30\n",
    "\n",
    "colors = list(plt.cm.brg(np.linspace(0,1,test_cases))) + [\"crimson\", \"indigo\"]\n",
    "\n",
    "cases = np.argsort(torch.linalg.norm(St_lb - St_given_x,axis=1))[0:test_cases]\n",
    "print(torch.linalg.norm(St_lb - St_given_x,axis=1)[cases])\n",
    "\n",
    "for i,case in enumerate(tqdm(cases)):\n",
    "    plt.plot(t,St_given_x[case],color=colors[i])\n",
    "    plt.plot(t,St_lb[case],'--',color=colors[i])\n",
    "    \n",
    "plt.ylabel(\"S(t)\"); plt.xlabel(\"Time\")\n",
    "plt.title(\"Individual Survival Curves Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "# lb,ub = lower_bound(model,X_train,0.1)\n",
    "    \n",
    "test_cases = 30\n",
    "cases = torch.flip(np.argsort(torch.linalg.norm(St_lb - St_given_x,axis=1)),dims=(0,))[0:test_cases]\n",
    "print(torch.linalg.norm(St_lb - St_given_x,axis=1)[cases])\n",
    "for i,case in enumerate(tqdm(cases)):\n",
    "    plt.plot(t,St_given_x[case],color=colors[i])\n",
    "    plt.plot(t,St_lb[case],'--',color=colors[i])\n",
    "    \n",
    "plt.ylabel(\"S(t)\"); plt.xlabel(\"Time\")\n",
    "plt.title(\"Individual Survival Curves Train\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "survival"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
