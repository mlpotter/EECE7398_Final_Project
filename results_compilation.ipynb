{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e065a4a9-2131-476d-8da5-02a43c1744e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1331ab-1ded-4f58-8250-001c66080009",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = \"fgsm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb9791e-0068-47aa-8d82-03cd236a5a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = osp.join(os.getcwd(),\"results\")\n",
    "attack_folder =  f\"attack_{attack}\"\n",
    "seeds = [str(i*111) for i in range(1,6)]\n",
    "results_folder = os.path.join(base_path, attack_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd347dbe-1270-4945-b60b-32c5739eb660",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\"draft\",\"noise\",\"fgsm\",\"pgd\",\"aae\",\"crownibp\"]\n",
    "exclude_datasets = [\"Dialysis\",\"divorce\",\"Pbc3\",\"vlbw\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d10bf9-8e60-4753-98ea-7108bd93cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI , IBS , NegLL\n",
    "metric = \"CI\"\n",
    "\n",
    "ascending = False if metric==\"CI\" else True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a785e79-53da-432e-99ca-6829fba73ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metric_excel_paths(base_path,metric):\n",
    "    # Use glob to match all CI.xlsx files in seed_{number} folders\n",
    "    pattern = os.path.join(base_path, \"results_*\", \"*\", \"seed_*\", f\"{metric}.xlsx\")\n",
    "    metric_excel_paths = glob.glob(pattern)\n",
    "    return metric_excel_paths\n",
    "\n",
    "def read_metric_data(file_path):\n",
    "    # Read the CI.xlsx file and extract epsilon and CI values\n",
    "    df = pd.read_excel(file_path)\n",
    "    if df.shape[1] != 2:\n",
    "        raise ValueError(f\"Unexpected format in {file_path}. Expected exactly two columns.\")\n",
    "\n",
    "    metric_name = df.columns[-1]\n",
    "    df.columns = ['epsilon', metric_name]\n",
    "    return df\n",
    "\n",
    "def extract_metadata_from_path(path):\n",
    "    # Extract dataset, algorithm, and attack method from the path\n",
    "    parts = path.split(os.sep)\n",
    "    attack_method = parts[-5].replace(\"attack_\", \"\")\n",
    "    algorithm = parts[-4].replace(\"results_\", \"\")\n",
    "    dataset = parts[-3]\n",
    "    seed = int(parts[-2].replace(\"seed_\", \"\"))\n",
    "    return dataset, algorithm, attack_method, seed\n",
    "\n",
    "def create_aggregated_dataframe(base_path,metric=\"CI\"):\n",
    "    # Get all CI.xlsx file paths\n",
    "    ci_excel_files = get_metric_excel_paths(base_path,metric)\n",
    "    \n",
    "    # Dictionary to store dataframes by (dataset, algorithm) keys\n",
    "    data_dict = {}\n",
    "    \n",
    "    # Process each CI.xlsx file\n",
    "    for file_path in ci_excel_files:\n",
    "        dataset, algorithm, attack_method, seed = extract_metadata_from_path(file_path)\n",
    "        metric_data = read_metric_data(file_path)\n",
    "        \n",
    "        # Use (dataset, algorithm) as key\n",
    "        key = (dataset, algorithm)\n",
    "        \n",
    "        # Initialize list for the key if not present\n",
    "        if key not in data_dict:\n",
    "            data_dict[key] = []\n",
    "        \n",
    "        # Append CI data to the list for that key\n",
    "        data_dict[key].append(metric_data.set_index('epsilon'))\n",
    "    \n",
    "    # Dictionary to store aggregated dataframes\n",
    "    aggregated_data = {}\n",
    "    \n",
    "    # Aggregate by dataset and algorithm\n",
    "    for (dataset, algorithm), dfs in data_dict.items():\n",
    "        # Concatenate along the columns to align by epsilon values and compute mean\n",
    "        concatenated_df = pd.concat(dfs, axis=1)\n",
    "        aggregated_df = concatenated_df.mean(axis=1).to_frame(name=(dataset, algorithm))\n",
    "        aggregated_data[(dataset, algorithm)] = aggregated_df\n",
    "    \n",
    "    # Combine all aggregated dataframes into a single dataframe with multi-index columns\n",
    "    final_df = pd.concat(aggregated_data.values(), axis=1)\n",
    "\n",
    "    # Sort the columns by dataset and then by algorithm for a clean MultiIndex\n",
    "    final_df = final_df.sort_index(axis=1, level=[0, 1])\n",
    "\n",
    "    # Create a MultiIndex for the columns with levels: dataset and algorithm\n",
    "    columns = pd.MultiIndex.from_tuples(final_df.columns, names=['Dataset', 'Algorithm'])\n",
    "    final_df.columns = columns\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210cbd95-f476-4a38-92c8-44d347af2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = create_aggregated_dataframe(results_folder,metric).reindex(columns=algorithms, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7880281-072a-4010-96b9-cb3deb550f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\"draft\":\"DRAFT\",\"noise\":\"Noise\",\"fgsm\":\"FGSM\",\"pgd\":\"PGD\",\"aae\":\"AAE-Cox\",\"crownibp\":\"SAWAR\"}\n",
    "algorithms_renamed = list(rename_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c0226-58e0-4cd4-958a-21428240a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.rename(columns=rename_dict, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888884ba-db8a-47fc-bbac-c5706f234673",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff9525e-1352-4c49-b084-735f2787caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[['zinc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af346d05-afc8-4d5e-8378-3f59e2272b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df==''] = np.NaN\n",
    "final_df = final_df.astype(float)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0affa729-5d3e-450c-a430-6dd5d5ecaa15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.applymap(np.isnan).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1324d-b8e8-41dc-ba5e-a645bcf2a2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_per_dataset_rank = final_df.groupby(level=0,axis=1).rank(axis=1,na_option='bottom',method=\"average\",ascending=ascending).reindex(columns=algorithms_renamed, level=1)\n",
    "best_per_dataset_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6704e3a-5f62-4994-bc88-991e14b8d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_per_dataset_rank.stack(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5423e-ed95-4861-86e5-a7d3e29f1d8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_per_dataset_avg_rank = best_per_dataset_rank.stack(level=1).mean(1).unstack(1).sort_values(by=\"epsilon\",ascending=False)\n",
    "best_per_dataset_avg_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcd1ec8-1873-423c-9a1d-71338024672b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not ascending:\n",
    "    best_per_dataset = final_df.groupby(level=0,axis=1).idxmax(1)\n",
    "else:\n",
    "    best_per_dataset = final_df.groupby(level=0,axis=1).idxmin(1)\n",
    "best_per_dataset.applymap(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84501a-a06c-4cbd-b2c0-120a7cedb706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percent_change_from_draft(df):\n",
    "    \"\"\"\n",
    "    Calculate the percent change from the 'draft' method for each dataset and algorithm in the given DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): MultiIndex DataFrame where level 0 is 'Dataset' and level 1 is 'Algorithm'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with percent changes from the 'draft' method for each dataset and algorithm.\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to store the percent changes\n",
    "    percent_change_df = df.copy()\n",
    "\n",
    "    # Loop over each dataset in level 0 of the MultiIndex\n",
    "    for dataset in df.columns.levels[0]:\n",
    "        # Get the \"draft\" column for the current dataset\n",
    "        draft_column = df[(dataset, \"DRAFT\")]\n",
    "\n",
    "        # Calculate percent change for each algorithm relative to \"draft\"\n",
    "        for algorithm in df.columns.levels[1]:\n",
    "            # Skip the \"draft\" column itself as it is 0% change\n",
    "            if algorithm == \"DRAFT\":\n",
    "                continue\n",
    "\n",
    "            # Calculate percent change and update in the new DataFrame\n",
    "            percent_change_df[(dataset, algorithm)] = (\n",
    "                (df[(dataset, algorithm)] - draft_column) / draft_column\n",
    "            ) * 100  # Multiply by 100 to convert to percentage\n",
    "\n",
    "    # The \"draft\" column itself should be 0% change from itself\n",
    "    for dataset in df.columns.levels[0]:\n",
    "        percent_change_df[(dataset, \"DRAFT\")] = 0\n",
    "\n",
    "    return percent_change_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98c1e5-6df4-425f-8c7e-5f80e9e172a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_change_df = calculate_percent_change_from_draft(final_df).reindex(columns=algorithms_renamed, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e8b08-9e42-4ad3-bcb8-46e97987a507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "percentage_change_mean =  percent_change_df.stack(level=1).mean(1).unstack(1).sort_values(by=\"epsilon\",ascending=False)\n",
    "percentage_change_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aafa024-7d1a-4669-bc64-4e5d8956a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_name = os.path.join(results_folder,f\"{metric}_all.xlsx\")\n",
    "with pd.ExcelWriter(excel_name) as writer:  \n",
    "    final_df.applymap(lambda x: np.round(x,3)).to_excel(writer,sheet_name=metric)\n",
    "    best_per_dataset_rank.to_excel(writer,sheet_name=\"rank\")\n",
    "    best_per_dataset_avg_rank.to_excel(writer,sheet_name=\"average_rank\")\n",
    "    best_per_dataset.applymap(lambda x: x[1]).to_excel(writer,sheet_name=\"best\")\n",
    "    percentage_change_mean.to_excel(writer,sheet_name=\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b0ed20-b49b-4eb3-9b39-8f4461207f01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_per_dataset_avg_rank.index = [\"{:.2f}\".format(float(x)) for x in np.round(best_per_dataset_avg_rank.index.tolist(),2)]\n",
    "best_per_dataset_avg_rank.index.name = r\"$\\epsilon$\"\n",
    "print(best_per_dataset_avg_rank.applymap(lambda x: str(np.round(x,2))).to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b35045-e7b7-4acb-b875-9f69878dd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_per_dataset_avg_rank.applymap(lambda x: str(np.round(x,2))).T.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5596d7-b858-4c23-9a08-e55ea6253690",
   "metadata": {},
   "outputs": [],
   "source": [
    "if metric == \"NegLL\":\n",
    "    final_df.index = [\"{:.2f}\".format(float(x)) for x in np.round(final_df.index.tolist(),2)]\n",
    "    final_df.index.name = r\"$\\epsilon$\"\n",
    "    print(final_df.applymap(lambda x: np.round(x,3)).applymap(lambda x: \"{:.2e}\".format(x)).to_latex(index=True,multicolumn_format=\"c\"))\n",
    "else:\n",
    "    final_df.index = [\"{:.2f}\".format(float(x)) for x in np.round(final_df.index.tolist(),2)]\n",
    "    final_df.index.name = r\"$\\epsilon$\"\n",
    "    print(final_df.applymap(lambda x: np.round(x,3)).applymap(str).to_latex(index=True,multicolumn_format=\"c\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82709563-3cca-48b1-813f-c7c6645ff89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = np.array(list(map(np.array,final_df.columns)))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a1845-2539-4d08-9678-3340e4a4d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a5c57a-575f-438f-bc10-a535042e90e9",
   "metadata": {},
   "source": [
    "## LONG TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b7428-767d-49a1-8c9e-d6fc4272b2ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4427e52-2b65-45c5-9b2c-04843fc5b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if metric == \"NegLL\":\n",
    "    print(final_df.T.applymap(lambda x: str(np.round(x,3))).applymap(lambda x: \"{:.2e}\".format(float(x))).to_latex(index=True,multicolumn_format=\"c\"))\n",
    "else:\n",
    "    print(final_df.T.applymap(lambda x: str(np.round(x,3))).to_latex(index=True,multicolumn_format=\"c\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f36df45-1687-42cb-8093-cd21437389e0",
   "metadata": {},
   "source": [
    "## PDF PICTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8312dafc-d6c1-4c21-b40b-7b00e1c6337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas\n",
    "import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1cabd4-d335-4a6b-b81b-28131031b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_excel_data(root_folder,excel_name,seed_key):\n",
    "    \"\"\"\n",
    "    Scrape data from Excel sheets in a nested folder structure into a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        root_folder (str): Root folder containing the data.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Nested dictionary where keys are datasets, sub-keys are algorithms,\n",
    "              and sub-sub-keys are seeds, with values being DataFrames from the Excel sheets.\n",
    "    \"\"\"\n",
    "    # Initialize the nested dictionary\n",
    "    data_dict = {}\n",
    "\n",
    "    # Traverse the root folder\n",
    "    \n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(root_folder):\n",
    "        # Traverse the results folder for each attack method\n",
    "        for algorithm_folder in tqdm(os.listdir(root_folder)):\n",
    "            algorithm_path = os.path.join(root_folder, algorithm_folder)\n",
    "\n",
    "            # Ensure it's a directory\n",
    "            if os.path.isdir(algorithm_path):\n",
    "                # Traverse the dataset folder for each algorithm\n",
    "                for dataset_folder in os.listdir(algorithm_path):\n",
    "                    dataset_path = os.path.join(algorithm_path, dataset_folder)\n",
    "\n",
    "                    # Ensure it's a directory\n",
    "                    if os.path.isdir(dataset_path):\n",
    "                        # Initialize sub-dictionary for each dataset\n",
    "                        if dataset_folder not in data_dict:\n",
    "                            data_dict[dataset_folder] = {}\n",
    "\n",
    "                        # Initialize sub-dictionary for each algorithm within the dataset\n",
    "                        if algorithm_folder not in data_dict[dataset_folder]:\n",
    "                            algo = algorithm_folder.split(\"_\")[-1]\n",
    "\n",
    "                            data_dict[dataset_folder][algo] = {}\n",
    "\n",
    "                        # Traverse the seed folders\n",
    "                        for seed_folder in os.listdir(dataset_path):\n",
    "                            if seed_key not in seed_folder:\n",
    "                                continue\n",
    "                            seed_path = os.path.join(dataset_path, seed_folder)\n",
    "\n",
    "                            # Ensure it's a directory and contains the Excel file\n",
    "                            if os.path.isdir(seed_path):\n",
    "                                excel_file_path = os.path.join(seed_path, excel_name)\n",
    "\n",
    "                                if os.path.exists(excel_file_path):\n",
    "                                    # Read the Excel file into a DataFrame\n",
    "                                    # print(excel_file_path)\n",
    "                                    df = pd.read_excel(excel_file_path,engine='openpyxl')\n",
    "\n",
    "                                    # Extract seed identifier from the folder name\n",
    "                                    seed_id = seed_folder.split('_')[-1]\n",
    "                                    \n",
    "                                    # Store the DataFrame in the nested dictionary\n",
    "                                    data_dict[dataset_folder][algo][seed_id] = df\n",
    "\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4114dd05-aaf3-4754-90c2-9f4009228565",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = \"fgsm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfad486-3fbe-4bff-965c-385944cbefb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_name = \"population_curves_attacked_test.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486ed03-ba73-4884-b75c-5d6f175a12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_interest = \"222\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc10cbdf-1a7b-41e7-9d5c-1c935d427675",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = osp.join(os.getcwd(),\"results\")\n",
    "attack_folder =  f\"attack_{attack}\"\n",
    "seeds = [str(i*111) for i in range(1,6)]\n",
    "results_folder = os.path.join(base_path, attack_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb25df-9f26-4032-88ff-daac0498d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = [\"draft\",\"noise\",\"fgsm\",\"pgd\",\"aae\",\"crownibp\"]\n",
    "exclude_datasets = [\"Dialysis\",\"divorce\",\"Pbc3\",\"vlbw\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48338de-8438-43d6-b542-931902559734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict = scrape_excel_data(results_folder,excel_name,seed_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b4fb45-437f-41a4-bd2d-dca843b3dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "save_folder = osp.join(cwd,\"results\",f\"attack_{attack}\",\"perturb_curves.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae4752-d436-48b5-8ace-2a484ba19bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(save_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccae906-f9d0-43f6-9623-0e0d49813748",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder\n",
    "\n",
    "n_rows = len(data_dict)\n",
    "n_cols = len(data_dict[\"stagec\"])\n",
    "\n",
    "rename_dict = {\"draft\":\"DRAFT\",\"noise\":\"Noise\",\"fgsm\":\"FGSM\",\"pgd\":\"PGD\",\"aae\":\"AAE-Cox\",\"crownibp\":\"SAWAR\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a06dbf9-d14f-46f6-818b-f96700acae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b37ce-32f4-4c5a-b27e-f59da29dbab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(n_rows,n_cols,figsize=(30,64),sharey=True)\n",
    "\n",
    "SMALL_SIZE = 80\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE//2)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE//2)  \n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)     # fontsize of the x and y labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=SMALL_SIZE)   # fontsize of the figure title\n",
    "\n",
    "\n",
    "for i,key_dataset in enumerate(data_dict.keys()):\n",
    "    for j,key_al in enumerate(algorithms):\n",
    "        print(i,j)\n",
    "\n",
    "\n",
    "        df_population = deepcopy(data_dict[key_dataset][key_al][seed_interest])\n",
    "\n",
    "        t = df_population.pop(\"t\")\n",
    "        base_models = df_population.iloc[:,:2]\n",
    "        base_models.columns = [col.split(\"_\")[0] for col in base_models.columns]\n",
    "        base_models = base_models.rename(columns={\"kmf\":\"KMC\",\"St\":\"NN\"})\n",
    "        \n",
    "        df_population= df_population.iloc[:,2:].iloc[:,-5:-1]\n",
    "        \n",
    "        df_population.columns = [\"$\\epsilon$={:.2f}\".format(eval(col.split(\"=\")[1])) for col in df_population.columns]\n",
    "\n",
    "        # print(base_models)\n",
    "        axes[i][j].plot(t,base_models.iloc[:,0],linewidth=3,c=\"b\")\n",
    "        axes[i][j].plot(t,base_models.iloc[:,1],linewidth=3,c=\"r\")\n",
    "\n",
    "  \n",
    "        perturb1 = axes[i][j].plot(t,df_population,'--',linewidth=3)\n",
    "  \n",
    "        if j == 0:\n",
    "            base1 = axes[i][j].plot(t,base_models.iloc[:,0],linewidth=3,c=\"b\")\n",
    "            base2 = axes[i][j].plot(t,base_models.iloc[:,1],linewidth=3,c=\"r\")\n",
    "            perturb = axes[i][j].plot(t,df_population,'--',linewidth=3)\n",
    "\n",
    "            axes[i][j].set_ylabel(f\"{key_dataset}\\n S(t)\" ,fontsize=SMALL_SIZE//1.5)\n",
    "            axes[i][j].set_xlabel(\"t\",fontsize=SMALL_SIZE//1.5)\n",
    "\n",
    "        axes[i][j].set_xlabel(\"t\",fontsize=SMALL_SIZE//1.5)\n",
    "\n",
    "for ax, col in zip(axes[0], algorithms):\n",
    "    col = \"SAWAR\" if col == \"crownibp\" else col\n",
    "    col = \"DRAFT\" if col == \"draft\" else col\n",
    "    col = \"PGD\" if col == \"pgd\" else col\n",
    "    col = \"FGSM\" if col == \"fgsm\" else col\n",
    "    col = \"Noise\" if col == \"noise\" else col\n",
    "    col = \"AAE-Cox\" if col == \"aae\" else col\n",
    "\n",
    "    ax.set_title(col,fontsize=SMALL_SIZE//1.5)\n",
    "\n",
    "labels = base_models.columns.tolist() + df_population.columns.tolist()\n",
    "# labels[labels.index(\"baseline\")] = \"non-robust\"\n",
    "\n",
    "fig.legend([base1, base2,perturb], labels=labels, \n",
    "           loc=\"upper center\",ncols=5,fontsize=30,bbox_to_anchor=(.5,1.06),prop={'size':SMALL_SIZE//1.7}) \n",
    "\n",
    "# axes[0][3].legend(base_models.columns.tolist() + robust_df.columns.tolist(),fontsize=20,ncol=2,loc=1)\n",
    "\n",
    "# plt.legend(base_models.columns.tolist() + robust_df.columns.tolist(),loc='upper center',ncol=5)\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(save_folder,dpi=1600,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8920be2-d97b-46f2-9c8f-5e954b8f9850",
   "metadata": {},
   "source": [
    "### DIST PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d0ea1-db71-4f38-8f33-583edbe94eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI , IBS , NegLL\n",
    "results_folder = fr\"results\\{attack}\"\n",
    "img_name = \"curve_distributions_test\"\n",
    "\n",
    " # aggregate all the CI files\n",
    "os.listdir(results_folder)\n",
    "excels = []\n",
    "for folder in os.listdir(results_folder):\n",
    "    glob_search = os.path.join(results_folder,folder,\"*\",f\"{img_name}.xlsx\")\n",
    "    excels.extend(glob.glob(glob_search))\n",
    "\n",
    "for dataset in exclude_datasets:\n",
    "    for exceli in excels:\n",
    "        if dataset in exceli:\n",
    "            print(\"remove \",dataset)\n",
    "            excels.remove(exceli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5d622-521a-4f21-9cfe-c1f795ec7027",
   "metadata": {},
   "outputs": [],
   "source": [
    "excels = np.sort(excels)\n",
    "files = pd.DataFrame(excels.reshape(-1,len(algorithms)-1,order=\"F\"),columns=[\"crownibp\",\"fgsm\",\"noise\",\"pgd\"]).reindex([\"noise\",\"fgsm\",\"pgd\",\"crownibp\"],axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c3dde-a972-4de7-b060-3bb189abd278",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = results_folder = os.path.join(r\"results\",attack,\"dist_curves.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e32a9f-39c6-48ab-8dc3-5a1e7fd4efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca484e11-5e3b-46a9-91e9-473f93095284",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(n_rows,n_cols+1,figsize=(30,64),sharey=True)\n",
    "\n",
    "SMALL_SIZE = 80\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE//2)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE//2)  \n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=SMALL_SIZE)     # fontsize of the x and y labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=SMALL_SIZE)   # fontsize of the figure title\n",
    "\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "        print(i,j)\n",
    "        fileij = files[i][j]\n",
    "        algo,dataset = fileij.split(\"\\\\\")[-3:-1]\n",
    "        algo = re.sub(\"results_\",\"\",algo)\n",
    "\n",
    "        df_population = pd.read_excel(fileij)\n",
    "\n",
    "        t = df_population.pop(\"t\")\n",
    "        base_models = df_population.iloc[:,:3]\n",
    "        base_models.columns = [col.split(\"_\")[0] for col in base_models.columns]\n",
    "        \n",
    "        robust_idx = [\"robust\" in col for col in df_population.columns]\n",
    "        baseline_idx = [\"baseline\" in col for col in df_population.columns]\n",
    "        robust_df = df_population.iloc[:,robust_idx]\n",
    "        baseline_df = df_population.iloc[:,baseline_idx]\n",
    "        \n",
    "        robust_df.columns = [col.split(\"_\")[1] for col in robust_df.columns]\n",
    "        baseline_df.columns =[col.split(\"_\")[1] for col in baseline_df.columns]\n",
    "        \n",
    "        mu = sns.lineplot(x=t, y=robust_df.iloc[:,0], label='Average S(t)', linewidth=3.0, ax=axes[i][j+1],c='b',legend=False)\n",
    "        q95 = sns.lineplot(x=t, y=robust_df.iloc[:,1], label='Confidence', linewidth=3.0, ax=axes[i][j+1],c='r',legend=False)\n",
    "        q05 = sns.lineplot(x=t, y=robust_df.iloc[:,2], label='Confidence', linewidth=3.0, ax=axes[i][j+1],c='r',legend=False)\n",
    "\n",
    "        line = q05.get_lines()\n",
    "        axes[i][j+1].fill_between(line[0].get_xdata(), line[1].get_ydata(), line[2].get_ydata(), color='blue', alpha=.3)\n",
    "        axes[i][j+1].set_xlabel(\"t\",fontsize=SMALL_SIZE//1.5)\n",
    "\n",
    "        if j == 0:\n",
    "            mu = sns.lineplot(x=t, y=baseline_df.iloc[:,0], label='Average S(t)', linewidth=3.0, ax=axes[i][j],c='b',legend=False)\n",
    "            q95 = sns.lineplot(x=t, y=baseline_df.iloc[:,1], label='Confidence', linewidth=3.0, ax=axes[i][j],c='r',legend=False)\n",
    "            q05 = sns.lineplot(x=t, y=baseline_df.iloc[:,2], label='Confidence', linewidth=3.0, ax=axes[i][j],c='r',legend=False)\n",
    "\n",
    "            axes[i][j].set_ylabel(f\"{dataset}\\n S(t)\" ,fontsize=SMALL_SIZE//1.5)\n",
    "            axes[i][j].set_xlabel(\"t\",fontsize=SMALL_SIZE//1.5)\n",
    "            line = q05.get_lines()\n",
    "            axes[i][j].fill_between(line[0].get_xdata(), line[1].get_ydata(), line[2].get_ydata(), color='blue', alpha=.3)\n",
    "            axes[i][j].set_xlabel(\"t\",fontsize=SMALL_SIZE//1.5)\n",
    "\n",
    "for ax, col in zip(axes[0], algorithms):\n",
    "    col = \"SAWAR\" if col == \"crownibp\" else col\n",
    "    col = \"DRAFT\" if col == \"baseline\" else col\n",
    "    col = \"PGD\" if col == \"pgd\" else col\n",
    "    col = \"FGSM\" if col == \"fgsm\" else col\n",
    "    col = \"Noise\" if col == \"noise\" else col\n",
    "    ax.set_title(col,fontsize=SMALL_SIZE//1.5)\n",
    "\n",
    "labels = [\"S(t)\",\"Credible Interval\",\"$Q_{95},Q_{05}$\"]\n",
    "fig.legend([mu,q95,q05], labels=labels, \n",
    "           loc=\"upper center\",ncols=4,fontsize=30,bbox_to_anchor=(.5,1.05),prop={'size':SMALL_SIZE}) \n",
    "\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(save_folder,dpi=1600,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a8c8a-cf4a-45b5-9aa3-4ccd0d26524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder = results_folder = os.path.join(r\"results\",attack,\"dist_curves_subset.pdf\")\n",
    "file_subset = files[[-7,-2,-1],:]\n",
    "\n",
    "fig,axes = plt.subplots(3,n_cols+1,figsize=(30,20),sharey=True)\n",
    "\n",
    "SMALL_SIZE = 20\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)  \n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(n_cols):\n",
    "        print(i,j)\n",
    "        fileij = file_subset[i][j]\n",
    "        algo,dataset = fileij.split(\"\\\\\")[-3:-1]\n",
    "        algo = re.sub(\"results_\",\"\",algo)\n",
    "\n",
    "        df_population = pd.read_excel(fileij)\n",
    "\n",
    "        t = df_population.pop(\"t\")\n",
    "        base_models = df_population.iloc[:,:3]\n",
    "        base_models.columns = [col.split(\"_\")[0] for col in base_models.columns]\n",
    "        \n",
    "        robust_idx = [\"robust\" in col for col in df_population.columns]\n",
    "        baseline_idx = [\"baseline\" in col for col in df_population.columns]\n",
    "        robust_df = df_population.iloc[:,robust_idx]\n",
    "        baseline_df = df_population.iloc[:,baseline_idx]\n",
    "        \n",
    "        robust_df.columns = [col.split(\"_\")[1] for col in robust_df.columns]\n",
    "        baseline_df.columns =[col.split(\"_\")[1] for col in baseline_df.columns]\n",
    "        \n",
    "        mu = sns.lineplot(x=t, y=robust_df.iloc[:,0], label='Average S(t)', linewidth=3.0, ax=axes[i][j+1],c='b',legend=False)\n",
    "        q95 = sns.lineplot(x=t, y=robust_df.iloc[:,1], label='Confidence', linewidth=3.0, ax=axes[i][j+1],c='r',legend=False)\n",
    "        q05 = sns.lineplot(x=t, y=robust_df.iloc[:,2], label='Confidence', linewidth=3.0, ax=axes[i][j+1],c='r',legend=False)\n",
    "        axes[i][j+1].set_xlabel(\"t\",fontsize=20)\n",
    "\n",
    "        line = q05.get_lines()\n",
    "        axes[i][j+1].fill_between(line[0].get_xdata(), line[1].get_ydata(), line[2].get_ydata(), color='blue', alpha=.3)\n",
    "        if j == 0:\n",
    "            mu = sns.lineplot(x=t, y=baseline_df.iloc[:,0], label='Average S(t)', linewidth=3.0, ax=axes[i][j],c='b',legend=False)\n",
    "            q95 = sns.lineplot(x=t, y=baseline_df.iloc[:,1], label='Confidence', linewidth=3.0, ax=axes[i][j],c='r',legend=False)\n",
    "            q05 = sns.lineplot(x=t, y=baseline_df.iloc[:,2], label='Confidence', linewidth=3.0, ax=axes[i][j],c='r',legend=False)\n",
    "\n",
    "            axes[i][j].set_ylabel(f\"S(t) {dataset}\" ,fontsize=30)\n",
    "            axes[i][j].set_xlabel(\"t\",fontsize=20)\n",
    "            line = q05.get_lines()\n",
    "            axes[i][j].fill_between(line[0].get_xdata(), line[1].get_ydata(), line[2].get_ydata(), color='blue', alpha=.3)\n",
    "            axes[i][j].set_xlabel(\"t\",fontsize=20)\n",
    "\n",
    "for ax, col in zip(axes[0], algorithms):\n",
    "    col = \"SAWAR\" if col == \"crownibp\" else col\n",
    "    col = \"DRAFT\" if col == \"baseline\" else col\n",
    "    col = \"PGD\" if col == \"pgd\" else col\n",
    "    col = \"FGSM\" if col == \"fgsm\" else col\n",
    "    col = \"Noise\" if col == \"noise\" else col\n",
    "    ax.set_title(col,fontsize=30)\n",
    "\n",
    "\n",
    "labels = [\"S(t)\",\"Credible Interval\",\"$Q_{95},Q_{05}$\"]\n",
    "fig.legend([mu,q95,q05], labels=labels, \n",
    "           loc=\"upper center\",ncols=4,fontsize=30,bbox_to_anchor=(.5,1.05)) \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_folder,dpi=1600,bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f5e5b-c45f-40fb-a888-6560f28a82c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "survival"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
