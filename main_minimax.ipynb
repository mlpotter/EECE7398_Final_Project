{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:37.188521Z",
     "start_time": "2023-11-07T20:21:28.626293Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpott\\anaconda3\\envs\\survival\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\lpott\\anaconda3\\envs\\survival\\lib\\site-packages\\torch\\utils\\cpp_extension.py:23: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  from pkg_resources import packaging  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "from src.models import Exponential_Model\n",
    "from src.criterion import right_censored,RightCensorWrapper,RankingWrapper\n",
    "from src.load_data import load_datasets,load_dataframe\n",
    "from src.utils import train_robust,lower_bound\n",
    "from src.visualizations import visualize_population_curves_attacked,visualize_individual_curves_attacked,visualize_individual_curves_changes,visualize_individual_lambda_histograms\n",
    "from src.metrics import concordance\n",
    "\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from lifelines import KaplanMeierFitter,CoxPHFitter,ExponentialFitter\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "from auto_LiRPA import BoundedModule, BoundedTensor\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import random\n",
    "# set seeds for random!!!\n",
    "torch.manual_seed(123)\n",
    "random.seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS(object):\n",
    "    def __init__(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ARGS()\n",
    "args.verify=False\n",
    "args.device=\"cpu\"\n",
    "\n",
    "args.seed = 123\n",
    "\n",
    "args.eps=0.5\n",
    "args.norm=np.inf\n",
    "args.bound_type = \"CROWN-IBP\"\n",
    "args.num_epochs=150\n",
    "args.lr = 1e-3\n",
    "args.batch_size= 32\n",
    "args.scheduler_name = \"SmoothedScheduler\"\n",
    "args.scheduler_opts = \"start=100,length=10\"\n",
    "args.hidden_dims = [50,50]\n",
    "args.pareto = [0.1,.9]\n",
    "args.save_model = \"\"\n",
    "args.dataset = \"TRACE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:37.272860Z",
     "start_time": "2023-11-07T20:21:37.188521Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# GOOD DATASETS\n",
    "# 1. TRACE\n",
    "# 2. divorce \n",
    "# 3. Dialysis\n",
    "# 3. Aids2\n",
    "# 5. Framingham\n",
    "# 6. rott2\n",
    "# 7. dataDIVAT1\n",
    "# 8. prostate\n",
    "dataset_train,dataset_test = load_datasets(args.dataset,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:37.295094Z",
     "start_time": "2023-11-07T20:21:37.272860Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input_dims = dataset_train.tensors[0].shape[1]\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:37.321189Z",
     "start_time": "2023-11-07T20:21:37.297057Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1502, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader_train = DataLoader(dataset_train,batch_size=args.batch_size,shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test,batch_size=args.batch_size,shuffle=False)\n",
    "\n",
    "dataloader_train.mean = dataloader_test.mean = dataset_train.mean\n",
    "dataloader_train.std = dataloader_test.std = dataset_train.std\n",
    "\n",
    "\n",
    "dataset_train.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_robust = Exponential_Model(input_dim=input_dims,hidden_layers=args.hidden_dims)\n",
    "clf_fragile = Exponential_Model(input_dim=input_dims,hidden_layers=args.hidden_dims)\n",
    "clf_fragile.load_state_dict(deepcopy(clf_robust.state_dict()))\n",
    "\n",
    "\n",
    "# # model = BoundedModule(clf, X_train)\n",
    "model_robust_wrap = BoundedModule(RightCensorWrapper(clf_robust),dataloader_train.dataset.tensors)\n",
    "model_fragile_wrap = BoundedModule(RightCensorWrapper(clf_fragile),dataloader_train.dataset.tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpott\\anaconda3\\envs\\survival\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:371: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, learning rate [0.001]\n",
      "[ 1:   0]: eps=0.00000000 Loss=164.2395 Time=0.0040\n",
      "[ 1:  10]: eps=0.00000000 Loss=123.6056 Time=0.0018\n",
      "[ 1:  20]: eps=0.00000000 Loss=113.0514 Time=0.0016\n",
      "[ 1:  30]: eps=0.00000000 Loss=101.5508 Time=0.0015\n",
      "[ 1:  40]: eps=0.00000000 Loss=92.2693 Time=0.0015\n",
      "[ 1:  46]: eps=0.00000000 Loss=87.8827 Time=0.0014\n",
      "Epoch time: 0.0779, Total time: 0.0779\n",
      "Evaluating...\n",
      "[ 1:  11]: eps=0.00000000 Loss=52.1146 Time=0.0010\n",
      "Epoch 2, learning rate [0.001]\n",
      "[ 2:   0]: eps=0.00000000 Loss=58.7290 Time=0.0021\n",
      "[ 2:  10]: eps=0.00000000 Loss=51.9201 Time=0.0013\n",
      "[ 2:  20]: eps=0.00000000 Loss=49.7441 Time=0.0014\n",
      "[ 2:  30]: eps=0.00000000 Loss=49.8742 Time=0.0014\n",
      "[ 2:  40]: eps=0.00000000 Loss=49.4224 Time=0.0014\n",
      "[ 2:  46]: eps=0.00000000 Loss=48.8536 Time=0.0014\n",
      "Epoch time: 0.0742, Total time: 0.1521\n",
      "Evaluating...\n",
      "[ 2:  11]: eps=0.00000000 Loss=45.0940 Time=0.0006\n",
      "Epoch 3, learning rate [0.001]\n",
      "[ 3:   0]: eps=0.00000000 Loss=41.7855 Time=0.0010\n",
      "[ 3:  10]: eps=0.00000000 Loss=45.4535 Time=0.0014\n",
      "[ 3:  20]: eps=0.00000000 Loss=46.9103 Time=0.0013\n",
      "[ 3:  30]: eps=0.00000000 Loss=46.3857 Time=0.0014\n",
      "[ 3:  40]: eps=0.00000000 Loss=46.7591 Time=0.0015\n",
      "[ 3:  46]: eps=0.00000000 Loss=46.3760 Time=0.0015\n",
      "Epoch time: 0.0784, Total time: 0.2304\n",
      "Evaluating...\n",
      "[ 3:  11]: eps=0.00000000 Loss=44.0959 Time=0.0010\n",
      "Epoch 4, learning rate [0.001]\n",
      "[ 4:   0]: eps=0.00000000 Loss=42.8839 Time=0.0020\n",
      "[ 4:  10]: eps=0.00000000 Loss=44.9383 Time=0.0012\n",
      "[ 4:  20]: eps=0.00000000 Loss=44.2204 Time=0.0013\n",
      "[ 4:  30]: eps=0.00000000 Loss=46.0338 Time=0.0014\n",
      "[ 4:  40]: eps=0.00000000 Loss=46.6363 Time=0.0013\n",
      "[ 4:  46]: eps=0.00000000 Loss=45.9480 Time=0.0014\n",
      "Epoch time: 0.0759, Total time: 0.3063\n",
      "Evaluating...\n",
      "[ 4:  11]: eps=0.00000000 Loss=43.6961 Time=0.0011\n",
      "Epoch 5, learning rate [0.001]\n",
      "[ 5:   0]: eps=0.00000000 Loss=34.8542 Time=0.0020\n",
      "[ 5:  10]: eps=0.00000000 Loss=41.8430 Time=0.0016\n",
      "[ 5:  20]: eps=0.00000000 Loss=43.8633 Time=0.0015\n",
      "[ 5:  30]: eps=0.00000000 Loss=45.4943 Time=0.0015\n",
      "[ 5:  40]: eps=0.00000000 Loss=45.6133 Time=0.0015\n",
      "[ 5:  46]: eps=0.00000000 Loss=45.5981 Time=0.0015\n",
      "Epoch time: 0.0752, Total time: 0.3816\n",
      "Evaluating...\n",
      "[ 5:  11]: eps=0.00000000 Loss=43.2884 Time=0.0004\n",
      "Epoch 6, learning rate [0.001]\n",
      "[ 6:   0]: eps=0.00000000 Loss=45.3757 Time=0.0011\n",
      "[ 6:  10]: eps=0.00000000 Loss=46.0872 Time=0.0014\n",
      "[ 6:  20]: eps=0.00000000 Loss=46.4677 Time=0.0015\n",
      "[ 6:  30]: eps=0.00000000 Loss=46.0103 Time=0.0017\n",
      "[ 6:  40]: eps=0.00000000 Loss=45.4015 Time=0.0020\n",
      "[ 6:  46]: eps=0.00000000 Loss=45.4067 Time=0.0021\n",
      "Epoch time: 0.1084, Total time: 0.4900\n",
      "Evaluating...\n",
      "[ 6:  11]: eps=0.00000000 Loss=43.1876 Time=0.0011\n",
      "Epoch 7, learning rate [0.001]\n",
      "[ 7:   0]: eps=0.00000000 Loss=42.1487 Time=0.0020\n",
      "[ 7:  10]: eps=0.00000000 Loss=46.1135 Time=0.0017\n",
      "[ 7:  20]: eps=0.00000000 Loss=45.0776 Time=0.0019\n",
      "[ 7:  30]: eps=0.00000000 Loss=45.1572 Time=0.0019\n",
      "[ 7:  40]: eps=0.00000000 Loss=45.1143 Time=0.0018\n",
      "[ 7:  46]: eps=0.00000000 Loss=45.2202 Time=0.0019\n",
      "Epoch time: 0.1000, Total time: 0.5900\n",
      "Evaluating...\n",
      "[ 7:  11]: eps=0.00000000 Loss=43.0649 Time=0.0014\n",
      "Epoch 8, learning rate [0.001]\n",
      "[ 8:   0]: eps=0.00000000 Loss=41.2913 Time=0.0020\n",
      "[ 8:  10]: eps=0.00000000 Loss=45.8097 Time=0.0018\n",
      "[ 8:  20]: eps=0.00000000 Loss=45.2427 Time=0.0020\n",
      "[ 8:  30]: eps=0.00000000 Loss=44.7454 Time=0.0018\n",
      "[ 8:  40]: eps=0.00000000 Loss=44.9231 Time=0.0019\n",
      "[ 8:  46]: eps=0.00000000 Loss=45.1799 Time=0.0019\n",
      "Epoch time: 0.1009, Total time: 0.6908\n",
      "Evaluating...\n",
      "[ 8:  11]: eps=0.00000000 Loss=42.7897 Time=0.0012\n",
      "Epoch 9, learning rate [0.001]\n",
      "[ 9:   0]: eps=0.00000000 Loss=48.2383 Time=0.0062\n",
      "[ 9:  10]: eps=0.00000000 Loss=45.2263 Time=0.0024\n",
      "[ 9:  20]: eps=0.00000000 Loss=45.6926 Time=0.0020\n",
      "[ 9:  30]: eps=0.00000000 Loss=45.0170 Time=0.0020\n",
      "[ 9:  40]: eps=0.00000000 Loss=45.4177 Time=0.0021\n",
      "[ 9:  46]: eps=0.00000000 Loss=45.0267 Time=0.0020\n",
      "Epoch time: 0.1044, Total time: 0.7953\n",
      "Evaluating...\n",
      "[ 9:  11]: eps=0.00000000 Loss=42.8718 Time=0.0014\n",
      "Epoch 10, learning rate [0.001]\n",
      "[10:   0]: eps=0.00000000 Loss=43.7439 Time=0.0020\n",
      "[10:  10]: eps=0.00000000 Loss=42.9871 Time=0.0024\n",
      "[10:  20]: eps=0.00000000 Loss=44.5407 Time=0.0023\n",
      "[10:  30]: eps=0.00000000 Loss=45.0956 Time=0.0021\n",
      "[10:  40]: eps=0.00000000 Loss=45.0714 Time=0.0020\n",
      "[10:  46]: eps=0.00000000 Loss=44.9014 Time=0.0020\n",
      "Epoch time: 0.1059, Total time: 0.9012\n",
      "Evaluating...\n",
      "[10:  11]: eps=0.00000000 Loss=42.4298 Time=0.0022\n",
      "Epoch 11, learning rate [0.001]\n",
      "[11:   0]: eps=0.00000000 Loss=41.7041 Time=0.0031\n",
      "[11:  10]: eps=0.00000000 Loss=41.8175 Time=0.0032\n",
      "[11:  20]: eps=0.00000000 Loss=43.1833 Time=0.0028\n",
      "[11:  30]: eps=0.00000000 Loss=44.7491 Time=0.0024\n",
      "[11:  40]: eps=0.00000000 Loss=44.8854 Time=0.0023\n",
      "[11:  46]: eps=0.00000000 Loss=44.8422 Time=0.0022\n",
      "Epoch time: 0.1111, Total time: 1.0123\n",
      "Evaluating...\n",
      "[11:  11]: eps=0.00000000 Loss=42.5994 Time=0.0011\n",
      "Epoch 12, learning rate [0.001]\n",
      "[12:   0]: eps=0.00000000 Loss=39.4889 Time=0.0020\n",
      "[12:  10]: eps=0.00000000 Loss=45.3309 Time=0.0025\n",
      "[12:  20]: eps=0.00000000 Loss=45.0137 Time=0.0020\n",
      "[12:  30]: eps=0.00000000 Loss=45.4598 Time=0.0025\n",
      "[12:  40]: eps=0.00000000 Loss=44.7780 Time=0.0026\n",
      "[12:  46]: eps=0.00000000 Loss=44.7699 Time=0.0024\n",
      "Epoch time: 0.1290, Total time: 1.1413\n",
      "Evaluating...\n",
      "[12:  11]: eps=0.00000000 Loss=42.7963 Time=0.0019\n",
      "Epoch 13, learning rate [0.001]\n",
      "[13:   0]: eps=0.00000000 Loss=46.4945 Time=0.0020\n",
      "[13:  10]: eps=0.00000000 Loss=42.8877 Time=0.0029\n",
      "[13:  20]: eps=0.00000000 Loss=44.2315 Time=0.0031\n",
      "[13:  30]: eps=0.00000000 Loss=44.7452 Time=0.0027\n",
      "[13:  40]: eps=0.00000000 Loss=44.6603 Time=0.0026\n",
      "[13:  46]: eps=0.00000000 Loss=44.6605 Time=0.0026\n",
      "Epoch time: 0.1326, Total time: 1.2739\n",
      "Evaluating...\n",
      "[13:  11]: eps=0.00000000 Loss=42.4940 Time=0.0019\n",
      "Epoch 14, learning rate [0.001]\n",
      "[14:   0]: eps=0.00000000 Loss=38.3629 Time=0.0030\n",
      "[14:  10]: eps=0.00000000 Loss=43.4842 Time=0.0019\n",
      "[14:  20]: eps=0.00000000 Loss=43.1219 Time=0.0020\n",
      "[14:  30]: eps=0.00000000 Loss=44.2314 Time=0.0021\n",
      "[14:  40]: eps=0.00000000 Loss=44.0693 Time=0.0025\n",
      "[14:  46]: eps=0.00000000 Loss=44.6531 Time=0.0025\n",
      "Epoch time: 0.1328, Total time: 1.4067\n",
      "Evaluating...\n",
      "[14:  11]: eps=0.00000000 Loss=42.6151 Time=0.0010\n",
      "Epoch 15, learning rate [0.001]\n",
      "[15:   0]: eps=0.00000000 Loss=41.3502 Time=0.0010\n",
      "[15:  10]: eps=0.00000000 Loss=45.0453 Time=0.0019\n",
      "[15:  20]: eps=0.00000000 Loss=44.3269 Time=0.0025\n",
      "[15:  30]: eps=0.00000000 Loss=44.6897 Time=0.0023\n",
      "[15:  40]: eps=0.00000000 Loss=44.6561 Time=0.0021\n",
      "[15:  46]: eps=0.00000000 Loss=44.6596 Time=0.0023\n",
      "Epoch time: 0.1194, Total time: 1.5261\n",
      "Evaluating...\n",
      "[15:  11]: eps=0.00000000 Loss=42.3834 Time=0.0016\n",
      "Epoch 16, learning rate [0.001]\n",
      "[16:   0]: eps=0.00000000 Loss=34.5446 Time=0.0030\n",
      "[16:  10]: eps=0.00000000 Loss=42.3718 Time=0.0028\n",
      "[16:  20]: eps=0.00000000 Loss=43.4352 Time=0.0023\n",
      "[16:  30]: eps=0.00000000 Loss=43.0592 Time=0.0024\n",
      "[16:  40]: eps=0.00000000 Loss=44.2451 Time=0.0023\n",
      "[16:  46]: eps=0.00000000 Loss=44.4847 Time=0.0023\n",
      "Epoch time: 0.1244, Total time: 1.6505\n",
      "Evaluating...\n",
      "[16:  11]: eps=0.00000000 Loss=42.4426 Time=0.0013\n",
      "Epoch 17, learning rate [0.001]\n",
      "[17:   0]: eps=0.00000000 Loss=36.4161 Time=0.0021\n",
      "[17:  10]: eps=0.00000000 Loss=40.9079 Time=0.0030\n",
      "[17:  20]: eps=0.00000000 Loss=43.7533 Time=0.0026\n",
      "[17:  30]: eps=0.00000000 Loss=44.7745 Time=0.0028\n",
      "[17:  40]: eps=0.00000000 Loss=44.3585 Time=0.0030\n",
      "[17:  46]: eps=0.00000000 Loss=44.3510 Time=0.0028\n",
      "Epoch time: 0.1457, Total time: 1.7963\n",
      "Evaluating...\n",
      "[17:  11]: eps=0.00000000 Loss=42.4253 Time=0.0015\n",
      "Epoch 18, learning rate [0.001]\n",
      "[18:   0]: eps=0.00000000 Loss=33.7383 Time=0.0020\n",
      "[18:  10]: eps=0.00000000 Loss=44.1821 Time=0.0023\n",
      "[18:  20]: eps=0.00000000 Loss=44.1937 Time=0.0023\n",
      "[18:  30]: eps=0.00000000 Loss=43.7585 Time=0.0026\n",
      "[18:  40]: eps=0.00000000 Loss=44.4004 Time=0.0024\n",
      "[18:  46]: eps=0.00000000 Loss=44.4612 Time=0.0024\n",
      "Epoch time: 0.1215, Total time: 1.9178\n",
      "Evaluating...\n",
      "[18:  11]: eps=0.00000000 Loss=42.1593 Time=0.0019\n",
      "Epoch 19, learning rate [0.001]\n",
      "[19:   0]: eps=0.00000000 Loss=53.4722 Time=0.0020\n",
      "[19:  10]: eps=0.00000000 Loss=45.2705 Time=0.0015\n",
      "[19:  20]: eps=0.00000000 Loss=44.9860 Time=0.0015\n",
      "[19:  30]: eps=0.00000000 Loss=44.4481 Time=0.0016\n",
      "[19:  40]: eps=0.00000000 Loss=44.0519 Time=0.0020\n",
      "[19:  46]: eps=0.00000000 Loss=44.3262 Time=0.0021\n",
      "Epoch time: 0.1116, Total time: 2.0294\n",
      "Evaluating...\n",
      "[19:  11]: eps=0.00000000 Loss=42.1469 Time=0.0021\n",
      "Epoch 20, learning rate [0.001]\n",
      "[20:   0]: eps=0.00000000 Loss=44.8974 Time=0.0025\n",
      "[20:  10]: eps=0.00000000 Loss=47.3866 Time=0.0019\n",
      "[20:  20]: eps=0.00000000 Loss=45.5680 Time=0.0019\n",
      "[20:  30]: eps=0.00000000 Loss=45.5388 Time=0.0019\n",
      "[20:  40]: eps=0.00000000 Loss=44.4386 Time=0.0022\n",
      "[20:  46]: eps=0.00000000 Loss=44.2969 Time=0.0021\n",
      "Epoch time: 0.1158, Total time: 2.1452\n",
      "Evaluating...\n",
      "[20:  11]: eps=0.00000000 Loss=41.9799 Time=0.0009\n",
      "Epoch 21, learning rate [0.001]\n",
      "[21:   0]: eps=0.00000000 Loss=46.2754 Time=0.0020\n",
      "[21:  10]: eps=0.00000000 Loss=42.5546 Time=0.0024\n",
      "[21:  20]: eps=0.00000000 Loss=43.2954 Time=0.0029\n",
      "[21:  30]: eps=0.00000000 Loss=43.6448 Time=0.0027\n",
      "[21:  40]: eps=0.00000000 Loss=43.7472 Time=0.0025\n",
      "[21:  46]: eps=0.00000000 Loss=44.3346 Time=0.0026\n",
      "Epoch time: 0.1435, Total time: 2.2887\n",
      "Evaluating...\n",
      "[21:  11]: eps=0.00000000 Loss=42.3112 Time=0.0011\n",
      "Epoch 22, learning rate [0.001]\n",
      "[22:   0]: eps=0.00000000 Loss=46.0149 Time=0.0059\n",
      "[22:  10]: eps=0.00000000 Loss=44.7728 Time=0.0035\n",
      "[22:  20]: eps=0.00000000 Loss=43.2971 Time=0.0030\n",
      "[22:  30]: eps=0.00000000 Loss=44.2062 Time=0.0026\n",
      "[22:  40]: eps=0.00000000 Loss=44.1896 Time=0.0024\n",
      "[22:  46]: eps=0.00000000 Loss=44.2317 Time=0.0025\n",
      "Epoch time: 0.1372, Total time: 2.4259\n",
      "Evaluating...\n",
      "[22:  11]: eps=0.00000000 Loss=42.3590 Time=0.0013\n",
      "Epoch 23, learning rate [0.001]\n",
      "[23:   0]: eps=0.00000000 Loss=50.5211 Time=0.0010\n",
      "[23:  10]: eps=0.00000000 Loss=40.9008 Time=0.0025\n",
      "[23:  20]: eps=0.00000000 Loss=43.7662 Time=0.0022\n",
      "[23:  30]: eps=0.00000000 Loss=44.2171 Time=0.0023\n",
      "[23:  40]: eps=0.00000000 Loss=44.8027 Time=0.0022\n",
      "[23:  46]: eps=0.00000000 Loss=44.2660 Time=0.0023\n",
      "Epoch time: 0.1206, Total time: 2.5465\n",
      "Evaluating...\n",
      "[23:  11]: eps=0.00000000 Loss=42.4648 Time=0.0013\n",
      "Epoch 24, learning rate [0.001]\n",
      "[24:   0]: eps=0.00000000 Loss=41.3274 Time=0.0010\n",
      "[24:  10]: eps=0.00000000 Loss=43.3438 Time=0.0014\n",
      "[24:  20]: eps=0.00000000 Loss=43.7314 Time=0.0016\n",
      "[24:  30]: eps=0.00000000 Loss=43.6021 Time=0.0018\n",
      "[24:  40]: eps=0.00000000 Loss=43.9095 Time=0.0019\n",
      "[24:  46]: eps=0.00000000 Loss=44.2321 Time=0.0019\n",
      "Epoch time: 0.1066, Total time: 2.6531\n",
      "Evaluating...\n",
      "[24:  11]: eps=0.00000000 Loss=42.3596 Time=0.0015\n",
      "Epoch 25, learning rate [0.001]\n",
      "[25:   0]: eps=0.00000000 Loss=31.6357 Time=0.0011\n",
      "[25:  10]: eps=0.00000000 Loss=41.1546 Time=0.0029\n",
      "[25:  20]: eps=0.00000000 Loss=42.9163 Time=0.0023\n",
      "[25:  30]: eps=0.00000000 Loss=43.6284 Time=0.0022\n",
      "[25:  40]: eps=0.00000000 Loss=43.8263 Time=0.0021\n",
      "[25:  46]: eps=0.00000000 Loss=44.0839 Time=0.0021\n",
      "Epoch time: 0.1107, Total time: 2.7638\n",
      "Evaluating...\n",
      "[25:  11]: eps=0.00000000 Loss=42.0851 Time=0.0012\n",
      "Epoch 26, learning rate [0.001]\n",
      "[26:   0]: eps=0.00000000 Loss=41.9805 Time=0.0010\n",
      "[26:  10]: eps=0.00000000 Loss=43.5443 Time=0.0022\n",
      "[26:  20]: eps=0.00000000 Loss=44.3831 Time=0.0023\n",
      "[26:  30]: eps=0.00000000 Loss=44.0794 Time=0.0022\n",
      "[26:  40]: eps=0.00000000 Loss=44.4279 Time=0.0025\n",
      "[26:  46]: eps=0.00000000 Loss=44.0909 Time=0.0025\n",
      "Epoch time: 0.1358, Total time: 2.8996\n",
      "Evaluating...\n",
      "[26:  11]: eps=0.00000000 Loss=42.2107 Time=0.0017\n",
      "Epoch 27, learning rate [0.001]\n",
      "[27:   0]: eps=0.00000000 Loss=46.3700 Time=0.0030\n",
      "[27:  10]: eps=0.00000000 Loss=42.7507 Time=0.0021\n",
      "[27:  20]: eps=0.00000000 Loss=44.6731 Time=0.0018\n",
      "[27:  30]: eps=0.00000000 Loss=45.2234 Time=0.0018\n",
      "[27:  40]: eps=0.00000000 Loss=44.2650 Time=0.0018\n",
      "[27:  46]: eps=0.00000000 Loss=44.0532 Time=0.0018\n",
      "Epoch time: 0.0957, Total time: 2.9953\n",
      "Evaluating...\n",
      "[27:  11]: eps=0.00000000 Loss=42.0896 Time=0.0009\n",
      "Epoch 28, learning rate [0.001]\n",
      "[28:   0]: eps=0.00000000 Loss=45.6054 Time=0.0030\n",
      "[28:  10]: eps=0.00000000 Loss=40.7832 Time=0.0023\n",
      "[28:  20]: eps=0.00000000 Loss=42.1924 Time=0.0020\n",
      "[28:  30]: eps=0.00000000 Loss=43.9182 Time=0.0018\n",
      "[28:  40]: eps=0.00000000 Loss=43.8152 Time=0.0018\n",
      "[28:  46]: eps=0.00000000 Loss=44.2335 Time=0.0018\n",
      "Epoch time: 0.0985, Total time: 3.0939\n",
      "Evaluating...\n",
      "[28:  11]: eps=0.00000000 Loss=42.3307 Time=0.0017\n",
      "Epoch 29, learning rate [0.001]\n",
      "[29:   0]: eps=0.00000000 Loss=41.5363 Time=0.0030\n",
      "[29:  10]: eps=0.00000000 Loss=42.8835 Time=0.0034\n",
      "[29:  20]: eps=0.00000000 Loss=45.0522 Time=0.0028\n",
      "[29:  30]: eps=0.00000000 Loss=43.7985 Time=0.0027\n",
      "[29:  40]: eps=0.00000000 Loss=44.0924 Time=0.0025\n",
      "[29:  46]: eps=0.00000000 Loss=44.0931 Time=0.0025\n",
      "Epoch time: 0.1302, Total time: 3.2240\n",
      "Evaluating...\n",
      "[29:  11]: eps=0.00000000 Loss=41.9151 Time=0.0017\n",
      "Epoch 30, learning rate [0.001]\n",
      "[30:   0]: eps=0.00000000 Loss=35.0465 Time=0.0020\n",
      "[30:  10]: eps=0.00000000 Loss=42.2952 Time=0.0026\n",
      "[30:  20]: eps=0.00000000 Loss=44.3073 Time=0.0028\n",
      "[30:  30]: eps=0.00000000 Loss=44.8254 Time=0.0029\n",
      "[30:  40]: eps=0.00000000 Loss=44.5759 Time=0.0027\n",
      "[30:  46]: eps=0.00000000 Loss=44.0592 Time=0.0026\n",
      "Epoch time: 0.1344, Total time: 3.3585\n",
      "Evaluating...\n",
      "[30:  11]: eps=0.00000000 Loss=42.2822 Time=0.0012\n",
      "Epoch 31, learning rate [0.001]\n",
      "[31:   0]: eps=0.00000000 Loss=47.1657 Time=0.0020\n",
      "[31:  10]: eps=0.00000000 Loss=45.5007 Time=0.0017\n",
      "[31:  20]: eps=0.00000000 Loss=44.9734 Time=0.0018\n",
      "[31:  30]: eps=0.00000000 Loss=44.4840 Time=0.0018\n",
      "[31:  40]: eps=0.00000000 Loss=44.3695 Time=0.0018\n",
      "[31:  46]: eps=0.00000000 Loss=44.0255 Time=0.0018\n",
      "Epoch time: 0.0943, Total time: 3.4527\n",
      "Evaluating...\n",
      "[31:  11]: eps=0.00000000 Loss=42.0827 Time=0.0014\n",
      "Epoch 32, learning rate [0.001]\n",
      "[32:   0]: eps=0.00000000 Loss=49.1614 Time=0.0019\n",
      "[32:  10]: eps=0.00000000 Loss=44.3186 Time=0.0021\n",
      "[32:  20]: eps=0.00000000 Loss=45.5023 Time=0.0024\n",
      "[32:  30]: eps=0.00000000 Loss=44.6466 Time=0.0023\n",
      "[32:  40]: eps=0.00000000 Loss=44.3979 Time=0.0022\n",
      "[32:  46]: eps=0.00000000 Loss=43.9742 Time=0.0023\n",
      "Epoch time: 0.1206, Total time: 3.5733\n",
      "Evaluating...\n",
      "[32:  11]: eps=0.00000000 Loss=42.2645 Time=0.0012\n",
      "Epoch 33, learning rate [0.001]\n",
      "[33:   0]: eps=0.00000000 Loss=54.1356 Time=0.0050\n",
      "[33:  10]: eps=0.00000000 Loss=43.2103 Time=0.0028\n",
      "[33:  20]: eps=0.00000000 Loss=44.5272 Time=0.0024\n",
      "[33:  30]: eps=0.00000000 Loss=44.9774 Time=0.0022\n",
      "[33:  40]: eps=0.00000000 Loss=45.0447 Time=0.0021\n",
      "[33:  46]: eps=0.00000000 Loss=43.9613 Time=0.0020\n",
      "Epoch time: 0.1037, Total time: 3.6770\n",
      "Evaluating...\n",
      "[33:  11]: eps=0.00000000 Loss=42.2572 Time=0.0011\n",
      "Epoch 34, learning rate [0.001]\n",
      "[34:   0]: eps=0.00000000 Loss=45.2417 Time=0.0020\n",
      "[34:  10]: eps=0.00000000 Loss=42.4506 Time=0.0016\n",
      "[34:  20]: eps=0.00000000 Loss=43.9694 Time=0.0017\n",
      "[34:  30]: eps=0.00000000 Loss=43.0139 Time=0.0017\n",
      "[34:  40]: eps=0.00000000 Loss=43.2126 Time=0.0017\n",
      "[34:  46]: eps=0.00000000 Loss=43.8453 Time=0.0017\n",
      "Epoch time: 0.0896, Total time: 3.7666\n",
      "Evaluating...\n",
      "[34:  11]: eps=0.00000000 Loss=42.0880 Time=0.0012\n",
      "Epoch 35, learning rate [0.001]\n",
      "[35:   0]: eps=0.00000000 Loss=43.9066 Time=0.0020\n",
      "[35:  10]: eps=0.00000000 Loss=45.3507 Time=0.0016\n",
      "[35:  20]: eps=0.00000000 Loss=44.8678 Time=0.0016\n",
      "[35:  30]: eps=0.00000000 Loss=44.7454 Time=0.0017\n",
      "[35:  40]: eps=0.00000000 Loss=44.3070 Time=0.0017\n",
      "[35:  46]: eps=0.00000000 Loss=43.9043 Time=0.0018\n",
      "Epoch time: 0.0931, Total time: 3.8597\n",
      "Evaluating...\n",
      "[35:  11]: eps=0.00000000 Loss=42.1587 Time=0.0011\n",
      "Epoch 36, learning rate [0.001]\n",
      "[36:   0]: eps=0.00000000 Loss=48.5242 Time=0.0020\n",
      "[36:  10]: eps=0.00000000 Loss=45.3102 Time=0.0019\n",
      "[36:  20]: eps=0.00000000 Loss=43.8298 Time=0.0020\n",
      "[36:  30]: eps=0.00000000 Loss=44.4881 Time=0.0022\n",
      "[36:  40]: eps=0.00000000 Loss=44.0330 Time=0.0022\n",
      "[36:  46]: eps=0.00000000 Loss=43.8764 Time=0.0022\n",
      "Epoch time: 0.1143, Total time: 3.9739\n",
      "Evaluating...\n",
      "[36:  11]: eps=0.00000000 Loss=42.2213 Time=0.0017\n",
      "Epoch 37, learning rate [0.001]\n",
      "[37:   0]: eps=0.00000000 Loss=54.2497 Time=0.0051\n",
      "[37:  10]: eps=0.00000000 Loss=43.9321 Time=0.0024\n",
      "[37:  20]: eps=0.00000000 Loss=44.7630 Time=0.0019\n",
      "[37:  30]: eps=0.00000000 Loss=44.5958 Time=0.0018\n",
      "[37:  40]: eps=0.00000000 Loss=44.1910 Time=0.0018\n",
      "[37:  46]: eps=0.00000000 Loss=43.7483 Time=0.0017\n",
      "Epoch time: 0.1018, Total time: 4.0757\n",
      "Evaluating...\n",
      "[37:  11]: eps=0.00000000 Loss=42.3440 Time=0.0012\n",
      "Epoch 38, learning rate [0.001]\n",
      "[38:   0]: eps=0.00000000 Loss=46.2655 Time=0.0010\n",
      "[38:  10]: eps=0.00000000 Loss=45.9237 Time=0.0016\n",
      "[38:  20]: eps=0.00000000 Loss=43.8913 Time=0.0017\n",
      "[38:  30]: eps=0.00000000 Loss=45.1029 Time=0.0016\n",
      "[38:  40]: eps=0.00000000 Loss=44.6971 Time=0.0016\n",
      "[38:  46]: eps=0.00000000 Loss=43.8618 Time=0.0016\n",
      "Epoch time: 0.0935, Total time: 4.1692\n",
      "Evaluating...\n",
      "[38:  11]: eps=0.00000000 Loss=42.2476 Time=0.0016\n",
      "Epoch 39, learning rate [0.001]\n",
      "[39:   0]: eps=0.00000000 Loss=46.7722 Time=0.0030\n",
      "[39:  10]: eps=0.00000000 Loss=43.8916 Time=0.0019\n",
      "[39:  20]: eps=0.00000000 Loss=44.5565 Time=0.0019\n",
      "[39:  30]: eps=0.00000000 Loss=44.1468 Time=0.0017\n",
      "[39:  40]: eps=0.00000000 Loss=43.8950 Time=0.0018\n",
      "[39:  46]: eps=0.00000000 Loss=43.7011 Time=0.0018\n",
      "Epoch time: 0.0936, Total time: 4.2628\n",
      "Evaluating...\n",
      "[39:  11]: eps=0.00000000 Loss=42.4888 Time=0.0010\n",
      "Epoch 40, learning rate [0.001]\n",
      "[40:   0]: eps=0.00000000 Loss=45.0650 Time=0.0026\n",
      "[40:  10]: eps=0.00000000 Loss=39.7520 Time=0.0024\n",
      "[40:  20]: eps=0.00000000 Loss=40.8589 Time=0.0029\n",
      "[40:  30]: eps=0.00000000 Loss=42.2473 Time=0.0026\n",
      "[40:  40]: eps=0.00000000 Loss=43.2043 Time=0.0027\n",
      "[40:  46]: eps=0.00000000 Loss=43.7797 Time=0.0026\n",
      "Epoch time: 0.1312, Total time: 4.3940\n",
      "Evaluating...\n",
      "[40:  11]: eps=0.00000000 Loss=42.3250 Time=0.0019\n",
      "Epoch 41, learning rate [0.001]\n",
      "[41:   0]: eps=0.00000000 Loss=43.2649 Time=0.0030\n",
      "[41:  10]: eps=0.00000000 Loss=45.2540 Time=0.0028\n",
      "[41:  20]: eps=0.00000000 Loss=44.1721 Time=0.0024\n",
      "[41:  30]: eps=0.00000000 Loss=43.7331 Time=0.0022\n",
      "[41:  40]: eps=0.00000000 Loss=44.1313 Time=0.0021\n",
      "[41:  46]: eps=0.00000000 Loss=43.7007 Time=0.0022\n",
      "Epoch time: 0.1083, Total time: 4.5022\n",
      "Evaluating...\n",
      "[41:  11]: eps=0.00000000 Loss=42.3749 Time=0.0012\n",
      "Epoch 42, learning rate [0.001]\n",
      "[42:   0]: eps=0.00000000 Loss=43.4521 Time=0.0020\n",
      "[42:  10]: eps=0.00000000 Loss=42.5278 Time=0.0022\n",
      "[42:  20]: eps=0.00000000 Loss=43.2387 Time=0.0021\n",
      "[42:  30]: eps=0.00000000 Loss=43.8083 Time=0.0022\n",
      "[42:  40]: eps=0.00000000 Loss=44.0194 Time=0.0021\n",
      "[42:  46]: eps=0.00000000 Loss=43.7263 Time=0.0023\n",
      "Epoch time: 0.1199, Total time: 4.6222\n",
      "Evaluating...\n",
      "[42:  11]: eps=0.00000000 Loss=42.4406 Time=0.0016\n",
      "Epoch 43, learning rate [0.001]\n",
      "[43:   0]: eps=0.00000000 Loss=48.3757 Time=0.0019\n",
      "[43:  10]: eps=0.00000000 Loss=44.8589 Time=0.0018\n",
      "[43:  20]: eps=0.00000000 Loss=44.6511 Time=0.0017\n",
      "[43:  30]: eps=0.00000000 Loss=43.6983 Time=0.0019\n",
      "[43:  40]: eps=0.00000000 Loss=43.9390 Time=0.0018\n",
      "[43:  46]: eps=0.00000000 Loss=43.6842 Time=0.0019\n",
      "Epoch time: 0.0991, Total time: 4.7213\n",
      "Evaluating...\n",
      "[43:  11]: eps=0.00000000 Loss=42.2518 Time=0.0013\n",
      "Epoch 44, learning rate [0.001]\n",
      "[44:   0]: eps=0.00000000 Loss=47.7659 Time=0.0020\n",
      "[44:  10]: eps=0.00000000 Loss=46.6600 Time=0.0017\n",
      "[44:  20]: eps=0.00000000 Loss=44.6835 Time=0.0025\n",
      "[44:  30]: eps=0.00000000 Loss=44.1794 Time=0.0025\n",
      "[44:  40]: eps=0.00000000 Loss=44.0804 Time=0.0024\n",
      "[44:  46]: eps=0.00000000 Loss=43.7054 Time=0.0023\n",
      "Epoch time: 0.1215, Total time: 4.8427\n",
      "Evaluating...\n",
      "[44:  11]: eps=0.00000000 Loss=42.4907 Time=0.0010\n",
      "Epoch 45, learning rate [0.001]\n",
      "[45:   0]: eps=0.00000000 Loss=45.6490 Time=0.0020\n",
      "[45:  10]: eps=0.00000000 Loss=44.0345 Time=0.0019\n",
      "[45:  20]: eps=0.00000000 Loss=44.0879 Time=0.0017\n",
      "[45:  30]: eps=0.00000000 Loss=43.5892 Time=0.0017\n",
      "[45:  40]: eps=0.00000000 Loss=43.5681 Time=0.0017\n",
      "[45:  46]: eps=0.00000000 Loss=43.6279 Time=0.0017\n",
      "Epoch time: 0.0873, Total time: 4.9300\n",
      "Evaluating...\n",
      "[45:  11]: eps=0.00000000 Loss=42.3434 Time=0.0012\n",
      "Epoch 46, learning rate [0.001]\n",
      "[46:   0]: eps=0.00000000 Loss=41.5804 Time=0.0010\n",
      "[46:  10]: eps=0.00000000 Loss=44.9018 Time=0.0019\n",
      "[46:  20]: eps=0.00000000 Loss=45.3903 Time=0.0020\n",
      "[46:  30]: eps=0.00000000 Loss=43.8538 Time=0.0018\n",
      "[46:  40]: eps=0.00000000 Loss=43.5066 Time=0.0020\n",
      "[46:  46]: eps=0.00000000 Loss=43.5653 Time=0.0019\n",
      "Epoch time: 0.1033, Total time: 5.0333\n",
      "Evaluating...\n",
      "[46:  11]: eps=0.00000000 Loss=42.7044 Time=0.0012\n",
      "Epoch 47, learning rate [0.001]\n",
      "[47:   0]: eps=0.00000000 Loss=48.2081 Time=0.0020\n",
      "[47:  10]: eps=0.00000000 Loss=42.4283 Time=0.0020\n",
      "[47:  20]: eps=0.00000000 Loss=43.7783 Time=0.0022\n",
      "[47:  30]: eps=0.00000000 Loss=43.2618 Time=0.0020\n",
      "[47:  40]: eps=0.00000000 Loss=43.6968 Time=0.0019\n",
      "[47:  46]: eps=0.00000000 Loss=43.5642 Time=0.0019\n",
      "Epoch time: 0.1006, Total time: 5.1338\n",
      "Evaluating...\n",
      "[47:  11]: eps=0.00000000 Loss=42.2813 Time=0.0016\n",
      "Epoch 48, learning rate [0.001]\n",
      "[48:   0]: eps=0.00000000 Loss=39.2241 Time=0.0020\n",
      "[48:  10]: eps=0.00000000 Loss=43.4010 Time=0.0035\n",
      "[48:  20]: eps=0.00000000 Loss=42.4870 Time=0.0029\n",
      "[48:  30]: eps=0.00000000 Loss=43.3452 Time=0.0026\n",
      "[48:  40]: eps=0.00000000 Loss=43.4412 Time=0.0024\n",
      "[48:  46]: eps=0.00000000 Loss=43.5738 Time=0.0024\n",
      "Epoch time: 0.1202, Total time: 5.2540\n",
      "Evaluating...\n",
      "[48:  11]: eps=0.00000000 Loss=42.3129 Time=0.0014\n",
      "Epoch 49, learning rate [0.001]\n",
      "[49:   0]: eps=0.00000000 Loss=37.7587 Time=0.0030\n",
      "[49:  10]: eps=0.00000000 Loss=44.6251 Time=0.0033\n",
      "[49:  20]: eps=0.00000000 Loss=43.2664 Time=0.0032\n",
      "[49:  30]: eps=0.00000000 Loss=43.7352 Time=0.0029\n",
      "[49:  40]: eps=0.00000000 Loss=43.4730 Time=0.0027\n",
      "[49:  46]: eps=0.00000000 Loss=43.6402 Time=0.0026\n",
      "Epoch time: 0.1387, Total time: 5.3927\n",
      "Evaluating...\n",
      "[49:  11]: eps=0.00000000 Loss=42.5396 Time=0.0021\n",
      "Epoch 50, learning rate [0.001]\n",
      "[50:   0]: eps=0.00000000 Loss=43.5644 Time=0.0050\n",
      "[50:  10]: eps=0.00000000 Loss=42.0835 Time=0.0027\n",
      "[50:  20]: eps=0.00000000 Loss=42.6664 Time=0.0025\n",
      "[50:  30]: eps=0.00000000 Loss=43.4512 Time=0.0022\n",
      "[50:  40]: eps=0.00000000 Loss=43.2137 Time=0.0023\n",
      "[50:  46]: eps=0.00000000 Loss=43.6109 Time=0.0023\n",
      "Epoch time: 0.1164, Total time: 5.5091\n",
      "Evaluating...\n",
      "[50:  11]: eps=0.00000000 Loss=42.6110 Time=0.0011\n",
      "Epoch 51, learning rate [0.001]\n",
      "[51:   0]: eps=0.00000000 Loss=40.3465 Time=0.0010\n",
      "[51:  10]: eps=0.00000000 Loss=43.6803 Time=0.0016\n",
      "[51:  20]: eps=0.00000000 Loss=44.4406 Time=0.0017\n",
      "[51:  30]: eps=0.00000000 Loss=44.0621 Time=0.0016\n",
      "[51:  40]: eps=0.00000000 Loss=43.9215 Time=0.0020\n",
      "[51:  46]: eps=0.00000000 Loss=43.5709 Time=0.0021\n",
      "Epoch time: 0.1136, Total time: 5.6227\n",
      "Evaluating...\n",
      "[51:  11]: eps=0.00000000 Loss=42.5777 Time=0.0011\n",
      "Epoch 52, learning rate [0.001]\n",
      "[52:   0]: eps=0.00000000 Loss=34.7203 Time=0.0021\n",
      "[52:  10]: eps=0.00000000 Loss=45.9139 Time=0.0020\n",
      "[52:  20]: eps=0.00000000 Loss=44.2725 Time=0.0020\n",
      "[52:  30]: eps=0.00000000 Loss=43.9207 Time=0.0022\n",
      "[52:  40]: eps=0.00000000 Loss=44.3162 Time=0.0023\n",
      "[52:  46]: eps=0.00000000 Loss=43.4983 Time=0.0022\n",
      "Epoch time: 0.1197, Total time: 5.7424\n",
      "Evaluating...\n",
      "[52:  11]: eps=0.00000000 Loss=42.6198 Time=0.0011\n",
      "Epoch 53, learning rate [0.001]\n",
      "[53:   0]: eps=0.00000000 Loss=43.2586 Time=0.0010\n",
      "[53:  10]: eps=0.00000000 Loss=45.1109 Time=0.0015\n",
      "[53:  20]: eps=0.00000000 Loss=43.6506 Time=0.0022\n",
      "[53:  30]: eps=0.00000000 Loss=42.9576 Time=0.0023\n",
      "[53:  40]: eps=0.00000000 Loss=43.5857 Time=0.0022\n",
      "[53:  46]: eps=0.00000000 Loss=43.5764 Time=0.0021\n",
      "Epoch time: 0.1157, Total time: 5.8581\n",
      "Evaluating...\n",
      "[53:  11]: eps=0.00000000 Loss=42.4195 Time=0.0009\n",
      "Epoch 54, learning rate [0.001]\n",
      "[54:   0]: eps=0.00000000 Loss=44.5309 Time=0.0020\n",
      "[54:  10]: eps=0.00000000 Loss=43.8893 Time=0.0020\n",
      "[54:  20]: eps=0.00000000 Loss=42.3580 Time=0.0019\n",
      "[54:  30]: eps=0.00000000 Loss=42.5256 Time=0.0019\n",
      "[54:  40]: eps=0.00000000 Loss=42.9777 Time=0.0020\n",
      "[54:  46]: eps=0.00000000 Loss=43.4687 Time=0.0021\n",
      "Epoch time: 0.1110, Total time: 5.9691\n",
      "Evaluating...\n",
      "[54:  11]: eps=0.00000000 Loss=42.3242 Time=0.0019\n",
      "Epoch 55, learning rate [0.001]\n",
      "[55:   0]: eps=0.00000000 Loss=33.6399 Time=0.0030\n",
      "[55:  10]: eps=0.00000000 Loss=42.7008 Time=0.0026\n",
      "[55:  20]: eps=0.00000000 Loss=43.8986 Time=0.0021\n",
      "[55:  30]: eps=0.00000000 Loss=44.0577 Time=0.0019\n",
      "[55:  40]: eps=0.00000000 Loss=43.6333 Time=0.0019\n",
      "[55:  46]: eps=0.00000000 Loss=43.4833 Time=0.0021\n",
      "Epoch time: 0.1100, Total time: 6.0791\n",
      "Evaluating...\n",
      "[55:  11]: eps=0.00000000 Loss=42.5867 Time=0.0015\n",
      "Epoch 56, learning rate [0.001]\n",
      "[56:   0]: eps=0.00000000 Loss=48.6886 Time=0.0010\n",
      "[56:  10]: eps=0.00000000 Loss=43.4783 Time=0.0020\n",
      "[56:  20]: eps=0.00000000 Loss=43.9093 Time=0.0019\n",
      "[56:  30]: eps=0.00000000 Loss=43.3578 Time=0.0018\n",
      "[56:  40]: eps=0.00000000 Loss=43.5993 Time=0.0018\n",
      "[56:  46]: eps=0.00000000 Loss=43.3981 Time=0.0018\n",
      "Epoch time: 0.0931, Total time: 6.1721\n",
      "Evaluating...\n",
      "[56:  11]: eps=0.00000000 Loss=42.3328 Time=0.0020\n",
      "Epoch 57, learning rate [0.001]\n",
      "[57:   0]: eps=0.00000000 Loss=39.2508 Time=0.0030\n",
      "[57:  10]: eps=0.00000000 Loss=44.0717 Time=0.0032\n",
      "[57:  20]: eps=0.00000000 Loss=44.9360 Time=0.0030\n",
      "[57:  30]: eps=0.00000000 Loss=44.9908 Time=0.0027\n",
      "[57:  40]: eps=0.00000000 Loss=43.9224 Time=0.0025\n",
      "[57:  46]: eps=0.00000000 Loss=43.4086 Time=0.0026\n",
      "Epoch time: 0.1368, Total time: 6.3089\n",
      "Evaluating...\n",
      "[57:  11]: eps=0.00000000 Loss=42.2628 Time=0.0008\n",
      "Epoch 58, learning rate [0.001]\n",
      "[58:   0]: eps=0.00000000 Loss=39.3647 Time=0.0021\n",
      "[58:  10]: eps=0.00000000 Loss=44.1886 Time=0.0017\n",
      "[58:  20]: eps=0.00000000 Loss=43.1571 Time=0.0018\n",
      "[58:  30]: eps=0.00000000 Loss=42.3537 Time=0.0017\n",
      "[58:  40]: eps=0.00000000 Loss=43.0167 Time=0.0017\n",
      "[58:  46]: eps=0.00000000 Loss=43.3979 Time=0.0017\n",
      "Epoch time: 0.0880, Total time: 6.3969\n",
      "Evaluating...\n",
      "[58:  11]: eps=0.00000000 Loss=42.6789 Time=0.0008\n",
      "Epoch 59, learning rate [0.001]\n",
      "[59:   0]: eps=0.00000000 Loss=45.4537 Time=0.0026\n",
      "[59:  10]: eps=0.00000000 Loss=42.1367 Time=0.0016\n",
      "[59:  20]: eps=0.00000000 Loss=43.9989 Time=0.0018\n",
      "[59:  30]: eps=0.00000000 Loss=44.7959 Time=0.0018\n",
      "[59:  40]: eps=0.00000000 Loss=44.0794 Time=0.0018\n",
      "[59:  46]: eps=0.00000000 Loss=43.3893 Time=0.0018\n",
      "Epoch time: 0.0955, Total time: 6.4925\n",
      "Evaluating...\n",
      "[59:  11]: eps=0.00000000 Loss=43.0646 Time=0.0018\n",
      "Epoch 60, learning rate [0.001]\n",
      "[60:   0]: eps=0.00000000 Loss=48.7528 Time=0.0030\n",
      "[60:  10]: eps=0.00000000 Loss=42.8504 Time=0.0022\n",
      "[60:  20]: eps=0.00000000 Loss=43.4994 Time=0.0021\n",
      "[60:  30]: eps=0.00000000 Loss=43.1869 Time=0.0020\n",
      "[60:  40]: eps=0.00000000 Loss=42.9369 Time=0.0020\n",
      "[60:  46]: eps=0.00000000 Loss=43.3405 Time=0.0020\n",
      "Epoch time: 0.1090, Total time: 6.6014\n",
      "Evaluating...\n",
      "[60:  11]: eps=0.00000000 Loss=42.4277 Time=0.0018\n",
      "Epoch 61, learning rate [0.001]\n",
      "[61:   0]: eps=0.00000000 Loss=36.4417 Time=0.0020\n",
      "[61:  10]: eps=0.00000000 Loss=46.0510 Time=0.0027\n",
      "[61:  20]: eps=0.00000000 Loss=45.3157 Time=0.0026\n",
      "[61:  30]: eps=0.00000000 Loss=44.5558 Time=0.0027\n",
      "[61:  40]: eps=0.00000000 Loss=43.5424 Time=0.0027\n",
      "[61:  46]: eps=0.00000000 Loss=43.3755 Time=0.0025\n",
      "Epoch time: 0.1397, Total time: 6.7412\n",
      "Evaluating...\n",
      "[61:  11]: eps=0.00000000 Loss=42.6203 Time=0.0013\n",
      "Epoch 62, learning rate [0.001]\n",
      "[62:   0]: eps=0.00000000 Loss=51.3439 Time=0.0020\n",
      "[62:  10]: eps=0.00000000 Loss=44.5733 Time=0.0017\n",
      "[62:  20]: eps=0.00000000 Loss=43.2653 Time=0.0016\n",
      "[62:  30]: eps=0.00000000 Loss=43.1875 Time=0.0018\n",
      "[62:  40]: eps=0.00000000 Loss=43.9575 Time=0.0019\n",
      "[62:  46]: eps=0.00000000 Loss=43.3694 Time=0.0020\n",
      "Epoch time: 0.1073, Total time: 6.8484\n",
      "Evaluating...\n",
      "[62:  11]: eps=0.00000000 Loss=42.5367 Time=0.0014\n",
      "Epoch 63, learning rate [0.001]\n",
      "[63:   0]: eps=0.00000000 Loss=44.0001 Time=0.0020\n",
      "[63:  10]: eps=0.00000000 Loss=45.8525 Time=0.0018\n",
      "[63:  20]: eps=0.00000000 Loss=42.6927 Time=0.0021\n",
      "[63:  30]: eps=0.00000000 Loss=42.5937 Time=0.0020\n",
      "[63:  40]: eps=0.00000000 Loss=43.1227 Time=0.0019\n",
      "[63:  46]: eps=0.00000000 Loss=43.3310 Time=0.0019\n",
      "Epoch time: 0.0987, Total time: 6.9472\n",
      "Evaluating...\n",
      "[63:  11]: eps=0.00000000 Loss=42.7109 Time=0.0011\n",
      "Epoch 64, learning rate [0.001]\n",
      "[64:   0]: eps=0.00000000 Loss=48.2278 Time=0.0020\n",
      "[64:  10]: eps=0.00000000 Loss=44.1085 Time=0.0022\n",
      "[64:  20]: eps=0.00000000 Loss=41.5369 Time=0.0021\n",
      "[64:  30]: eps=0.00000000 Loss=41.9169 Time=0.0022\n",
      "[64:  40]: eps=0.00000000 Loss=42.9282 Time=0.0020\n",
      "[64:  46]: eps=0.00000000 Loss=43.3180 Time=0.0020\n",
      "Epoch time: 0.1073, Total time: 7.0544\n",
      "Evaluating...\n",
      "[64:  11]: eps=0.00000000 Loss=42.6221 Time=0.0013\n",
      "Epoch 65, learning rate [0.001]\n",
      "[65:   0]: eps=0.00000000 Loss=49.0531 Time=0.0020\n",
      "[65:  10]: eps=0.00000000 Loss=45.5416 Time=0.0018\n",
      "[65:  20]: eps=0.00000000 Loss=44.3617 Time=0.0018\n",
      "[65:  30]: eps=0.00000000 Loss=43.9986 Time=0.0017\n",
      "[65:  40]: eps=0.00000000 Loss=42.6889 Time=0.0017\n",
      "[65:  46]: eps=0.00000000 Loss=43.2743 Time=0.0018\n",
      "Epoch time: 0.0876, Total time: 7.1420\n",
      "Evaluating...\n",
      "[65:  11]: eps=0.00000000 Loss=42.5836 Time=0.0012\n",
      "Epoch 66, learning rate [0.001]\n",
      "[66:   0]: eps=0.00000000 Loss=42.7103 Time=0.0020\n",
      "[66:  10]: eps=0.00000000 Loss=44.6488 Time=0.0023\n",
      "[66:  20]: eps=0.00000000 Loss=43.0676 Time=0.0020\n",
      "[66:  30]: eps=0.00000000 Loss=43.7208 Time=0.0020\n",
      "[66:  40]: eps=0.00000000 Loss=43.2306 Time=0.0019\n",
      "[66:  46]: eps=0.00000000 Loss=43.1149 Time=0.0020\n",
      "Epoch time: 0.1061, Total time: 7.2482\n",
      "Evaluating...\n",
      "[66:  11]: eps=0.00000000 Loss=43.2082 Time=0.0012\n",
      "Epoch 67, learning rate [0.001]\n",
      "[67:   0]: eps=0.00000000 Loss=48.3778 Time=0.0011\n",
      "[67:  10]: eps=0.00000000 Loss=42.8821 Time=0.0018\n",
      "[67:  20]: eps=0.00000000 Loss=42.6722 Time=0.0019\n",
      "[67:  30]: eps=0.00000000 Loss=42.5235 Time=0.0019\n",
      "[67:  40]: eps=0.00000000 Loss=43.4241 Time=0.0018\n",
      "[67:  46]: eps=0.00000000 Loss=43.5624 Time=0.0018\n",
      "Epoch time: 0.0934, Total time: 7.3415\n",
      "Evaluating...\n",
      "[67:  11]: eps=0.00000000 Loss=43.0520 Time=0.0014\n",
      "Epoch 68, learning rate [0.001]\n",
      "[68:   0]: eps=0.00000000 Loss=54.9017 Time=0.0011\n",
      "[68:  10]: eps=0.00000000 Loss=43.5354 Time=0.0025\n",
      "[68:  20]: eps=0.00000000 Loss=42.6246 Time=0.0027\n",
      "[68:  30]: eps=0.00000000 Loss=42.2100 Time=0.0024\n",
      "[68:  40]: eps=0.00000000 Loss=43.0520 Time=0.0022\n",
      "[68:  46]: eps=0.00000000 Loss=43.3654 Time=0.0022\n",
      "Epoch time: 0.1147, Total time: 7.4562\n",
      "Evaluating...\n",
      "[68:  11]: eps=0.00000000 Loss=43.0192 Time=0.0011\n",
      "Epoch 69, learning rate [0.001]\n",
      "[69:   0]: eps=0.00000000 Loss=32.6921 Time=0.0020\n",
      "[69:  10]: eps=0.00000000 Loss=41.2930 Time=0.0017\n",
      "[69:  20]: eps=0.00000000 Loss=41.6866 Time=0.0018\n",
      "[69:  30]: eps=0.00000000 Loss=41.9564 Time=0.0018\n",
      "[69:  40]: eps=0.00000000 Loss=42.7470 Time=0.0018\n",
      "[69:  46]: eps=0.00000000 Loss=43.2179 Time=0.0018\n",
      "Epoch time: 0.0923, Total time: 7.5485\n",
      "Evaluating...\n",
      "[69:  11]: eps=0.00000000 Loss=42.9567 Time=0.0014\n",
      "Epoch 70, learning rate [0.001]\n",
      "[70:   0]: eps=0.00000000 Loss=36.8896 Time=0.0010\n",
      "[70:  10]: eps=0.00000000 Loss=43.8541 Time=0.0020\n",
      "[70:  20]: eps=0.00000000 Loss=44.2131 Time=0.0019\n",
      "[70:  30]: eps=0.00000000 Loss=43.3216 Time=0.0018\n",
      "[70:  40]: eps=0.00000000 Loss=43.5049 Time=0.0019\n",
      "[70:  46]: eps=0.00000000 Loss=43.3612 Time=0.0018\n",
      "Epoch time: 0.0929, Total time: 7.6414\n",
      "Evaluating...\n",
      "[70:  11]: eps=0.00000000 Loss=42.7935 Time=0.0013\n",
      "Epoch 71, learning rate [0.001]\n",
      "[71:   0]: eps=0.00000000 Loss=45.1189 Time=0.0010\n",
      "[71:  10]: eps=0.00000000 Loss=41.7889 Time=0.0023\n",
      "[71:  20]: eps=0.00000000 Loss=42.2438 Time=0.0024\n",
      "[71:  30]: eps=0.00000000 Loss=42.1479 Time=0.0027\n",
      "[71:  40]: eps=0.00000000 Loss=43.3313 Time=0.0026\n",
      "[71:  46]: eps=0.00000000 Loss=43.1575 Time=0.0025\n",
      "Epoch time: 0.1387, Total time: 7.7801\n",
      "Evaluating...\n",
      "[71:  11]: eps=0.00000000 Loss=42.8047 Time=0.0014\n",
      "Epoch 72, learning rate [0.001]\n",
      "[72:   0]: eps=0.00000000 Loss=45.4445 Time=0.0019\n",
      "[72:  10]: eps=0.00000000 Loss=42.9257 Time=0.0019\n",
      "[72:  20]: eps=0.00000000 Loss=41.4027 Time=0.0019\n",
      "[72:  30]: eps=0.00000000 Loss=41.5635 Time=0.0019\n",
      "[72:  40]: eps=0.00000000 Loss=42.5837 Time=0.0018\n",
      "[72:  46]: eps=0.00000000 Loss=43.1455 Time=0.0018\n",
      "Epoch time: 0.0916, Total time: 7.8717\n",
      "Evaluating...\n",
      "[72:  11]: eps=0.00000000 Loss=42.6594 Time=0.0012\n",
      "Epoch 73, learning rate [0.001]\n",
      "[73:   0]: eps=0.00000000 Loss=41.1387 Time=0.0009\n",
      "[73:  10]: eps=0.00000000 Loss=43.0238 Time=0.0015\n",
      "[73:  20]: eps=0.00000000 Loss=44.0675 Time=0.0015\n",
      "[73:  30]: eps=0.00000000 Loss=43.7978 Time=0.0016\n",
      "[73:  40]: eps=0.00000000 Loss=43.0235 Time=0.0021\n",
      "[73:  46]: eps=0.00000000 Loss=43.1834 Time=0.0020\n",
      "Epoch time: 0.1103, Total time: 7.9819\n",
      "Evaluating...\n",
      "[73:  11]: eps=0.00000000 Loss=43.2260 Time=0.0010\n",
      "Epoch 74, learning rate [0.001]\n",
      "[74:   0]: eps=0.00000000 Loss=44.7197 Time=0.0020\n",
      "[74:  10]: eps=0.00000000 Loss=42.2995 Time=0.0016\n",
      "[74:  20]: eps=0.00000000 Loss=42.5789 Time=0.0017\n",
      "[74:  30]: eps=0.00000000 Loss=42.9319 Time=0.0017\n",
      "[74:  40]: eps=0.00000000 Loss=42.9890 Time=0.0018\n",
      "[74:  46]: eps=0.00000000 Loss=43.1582 Time=0.0018\n",
      "Epoch time: 0.0954, Total time: 8.0773\n",
      "Evaluating...\n",
      "[74:  11]: eps=0.00000000 Loss=42.8592 Time=0.0014\n",
      "Epoch 75, learning rate [0.001]\n",
      "[75:   0]: eps=0.00000000 Loss=41.4145 Time=0.0024\n",
      "[75:  10]: eps=0.00000000 Loss=44.6354 Time=0.0031\n",
      "[75:  20]: eps=0.00000000 Loss=43.5358 Time=0.0027\n",
      "[75:  30]: eps=0.00000000 Loss=43.8492 Time=0.0029\n",
      "[75:  40]: eps=0.00000000 Loss=43.2211 Time=0.0030\n",
      "[75:  46]: eps=0.00000000 Loss=43.2016 Time=0.0029\n",
      "Epoch time: 0.1590, Total time: 8.2363\n",
      "Evaluating...\n",
      "[75:  11]: eps=0.00000000 Loss=42.8268 Time=0.0010\n",
      "Epoch 76, learning rate [0.001]\n",
      "[76:   0]: eps=0.00000000 Loss=45.8640 Time=0.0015\n",
      "[76:  10]: eps=0.00000000 Loss=45.5329 Time=0.0023\n",
      "[76:  20]: eps=0.00000000 Loss=43.6801 Time=0.0021\n",
      "[76:  30]: eps=0.00000000 Loss=42.5692 Time=0.0021\n",
      "[76:  40]: eps=0.00000000 Loss=42.8956 Time=0.0020\n",
      "[76:  46]: eps=0.00000000 Loss=43.0863 Time=0.0019\n",
      "Epoch time: 0.1018, Total time: 8.3381\n",
      "Evaluating...\n",
      "[76:  11]: eps=0.00000000 Loss=43.1181 Time=0.0010\n",
      "Epoch 77, learning rate [0.001]\n",
      "[77:   0]: eps=0.00000000 Loss=46.6604 Time=0.0020\n",
      "[77:  10]: eps=0.00000000 Loss=44.4959 Time=0.0023\n",
      "[77:  20]: eps=0.00000000 Loss=44.3819 Time=0.0030\n",
      "[77:  30]: eps=0.00000000 Loss=43.5244 Time=0.0031\n",
      "[77:  40]: eps=0.00000000 Loss=43.2815 Time=0.0032\n",
      "[77:  46]: eps=0.00000000 Loss=43.1221 Time=0.0033\n",
      "Epoch time: 0.1753, Total time: 8.5134\n",
      "Evaluating...\n",
      "[77:  11]: eps=0.00000000 Loss=42.8660 Time=0.0022\n",
      "Epoch 78, learning rate [0.001]\n",
      "[78:   0]: eps=0.00000000 Loss=35.2382 Time=0.0040\n",
      "[78:  10]: eps=0.00000000 Loss=42.0016 Time=0.0033\n",
      "[78:  20]: eps=0.00000000 Loss=43.0526 Time=0.0032\n",
      "[78:  30]: eps=0.00000000 Loss=42.8301 Time=0.0027\n",
      "[78:  40]: eps=0.00000000 Loss=42.8762 Time=0.0025\n",
      "[78:  46]: eps=0.00000000 Loss=43.1172 Time=0.0024\n",
      "Epoch time: 0.1231, Total time: 8.6365\n",
      "Evaluating...\n",
      "[78:  11]: eps=0.00000000 Loss=42.9463 Time=0.0011\n",
      "Epoch 79, learning rate [0.001]\n",
      "[79:   0]: eps=0.00000000 Loss=42.0799 Time=0.0020\n",
      "[79:  10]: eps=0.00000000 Loss=44.4813 Time=0.0020\n",
      "[79:  20]: eps=0.00000000 Loss=42.4576 Time=0.0019\n",
      "[79:  30]: eps=0.00000000 Loss=42.4354 Time=0.0020\n",
      "[79:  40]: eps=0.00000000 Loss=43.0707 Time=0.0020\n",
      "[79:  46]: eps=0.00000000 Loss=43.0247 Time=0.0020\n",
      "Epoch time: 0.1040, Total time: 8.7405\n",
      "Evaluating...\n",
      "[79:  11]: eps=0.00000000 Loss=43.0598 Time=0.0011\n",
      "Epoch 80, learning rate [0.001]\n",
      "[80:   0]: eps=0.00000000 Loss=36.9125 Time=0.0020\n",
      "[80:  10]: eps=0.00000000 Loss=43.5497 Time=0.0019\n",
      "[80:  20]: eps=0.00000000 Loss=43.9779 Time=0.0018\n",
      "[80:  30]: eps=0.00000000 Loss=42.4853 Time=0.0020\n",
      "[80:  40]: eps=0.00000000 Loss=43.3415 Time=0.0022\n",
      "[80:  46]: eps=0.00000000 Loss=43.0771 Time=0.0023\n",
      "Epoch time: 0.1227, Total time: 8.8633\n",
      "Evaluating...\n",
      "[80:  11]: eps=0.00000000 Loss=43.0560 Time=0.0020\n",
      "Epoch 81, learning rate [0.001]\n",
      "[81:   0]: eps=0.00000000 Loss=48.2950 Time=0.0030\n",
      "[81:  10]: eps=0.00000000 Loss=40.4418 Time=0.0032\n",
      "[81:  20]: eps=0.00000000 Loss=41.2346 Time=0.0027\n",
      "[81:  30]: eps=0.00000000 Loss=41.9740 Time=0.0025\n",
      "[81:  40]: eps=0.00000000 Loss=42.7442 Time=0.0023\n",
      "[81:  46]: eps=0.00000000 Loss=43.0601 Time=0.0023\n",
      "Epoch time: 0.1186, Total time: 8.9819\n",
      "Evaluating...\n",
      "[81:  11]: eps=0.00000000 Loss=43.5079 Time=0.0016\n",
      "Epoch 82, learning rate [0.001]\n",
      "[82:   0]: eps=0.00000000 Loss=54.1271 Time=0.0019\n",
      "[82:  10]: eps=0.00000000 Loss=44.6006 Time=0.0022\n",
      "[82:  20]: eps=0.00000000 Loss=44.1238 Time=0.0021\n",
      "[82:  30]: eps=0.00000000 Loss=43.3598 Time=0.0021\n",
      "[82:  40]: eps=0.00000000 Loss=43.2600 Time=0.0021\n",
      "[82:  46]: eps=0.00000000 Loss=43.0363 Time=0.0021\n",
      "Epoch time: 0.1076, Total time: 9.0894\n",
      "Evaluating...\n",
      "[82:  11]: eps=0.00000000 Loss=43.0005 Time=0.0011\n",
      "Epoch 83, learning rate [0.001]\n",
      "[83:   0]: eps=0.00000000 Loss=53.9422 Time=0.0025\n",
      "[83:  10]: eps=0.00000000 Loss=44.7323 Time=0.0019\n",
      "[83:  20]: eps=0.00000000 Loss=42.5798 Time=0.0022\n",
      "[83:  30]: eps=0.00000000 Loss=42.7476 Time=0.0027\n",
      "[83:  40]: eps=0.00000000 Loss=43.0766 Time=0.0028\n",
      "[83:  46]: eps=0.00000000 Loss=42.9149 Time=0.0028\n",
      "Epoch time: 0.1464, Total time: 9.2358\n",
      "Evaluating...\n",
      "[83:  11]: eps=0.00000000 Loss=43.3356 Time=0.0014\n",
      "Epoch 84, learning rate [0.001]\n",
      "[84:   0]: eps=0.00000000 Loss=56.3293 Time=0.0017\n",
      "[84:  10]: eps=0.00000000 Loss=46.7973 Time=0.0020\n",
      "[84:  20]: eps=0.00000000 Loss=43.4678 Time=0.0027\n",
      "[84:  30]: eps=0.00000000 Loss=42.3690 Time=0.0028\n",
      "[84:  40]: eps=0.00000000 Loss=43.1050 Time=0.0027\n",
      "[84:  46]: eps=0.00000000 Loss=43.0671 Time=0.0027\n",
      "Epoch time: 0.1467, Total time: 9.3825\n",
      "Evaluating...\n",
      "[84:  11]: eps=0.00000000 Loss=43.8369 Time=0.0020\n",
      "Epoch 85, learning rate [0.001]\n",
      "[85:   0]: eps=0.00000000 Loss=40.9974 Time=0.0030\n",
      "[85:  10]: eps=0.00000000 Loss=46.4309 Time=0.0031\n",
      "[85:  20]: eps=0.00000000 Loss=45.0597 Time=0.0028\n",
      "[85:  30]: eps=0.00000000 Loss=42.9847 Time=0.0025\n",
      "[85:  40]: eps=0.00000000 Loss=43.2638 Time=0.0023\n",
      "[85:  46]: eps=0.00000000 Loss=42.9716 Time=0.0022\n",
      "Epoch time: 0.1179, Total time: 9.5005\n",
      "Evaluating...\n",
      "[85:  11]: eps=0.00000000 Loss=43.8444 Time=0.0011\n",
      "Epoch 86, learning rate [0.001]\n",
      "[86:   0]: eps=0.00000000 Loss=45.5061 Time=0.0020\n",
      "[86:  10]: eps=0.00000000 Loss=43.3066 Time=0.0017\n",
      "[86:  20]: eps=0.00000000 Loss=42.4208 Time=0.0016\n",
      "[86:  30]: eps=0.00000000 Loss=43.2324 Time=0.0016\n",
      "[86:  40]: eps=0.00000000 Loss=42.6316 Time=0.0017\n",
      "[86:  46]: eps=0.00000000 Loss=43.0272 Time=0.0017\n",
      "Epoch time: 0.0887, Total time: 9.5892\n",
      "Evaluating...\n",
      "[86:  11]: eps=0.00000000 Loss=43.3052 Time=0.0009\n",
      "Epoch 87, learning rate [0.001]\n",
      "[87:   0]: eps=0.00000000 Loss=43.0760 Time=0.0010\n",
      "[87:  10]: eps=0.00000000 Loss=41.3396 Time=0.0017\n",
      "[87:  20]: eps=0.00000000 Loss=43.0778 Time=0.0020\n",
      "[87:  30]: eps=0.00000000 Loss=43.3741 Time=0.0019\n",
      "[87:  40]: eps=0.00000000 Loss=42.9679 Time=0.0019\n",
      "[87:  46]: eps=0.00000000 Loss=42.9202 Time=0.0018\n",
      "Epoch time: 0.0926, Total time: 9.6818\n",
      "Evaluating...\n",
      "[87:  11]: eps=0.00000000 Loss=43.2944 Time=0.0012\n",
      "Epoch 88, learning rate [0.001]\n",
      "[88:   0]: eps=0.00000000 Loss=45.6038 Time=0.0020\n",
      "[88:  10]: eps=0.00000000 Loss=42.9962 Time=0.0017\n",
      "[88:  20]: eps=0.00000000 Loss=42.8806 Time=0.0019\n",
      "[88:  30]: eps=0.00000000 Loss=42.4443 Time=0.0019\n",
      "[88:  40]: eps=0.00000000 Loss=42.8440 Time=0.0018\n",
      "[88:  46]: eps=0.00000000 Loss=42.9360 Time=0.0020\n",
      "Epoch time: 0.1092, Total time: 9.7910\n",
      "Evaluating...\n",
      "[88:  11]: eps=0.00000000 Loss=43.7026 Time=0.0015\n",
      "Epoch 89, learning rate [0.001]\n",
      "[89:   0]: eps=0.00000000 Loss=53.4754 Time=0.0021\n",
      "[89:  10]: eps=0.00000000 Loss=44.9982 Time=0.0018\n",
      "[89:  20]: eps=0.00000000 Loss=43.1951 Time=0.0017\n",
      "[89:  30]: eps=0.00000000 Loss=42.7161 Time=0.0017\n",
      "[89:  40]: eps=0.00000000 Loss=42.9903 Time=0.0019\n",
      "[89:  46]: eps=0.00000000 Loss=42.8846 Time=0.0020\n",
      "Epoch time: 0.1036, Total time: 9.8946\n",
      "Evaluating...\n",
      "[89:  11]: eps=0.00000000 Loss=43.4663 Time=0.0014\n",
      "Epoch 90, learning rate [0.001]\n",
      "[90:   0]: eps=0.00000000 Loss=38.9453 Time=0.0030\n",
      "[90:  10]: eps=0.00000000 Loss=41.3469 Time=0.0019\n",
      "[90:  20]: eps=0.00000000 Loss=42.4133 Time=0.0019\n",
      "[90:  30]: eps=0.00000000 Loss=42.5131 Time=0.0020\n",
      "[90:  40]: eps=0.00000000 Loss=43.0447 Time=0.0020\n",
      "[90:  46]: eps=0.00000000 Loss=43.0260 Time=0.0020\n",
      "Epoch time: 0.1036, Total time: 9.9982\n",
      "Evaluating...\n",
      "[90:  11]: eps=0.00000000 Loss=43.4432 Time=0.0011\n",
      "Epoch 91, learning rate [0.001]\n",
      "[91:   0]: eps=0.00000000 Loss=42.9370 Time=0.0037\n",
      "[91:  10]: eps=0.00000000 Loss=41.7580 Time=0.0019\n",
      "[91:  20]: eps=0.00000000 Loss=42.0300 Time=0.0019\n",
      "[91:  30]: eps=0.00000000 Loss=42.0321 Time=0.0018\n",
      "[91:  40]: eps=0.00000000 Loss=43.0144 Time=0.0017\n",
      "[91:  46]: eps=0.00000000 Loss=42.9537 Time=0.0018\n",
      "Epoch time: 0.0927, Total time: 10.0909\n",
      "Evaluating...\n",
      "[91:  11]: eps=0.00000000 Loss=43.4007 Time=0.0013\n",
      "Epoch 92, learning rate [0.001]\n",
      "[92:   0]: eps=0.00000000 Loss=33.7544 Time=0.0040\n",
      "[92:  10]: eps=0.00000000 Loss=42.1524 Time=0.0021\n",
      "[92:  20]: eps=0.00000000 Loss=42.3920 Time=0.0021\n",
      "[92:  30]: eps=0.00000000 Loss=42.8742 Time=0.0020\n",
      "[92:  40]: eps=0.00000000 Loss=42.9525 Time=0.0020\n",
      "[92:  46]: eps=0.00000000 Loss=42.9656 Time=0.0020\n",
      "Epoch time: 0.1047, Total time: 10.1956\n",
      "Evaluating...\n",
      "[92:  11]: eps=0.00000000 Loss=43.5309 Time=0.0009\n",
      "Epoch 93, learning rate [0.001]\n",
      "[93:   0]: eps=0.00000000 Loss=50.5817 Time=0.0020\n",
      "[93:  10]: eps=0.00000000 Loss=44.6691 Time=0.0020\n",
      "[93:  20]: eps=0.00000000 Loss=41.7051 Time=0.0018\n",
      "[93:  30]: eps=0.00000000 Loss=41.9624 Time=0.0020\n",
      "[93:  40]: eps=0.00000000 Loss=42.6574 Time=0.0023\n",
      "[93:  46]: eps=0.00000000 Loss=42.9083 Time=0.0024\n",
      "Epoch time: 0.1267, Total time: 10.3223\n",
      "Evaluating...\n",
      "[93:  11]: eps=0.00000000 Loss=43.1977 Time=0.0013\n",
      "Epoch 94, learning rate [0.001]\n",
      "[94:   0]: eps=0.00000000 Loss=32.3494 Time=0.0020\n",
      "[94:  10]: eps=0.00000000 Loss=40.4371 Time=0.0018\n",
      "[94:  20]: eps=0.00000000 Loss=41.7507 Time=0.0018\n",
      "[94:  30]: eps=0.00000000 Loss=42.7849 Time=0.0018\n",
      "[94:  40]: eps=0.00000000 Loss=42.7794 Time=0.0018\n",
      "[94:  46]: eps=0.00000000 Loss=42.7371 Time=0.0020\n",
      "Epoch time: 0.1026, Total time: 10.4249\n",
      "Evaluating...\n",
      "[94:  11]: eps=0.00000000 Loss=43.6824 Time=0.0020\n",
      "Epoch 95, learning rate [0.001]\n",
      "[95:   0]: eps=0.00000000 Loss=36.6433 Time=0.0019\n",
      "[95:  10]: eps=0.00000000 Loss=41.3175 Time=0.0019\n",
      "[95:  20]: eps=0.00000000 Loss=42.9548 Time=0.0019\n",
      "[95:  30]: eps=0.00000000 Loss=43.1985 Time=0.0020\n",
      "[95:  40]: eps=0.00000000 Loss=43.1099 Time=0.0019\n",
      "[95:  46]: eps=0.00000000 Loss=42.9216 Time=0.0019\n",
      "Epoch time: 0.1027, Total time: 10.5276\n",
      "Evaluating...\n",
      "[95:  11]: eps=0.00000000 Loss=43.2710 Time=0.0010\n",
      "Epoch 96, learning rate [0.001]\n",
      "[96:   0]: eps=0.00000000 Loss=42.4748 Time=0.0020\n",
      "[96:  10]: eps=0.00000000 Loss=42.6884 Time=0.0025\n",
      "[96:  20]: eps=0.00000000 Loss=41.9012 Time=0.0023\n",
      "[96:  30]: eps=0.00000000 Loss=42.5287 Time=0.0023\n",
      "[96:  40]: eps=0.00000000 Loss=42.7051 Time=0.0022\n",
      "[96:  46]: eps=0.00000000 Loss=42.8383 Time=0.0022\n",
      "Epoch time: 0.1159, Total time: 10.6434\n",
      "Evaluating...\n",
      "[96:  11]: eps=0.00000000 Loss=43.7413 Time=0.0012\n",
      "Epoch 97, learning rate [0.001]\n",
      "[97:   0]: eps=0.00000000 Loss=41.7089 Time=0.0010\n",
      "[97:  10]: eps=0.00000000 Loss=40.8522 Time=0.0020\n",
      "[97:  20]: eps=0.00000000 Loss=42.0988 Time=0.0022\n",
      "[97:  30]: eps=0.00000000 Loss=42.3111 Time=0.0023\n",
      "[97:  40]: eps=0.00000000 Loss=42.1454 Time=0.0022\n",
      "[97:  46]: eps=0.00000000 Loss=43.0039 Time=0.0022\n",
      "Epoch time: 0.1187, Total time: 10.7622\n",
      "Evaluating...\n",
      "[97:  11]: eps=0.00000000 Loss=43.3697 Time=0.0014\n",
      "Epoch 98, learning rate [0.001]\n",
      "[98:   0]: eps=0.00000000 Loss=36.1695 Time=0.0026\n",
      "[98:  10]: eps=0.00000000 Loss=40.7615 Time=0.0025\n",
      "[98:  20]: eps=0.00000000 Loss=43.2672 Time=0.0021\n",
      "[98:  30]: eps=0.00000000 Loss=43.5094 Time=0.0025\n",
      "[98:  40]: eps=0.00000000 Loss=43.5104 Time=0.0024\n",
      "[98:  46]: eps=0.00000000 Loss=43.1582 Time=0.0023\n",
      "Epoch time: 0.1181, Total time: 10.8802\n",
      "Evaluating...\n",
      "[98:  11]: eps=0.00000000 Loss=44.1956 Time=0.0012\n",
      "Epoch 99, learning rate [0.001]\n",
      "[99:   0]: eps=0.00000000 Loss=47.2368 Time=0.0020\n",
      "[99:  10]: eps=0.00000000 Loss=44.4974 Time=0.0018\n",
      "[99:  20]: eps=0.00000000 Loss=42.7884 Time=0.0019\n",
      "[99:  30]: eps=0.00000000 Loss=42.8227 Time=0.0021\n",
      "[99:  40]: eps=0.00000000 Loss=42.9873 Time=0.0020\n",
      "[99:  46]: eps=0.00000000 Loss=43.0115 Time=0.0020\n",
      "Epoch time: 0.1018, Total time: 10.9820\n",
      "Evaluating...\n",
      "[99:  11]: eps=0.00000000 Loss=43.5150 Time=0.0017\n",
      "Epoch 100, learning rate [0.001]\n",
      "[100:   0]: eps=0.00000000 Loss=38.9209 Time=0.0040\n",
      "[100:  10]: eps=0.00000204 Loss=39.4324 Time=0.0151 Robust_Loss=39.4835\n",
      "[100:  20]: eps=0.00003267 Loss=41.5663 Time=0.0130 Robust_Loss=41.6989\n",
      "[100:  30]: eps=0.00016538 Loss=42.5794 Time=0.0118 Robust_Loss=42.7033\n",
      "[100:  40]: eps=0.00052269 Loss=42.9730 Time=0.0112 Robust_Loss=43.0803\n",
      "[100:  46]: eps=0.00091419 Loss=42.8269 Time=0.0112 Robust_Loss=42.9221\n",
      "Epoch time: 0.5368, Total time: 11.5188\n",
      "Evaluating...\n",
      "[100:  11]: eps=0.00091419 Loss=44.1625 Robust_Loss=44.2158 Time=0.0062\n",
      "Epoch 101, learning rate [0.001]\n",
      "[101:   0]: eps=0.00099631 Loss=37.0971 Robust_Loss=37.1444 Time=0.0091\n",
      "[101:  10]: eps=0.00215528 Loss=45.1506 Robust_Loss=45.2290 Time=0.0094\n",
      "[101:  20]: eps=0.00411438 Loss=43.8040 Robust_Loss=43.9306 Time=0.0097\n",
      "[101:  30]: eps=0.00717741 Loss=44.4162 Robust_Loss=44.6342 Time=0.0098\n",
      "[101:  40]: eps=0.01169720 Loss=43.2903 Robust_Loss=43.6603 Time=0.0100\n",
      "[101:  46]: eps=0.01527344 Loss=43.1373 Robust_Loss=43.6187 Time=0.0105\n",
      "Epoch time: 0.5045, Total time: 12.0233\n",
      "Evaluating...\n",
      "[101:  11]: eps=0.01527344 Loss=44.9683 Robust_Loss=46.6367 Time=0.0078\n",
      "Epoch 102, learning rate [0.001]\n",
      "[102:   0]: eps=0.01594103 Loss=45.6919 Robust_Loss=47.3578 Time=0.0086\n",
      "[102:  10]: eps=0.02388572 Loss=45.6129 Robust_Loss=47.8656 Time=0.0100\n",
      "[102:  20]: eps=0.03448453 Loss=46.6175 Robust_Loss=50.4632 Time=0.0094\n",
      "[102:  30]: eps=0.04741661 Loss=47.7611 Robust_Loss=52.9217 Time=0.0095\n",
      "[102:  40]: eps=0.06049706 Loss=49.9667 Robust_Loss=56.8397 Time=0.0095\n",
      "[102:  46]: eps=0.06834532 Loss=51.1214 Robust_Loss=59.3795 Time=0.0098\n",
      "Epoch time: 0.4731, Total time: 12.4964\n",
      "Evaluating...\n",
      "[102:  11]: eps=0.06834532 Loss=64.6571 Robust_Loss=85.4401 Time=0.0059\n",
      "Epoch 103, learning rate [0.001]\n",
      "[103:   0]: eps=0.06965337 Loss=48.9301 Robust_Loss=66.3430 Time=0.0091\n",
      "[103:  10]: eps=0.08273381 Loss=65.3720 Robust_Loss=88.7961 Time=0.0088\n",
      "[103:  20]: eps=0.09581426 Loss=66.2866 Robust_Loss=89.2676 Time=0.0096\n",
      "[103:  30]: eps=0.10889470 Loss=67.3894 Robust_Loss=91.2041 Time=0.0092\n",
      "[103:  40]: eps=0.12197515 Loss=67.9788 Robust_Loss=92.3312 Time=0.0097\n",
      "[103:  46]: eps=0.12982341 Loss=68.8732 Robust_Loss=93.7042 Time=0.0097\n",
      "Epoch time: 0.4659, Total time: 12.9624\n",
      "Evaluating...\n",
      "[103:  11]: eps=0.12982341 Loss=76.8205 Robust_Loss=109.4742 Time=0.0074\n",
      "Epoch 104, learning rate [0.001]\n",
      "[104:   0]: eps=0.13113146 Loss=91.3632 Robust_Loss=133.4698 Time=0.0081\n",
      "[104:  10]: eps=0.14421190 Loss=75.6679 Robust_Loss=106.7351 Time=0.0101\n",
      "[104:  20]: eps=0.15729235 Loss=73.6468 Robust_Loss=103.9934 Time=0.0098\n",
      "[104:  30]: eps=0.17037279 Loss=75.0036 Robust_Loss=106.2995 Time=0.0095\n",
      "[104:  40]: eps=0.18345324 Loss=76.6226 Robust_Loss=108.1067 Time=0.0098\n",
      "[104:  46]: eps=0.19130150 Loss=75.8797 Robust_Loss=107.0120 Time=0.0098\n",
      "Epoch time: 0.4744, Total time: 13.4367\n",
      "Evaluating...\n",
      "[104:  11]: eps=0.19130150 Loss=74.3888 Robust_Loss=104.6574 Time=0.0061\n",
      "Epoch 105, learning rate [0.001]\n",
      "[105:   0]: eps=0.19260955 Loss=67.0458 Robust_Loss=92.8640 Time=0.0074\n",
      "[105:  10]: eps=0.20568999 Loss=73.2282 Robust_Loss=102.6122 Time=0.0093\n",
      "[105:  20]: eps=0.21877044 Loss=74.0336 Robust_Loss=102.9457 Time=0.0098\n",
      "[105:  30]: eps=0.23185088 Loss=73.1603 Robust_Loss=101.7917 Time=0.0100\n",
      "[105:  40]: eps=0.24493133 Loss=74.2770 Robust_Loss=103.2612 Time=0.0099\n",
      "[105:  46]: eps=0.25277959 Loss=74.2452 Robust_Loss=103.1332 Time=0.0098\n",
      "Epoch time: 0.4722, Total time: 13.9090\n",
      "Evaluating...\n",
      "[105:  11]: eps=0.25277959 Loss=73.9661 Robust_Loss=103.6837 Time=0.0060\n",
      "Epoch 106, learning rate [0.001]\n",
      "[106:   0]: eps=0.25408764 Loss=79.7261 Robust_Loss=114.0422 Time=0.0083\n",
      "[106:  10]: eps=0.26716808 Loss=72.7489 Robust_Loss=100.6418 Time=0.0086\n",
      "[106:  20]: eps=0.28024853 Loss=73.5946 Robust_Loss=102.4103 Time=0.0094\n",
      "[106:  30]: eps=0.29332897 Loss=74.6169 Robust_Loss=104.0161 Time=0.0099\n",
      "[106:  40]: eps=0.30640942 Loss=75.0780 Robust_Loss=104.9439 Time=0.0097\n",
      "[106:  46]: eps=0.31425768 Loss=75.4057 Robust_Loss=105.3048 Time=0.0095\n",
      "Epoch time: 0.4574, Total time: 14.3663\n",
      "Evaluating...\n",
      "[106:  11]: eps=0.31425768 Loss=75.1889 Robust_Loss=106.0481 Time=0.0057\n",
      "Epoch 107, learning rate [0.001]\n",
      "[107:   0]: eps=0.31556573 Loss=71.1364 Robust_Loss=101.0578 Time=0.0125\n",
      "[107:  10]: eps=0.32864617 Loss=74.4415 Robust_Loss=104.8909 Time=0.0093\n",
      "[107:  20]: eps=0.34172662 Loss=75.7375 Robust_Loss=105.9093 Time=0.0110\n",
      "[107:  30]: eps=0.35480706 Loss=76.3473 Robust_Loss=106.5449 Time=0.0109\n",
      "[107:  40]: eps=0.36788751 Loss=75.9303 Robust_Loss=106.0163 Time=0.0107\n",
      "[107:  46]: eps=0.37573578 Loss=75.8718 Robust_Loss=106.0321 Time=0.0106\n",
      "Epoch time: 0.5048, Total time: 14.8711\n",
      "Evaluating...\n",
      "[107:  11]: eps=0.37573578 Loss=75.4286 Robust_Loss=106.4929 Time=0.0057\n",
      "Epoch 108, learning rate [0.001]\n",
      "[108:   0]: eps=0.37704382 Loss=79.3844 Robust_Loss=111.1138 Time=0.0181\n",
      "[108:  10]: eps=0.39012426 Loss=76.2081 Robust_Loss=105.8853 Time=0.0106\n",
      "[108:  20]: eps=0.40320471 Loss=77.4540 Robust_Loss=108.3551 Time=0.0100\n",
      "[108:  30]: eps=0.41628515 Loss=77.0488 Robust_Loss=107.8467 Time=0.0100\n",
      "[108:  40]: eps=0.42936560 Loss=76.5946 Robust_Loss=107.6385 Time=0.0096\n",
      "[108:  46]: eps=0.43721387 Loss=76.9009 Robust_Loss=107.9549 Time=0.0095\n",
      "Epoch time: 0.4552, Total time: 15.3263\n",
      "Evaluating...\n",
      "[108:  11]: eps=0.43721387 Loss=76.3568 Robust_Loss=108.2340 Time=0.0069\n",
      "Epoch 109, learning rate [0.001]\n",
      "[109:   0]: eps=0.43852191 Loss=76.4739 Robust_Loss=109.0091 Time=0.0101\n",
      "[109:  10]: eps=0.45160235 Loss=76.2757 Robust_Loss=106.9682 Time=0.0125\n",
      "[109:  20]: eps=0.46468280 Loss=75.9760 Robust_Loss=106.8157 Time=0.0118\n",
      "[109:  30]: eps=0.47776324 Loss=76.0552 Robust_Loss=107.0864 Time=0.0112\n",
      "[109:  40]: eps=0.49084369 Loss=77.0359 Robust_Loss=108.3845 Time=0.0108\n",
      "[109:  46]: eps=0.49869196 Loss=77.3983 Robust_Loss=108.7804 Time=0.0105\n",
      "Epoch time: 0.5039, Total time: 15.8302\n",
      "Evaluating...\n",
      "[109:  11]: eps=0.49869196 Loss=75.7293 Robust_Loss=106.7920 Time=0.0052\n",
      "Epoch 110, learning rate [0.001]\n",
      "[110:   0]: eps=0.50000000 Loss=66.5652 Robust_Loss=96.3955 Time=0.0090\n",
      "[110:  10]: eps=0.50000000 Loss=75.1950 Robust_Loss=104.7374 Time=0.0096\n",
      "[110:  20]: eps=0.50000000 Loss=75.8352 Robust_Loss=105.5630 Time=0.0096\n",
      "[110:  30]: eps=0.50000000 Loss=75.8705 Robust_Loss=104.7412 Time=0.0097\n",
      "[110:  40]: eps=0.50000000 Loss=74.2167 Robust_Loss=102.3044 Time=0.0093\n",
      "[110:  46]: eps=0.50000000 Loss=73.9463 Robust_Loss=101.7665 Time=0.0097\n",
      "Epoch time: 0.4644, Total time: 16.2947\n",
      "Evaluating...\n",
      "[110:  11]: eps=0.50000000 Loss=70.6903 Robust_Loss=96.7164 Time=0.0067\n",
      "Epoch 111, learning rate [0.001]\n",
      "[111:   0]: eps=0.50000000 Loss=67.5729 Robust_Loss=92.3436 Time=0.0080\n",
      "[111:  10]: eps=0.50000000 Loss=72.9302 Robust_Loss=97.6720 Time=0.0103\n",
      "[111:  20]: eps=0.50000000 Loss=71.0641 Robust_Loss=95.7677 Time=0.0099\n",
      "[111:  30]: eps=0.50000000 Loss=70.8189 Robust_Loss=95.2318 Time=0.0106\n",
      "[111:  40]: eps=0.50000000 Loss=70.3573 Robust_Loss=94.4604 Time=0.0107\n",
      "[111:  46]: eps=0.50000000 Loss=70.0325 Robust_Loss=93.9264 Time=0.0105\n",
      "Epoch time: 0.5015, Total time: 16.7962\n",
      "Evaluating...\n",
      "[111:  11]: eps=0.50000000 Loss=67.0333 Robust_Loss=89.3487 Time=0.0058\n",
      "Epoch 112, learning rate [0.001]\n",
      "[112:   0]: eps=0.50000000 Loss=60.0214 Robust_Loss=83.6693 Time=0.0070\n",
      "[112:  10]: eps=0.50000000 Loss=67.8675 Robust_Loss=89.9704 Time=0.0084\n",
      "[112:  20]: eps=0.50000000 Loss=66.7952 Robust_Loss=88.0731 Time=0.0088\n",
      "[112:  30]: eps=0.50000000 Loss=66.9177 Robust_Loss=87.9216 Time=0.0089\n",
      "[112:  40]: eps=0.50000000 Loss=67.3450 Robust_Loss=88.3664 Time=0.0092\n",
      "[112:  46]: eps=0.50000000 Loss=67.0907 Robust_Loss=88.0290 Time=0.0096\n",
      "Epoch time: 0.4595, Total time: 17.2556\n",
      "Evaluating...\n",
      "[112:  11]: eps=0.50000000 Loss=64.9331 Robust_Loss=85.2431 Time=0.0060\n",
      "Epoch 113, learning rate [0.001]\n",
      "[113:   0]: eps=0.50000000 Loss=65.4167 Robust_Loss=86.2862 Time=0.0118\n",
      "[113:  10]: eps=0.50000000 Loss=67.1510 Robust_Loss=87.3420 Time=0.0125\n",
      "[113:  20]: eps=0.50000000 Loss=66.1810 Robust_Loss=85.6793 Time=0.0110\n",
      "[113:  30]: eps=0.50000000 Loss=65.0211 Robust_Loss=84.3434 Time=0.0109\n",
      "[113:  40]: eps=0.50000000 Loss=65.2807 Robust_Loss=84.3414 Time=0.0105\n",
      "[113:  46]: eps=0.50000000 Loss=64.9487 Robust_Loss=83.7627 Time=0.0103\n",
      "Epoch time: 0.4977, Total time: 17.7533\n",
      "Evaluating...\n",
      "[113:  11]: eps=0.50000000 Loss=62.3167 Robust_Loss=79.8918 Time=0.0090\n",
      "Epoch 114, learning rate [0.001]\n",
      "[114:   0]: eps=0.50000000 Loss=58.3048 Robust_Loss=77.0383 Time=0.0070\n",
      "[114:  10]: eps=0.50000000 Loss=62.9812 Robust_Loss=80.0352 Time=0.0094\n",
      "[114:  20]: eps=0.50000000 Loss=63.7969 Robust_Loss=81.0692 Time=0.0095\n",
      "[114:  30]: eps=0.50000000 Loss=63.0694 Robust_Loss=80.2867 Time=0.0096\n",
      "[114:  40]: eps=0.50000000 Loss=62.8973 Robust_Loss=79.8667 Time=0.0097\n",
      "[114:  46]: eps=0.50000000 Loss=62.9247 Robust_Loss=79.7576 Time=0.0106\n",
      "Epoch time: 0.5104, Total time: 18.2637\n",
      "Evaluating...\n",
      "[114:  11]: eps=0.50000000 Loss=60.6839 Robust_Loss=76.7593 Time=0.0072\n",
      "Epoch 115, learning rate [0.001]\n",
      "[115:   0]: eps=0.50000000 Loss=60.4361 Robust_Loss=75.4066 Time=0.0211\n",
      "[115:  10]: eps=0.50000000 Loss=62.7019 Robust_Loss=78.8770 Time=0.0143\n",
      "[115:  20]: eps=0.50000000 Loss=62.3019 Robust_Loss=78.2646 Time=0.0125\n",
      "[115:  30]: eps=0.50000000 Loss=61.5088 Robust_Loss=77.1676 Time=0.0122\n",
      "[115:  40]: eps=0.50000000 Loss=61.8667 Robust_Loss=77.3276 Time=0.0117\n",
      "[115:  46]: eps=0.50000000 Loss=61.4343 Robust_Loss=76.8014 Time=0.0113\n",
      "Epoch time: 0.5441, Total time: 18.8078\n",
      "Evaluating...\n",
      "[115:  11]: eps=0.50000000 Loss=59.2744 Robust_Loss=73.9768 Time=0.0057\n",
      "Epoch 116, learning rate [0.001]\n",
      "[116:   0]: eps=0.50000000 Loss=58.5271 Robust_Loss=73.2212 Time=0.0092\n",
      "[116:  10]: eps=0.50000000 Loss=61.3888 Robust_Loss=76.0794 Time=0.0117\n",
      "[116:  20]: eps=0.50000000 Loss=59.6508 Robust_Loss=74.0959 Time=0.0101\n",
      "[116:  30]: eps=0.50000000 Loss=60.5922 Robust_Loss=74.9164 Time=0.0097\n",
      "[116:  40]: eps=0.50000000 Loss=60.4826 Robust_Loss=74.7616 Time=0.0095\n",
      "[116:  46]: eps=0.50000000 Loss=60.2189 Robust_Loss=74.4125 Time=0.0097\n",
      "Epoch time: 0.4668, Total time: 19.2746\n",
      "Evaluating...\n",
      "[116:  11]: eps=0.50000000 Loss=58.2450 Robust_Loss=72.0403 Time=0.0063\n",
      "Epoch 117, learning rate [0.001]\n",
      "[117:   0]: eps=0.50000000 Loss=58.5395 Robust_Loss=73.3549 Time=0.0080\n",
      "[117:  10]: eps=0.50000000 Loss=58.9891 Robust_Loss=72.3865 Time=0.0103\n",
      "[117:  20]: eps=0.50000000 Loss=59.4846 Robust_Loss=72.7572 Time=0.0104\n",
      "[117:  30]: eps=0.50000000 Loss=58.7133 Robust_Loss=71.9174 Time=0.0105\n",
      "[117:  40]: eps=0.50000000 Loss=59.3117 Robust_Loss=72.6162 Time=0.0099\n",
      "[117:  46]: eps=0.50000000 Loss=59.1635 Robust_Loss=72.3663 Time=0.0098\n",
      "Epoch time: 0.4739, Total time: 19.7485\n",
      "Evaluating...\n",
      "[117:  11]: eps=0.50000000 Loss=57.0949 Robust_Loss=69.7622 Time=0.0074\n",
      "Epoch 118, learning rate [0.001]\n",
      "[118:   0]: eps=0.50000000 Loss=51.6335 Robust_Loss=64.1103 Time=0.0090\n",
      "[118:  10]: eps=0.50000000 Loss=55.8671 Robust_Loss=68.1173 Time=0.0090\n",
      "[118:  20]: eps=0.50000000 Loss=56.1953 Robust_Loss=68.6832 Time=0.0088\n",
      "[118:  30]: eps=0.50000000 Loss=57.5886 Robust_Loss=70.0965 Time=0.0094\n",
      "[118:  40]: eps=0.50000000 Loss=58.3829 Robust_Loss=70.8242 Time=0.0093\n",
      "[118:  46]: eps=0.50000000 Loss=58.3267 Robust_Loss=70.7023 Time=0.0094\n",
      "Epoch time: 0.4547, Total time: 20.2031\n",
      "Evaluating...\n",
      "[118:  11]: eps=0.50000000 Loss=56.4366 Robust_Loss=68.4689 Time=0.0067\n",
      "Epoch 119, learning rate [0.001]\n",
      "[119:   0]: eps=0.50000000 Loss=54.0531 Robust_Loss=65.8552 Time=0.0134\n",
      "[119:  10]: eps=0.50000000 Loss=59.7154 Robust_Loss=71.9814 Time=0.0119\n",
      "[119:  20]: eps=0.50000000 Loss=57.7829 Robust_Loss=69.5425 Time=0.0101\n",
      "[119:  30]: eps=0.50000000 Loss=57.2767 Robust_Loss=68.7751 Time=0.0114\n",
      "[119:  40]: eps=0.50000000 Loss=57.0984 Robust_Loss=68.6599 Time=0.0120\n",
      "[119:  46]: eps=0.50000000 Loss=57.3935 Robust_Loss=68.8775 Time=0.0129\n",
      "Epoch time: 0.6314, Total time: 20.8345\n",
      "Evaluating...\n",
      "[119:  11]: eps=0.50000000 Loss=55.5643 Robust_Loss=66.8023 Time=0.0083\n",
      "Epoch 120, learning rate [0.001]\n",
      "[120:   0]: eps=0.50000000 Loss=53.9966 Robust_Loss=64.8939 Time=0.0124\n",
      "[120:  10]: eps=0.50000000 Loss=57.5784 Robust_Loss=68.8756 Time=0.0093\n",
      "[120:  20]: eps=0.50000000 Loss=58.4959 Robust_Loss=69.4645 Time=0.0095\n",
      "[120:  30]: eps=0.50000000 Loss=57.7782 Robust_Loss=68.6523 Time=0.0097\n",
      "[120:  40]: eps=0.50000000 Loss=57.6793 Robust_Loss=68.5715 Time=0.0108\n",
      "[120:  46]: eps=0.50000000 Loss=56.7378 Robust_Loss=67.5980 Time=0.0107\n",
      "Epoch time: 0.5198, Total time: 21.3543\n",
      "Evaluating...\n",
      "[120:  11]: eps=0.50000000 Loss=55.4082 Robust_Loss=66.5594 Time=0.0092\n",
      "Epoch 121, learning rate [0.001]\n",
      "[121:   0]: eps=0.50000000 Loss=60.3604 Robust_Loss=72.5876 Time=0.0100\n",
      "[121:  10]: eps=0.50000000 Loss=56.1617 Robust_Loss=67.3679 Time=0.0106\n",
      "[121:  20]: eps=0.50000000 Loss=55.7243 Robust_Loss=66.7202 Time=0.0095\n",
      "[121:  30]: eps=0.50000000 Loss=56.0812 Robust_Loss=67.0448 Time=0.0091\n",
      "[121:  40]: eps=0.50000000 Loss=56.9646 Robust_Loss=67.9557 Time=0.0092\n",
      "[121:  46]: eps=0.50000000 Loss=56.7338 Robust_Loss=67.6458 Time=0.0096\n",
      "Epoch time: 0.4653, Total time: 21.8197\n",
      "Evaluating...\n",
      "[121:  11]: eps=0.50000000 Loss=54.5148 Robust_Loss=64.7062 Time=0.0054\n",
      "Epoch 122, learning rate [0.001]\n",
      "[122:   0]: eps=0.50000000 Loss=51.5841 Robust_Loss=61.7416 Time=0.0074\n",
      "[122:  10]: eps=0.50000000 Loss=57.0330 Robust_Loss=67.1836 Time=0.0086\n",
      "[122:  20]: eps=0.50000000 Loss=56.6778 Robust_Loss=66.8196 Time=0.0087\n",
      "[122:  30]: eps=0.50000000 Loss=56.0660 Robust_Loss=66.1583 Time=0.0092\n",
      "[122:  40]: eps=0.50000000 Loss=56.0451 Robust_Loss=66.1521 Time=0.0103\n",
      "[122:  46]: eps=0.50000000 Loss=55.9902 Robust_Loss=66.1384 Time=0.0108\n",
      "Epoch time: 0.5233, Total time: 22.3430\n",
      "Evaluating...\n",
      "[122:  11]: eps=0.50000000 Loss=54.1605 Robust_Loss=64.0118 Time=0.0066\n",
      "Epoch 123, learning rate [0.001]\n",
      "[123:   0]: eps=0.50000000 Loss=57.0663 Robust_Loss=68.1003 Time=0.0127\n",
      "[123:  10]: eps=0.50000000 Loss=55.4150 Robust_Loss=65.5476 Time=0.0120\n",
      "[123:  20]: eps=0.50000000 Loss=56.8780 Robust_Loss=66.9013 Time=0.0102\n",
      "[123:  30]: eps=0.50000000 Loss=55.6965 Robust_Loss=65.4210 Time=0.0100\n",
      "[123:  40]: eps=0.50000000 Loss=55.2457 Robust_Loss=64.9493 Time=0.0099\n",
      "[123:  46]: eps=0.50000000 Loss=55.4672 Robust_Loss=65.1280 Time=0.0097\n",
      "Epoch time: 0.4737, Total time: 22.8167\n",
      "Evaluating...\n",
      "[123:  11]: eps=0.50000000 Loss=53.7364 Robust_Loss=63.2052 Time=0.0057\n",
      "Epoch 124, learning rate [0.001]\n",
      "[124:   0]: eps=0.50000000 Loss=59.2109 Robust_Loss=68.2200 Time=0.0090\n",
      "[124:  10]: eps=0.50000000 Loss=56.0747 Robust_Loss=65.8315 Time=0.0101\n",
      "[124:  20]: eps=0.50000000 Loss=55.1026 Robust_Loss=64.6876 Time=0.0098\n",
      "[124:  30]: eps=0.50000000 Loss=55.0135 Robust_Loss=64.6287 Time=0.0105\n",
      "[124:  40]: eps=0.50000000 Loss=55.5681 Robust_Loss=65.2427 Time=0.0107\n",
      "[124:  46]: eps=0.50000000 Loss=55.5500 Robust_Loss=65.2212 Time=0.0103\n",
      "Epoch time: 0.4985, Total time: 23.3152\n",
      "Evaluating...\n",
      "[124:  11]: eps=0.50000000 Loss=53.9704 Robust_Loss=63.5606 Time=0.0054\n",
      "Epoch 125, learning rate [0.00025]\n",
      "[125:   0]: eps=0.50000000 Loss=46.3109 Robust_Loss=54.9091 Time=0.0078\n",
      "[125:  10]: eps=0.50000000 Loss=52.6014 Robust_Loss=61.7732 Time=0.0091\n",
      "[125:  20]: eps=0.50000000 Loss=54.4288 Robust_Loss=63.8346 Time=0.0107\n",
      "[125:  30]: eps=0.50000000 Loss=55.5443 Robust_Loss=65.1724 Time=0.0110\n",
      "[125:  40]: eps=0.50000000 Loss=55.5349 Robust_Loss=65.0421 Time=0.0109\n",
      "[125:  46]: eps=0.50000000 Loss=55.2836 Robust_Loss=64.8006 Time=0.0106\n",
      "Epoch time: 0.5131, Total time: 23.8283\n",
      "Evaluating...\n",
      "[125:  11]: eps=0.50000000 Loss=53.2722 Robust_Loss=62.3738 Time=0.0066\n",
      "Epoch 126, learning rate [0.0005]\n",
      "[126:   0]: eps=0.50000000 Loss=47.4181 Robust_Loss=56.9053 Time=0.0081\n",
      "[126:  10]: eps=0.50000000 Loss=55.3656 Robust_Loss=64.5643 Time=0.0097\n",
      "[126:  20]: eps=0.50000000 Loss=54.9112 Robust_Loss=64.0834 Time=0.0112\n",
      "[126:  30]: eps=0.50000000 Loss=54.4464 Robust_Loss=63.5851 Time=0.0106\n",
      "[126:  40]: eps=0.50000000 Loss=55.6675 Robust_Loss=64.9412 Time=0.0105\n",
      "[126:  46]: eps=0.50000000 Loss=54.9073 Robust_Loss=64.0709 Time=0.0104\n",
      "Epoch time: 0.5026, Total time: 24.3309\n",
      "Evaluating...\n",
      "[126:  11]: eps=0.50000000 Loss=53.1073 Robust_Loss=62.0456 Time=0.0060\n",
      "Epoch 127, learning rate [0.0005]\n",
      "[127:   0]: eps=0.50000000 Loss=53.7929 Robust_Loss=63.1041 Time=0.0078\n",
      "[127:  10]: eps=0.50000000 Loss=55.0886 Robust_Loss=64.0069 Time=0.0093\n",
      "[127:  20]: eps=0.50000000 Loss=54.3126 Robust_Loss=63.1573 Time=0.0089\n",
      "[127:  30]: eps=0.50000000 Loss=54.8198 Robust_Loss=63.6377 Time=0.0094\n",
      "[127:  40]: eps=0.50000000 Loss=54.4176 Robust_Loss=63.3034 Time=0.0103\n",
      "[127:  46]: eps=0.50000000 Loss=54.6069 Robust_Loss=63.4878 Time=0.0103\n",
      "Epoch time: 0.4983, Total time: 24.8292\n",
      "Evaluating...\n"
     ]
    }
   ],
   "source": [
    "train_robust(model_robust_wrap,dataloader_train,dataloader_test,method=\"robust\",args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_robust(model_fragile_wrap,dataloader_train,dataloader_test,method=\"natural\",args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,T_train,E_train = dataloader_train.dataset.tensors\n",
    "t = torch.linspace(0,T_train.max(),10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_exp = ExponentialFitter()\n",
    "clf_exp.fit(durations=T_train.ravel(),event_observed=E_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T20:21:57.396625Z",
     "start_time": "2023-11-07T20:21:56.638091Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "St_robust_x = clf_robust.survival_qdf(X_train,t).detach()\n",
    "St_fragile_x = clf_fragile.survival_qdf(X_train,t).detach()\n",
    "\n",
    "kmf = KaplanMeierFitter()\n",
    "kmf.fit(durations=T_train,event_observed=E_train)\n",
    "St_kmf  = kmf.predict(times=t.ravel().numpy())\n",
    "\n",
    "clf_exp = ExponentialFitter()\n",
    "clf_exp.fit(durations=T_train.ravel(),event_observed=E_train.ravel())\n",
    "St_exp = clf_exp.predict(times=t.ravel().numpy())\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(t,St_kmf)\n",
    "plt.plot(t,St_exp)\n",
    "plt.plot(t,St_fragile_x.mean(0))\n",
    "\n",
    "plt.plot(t,St_robust_x.mean(0))\n",
    "\n",
    "plt.ylabel(\"S(t)\"); plt.xlabel(\"Time\")\n",
    "plt.legend([\"Kaplan Meier Numerical\",f\"Exponential Fit $\\lambda$={np.round(1/clf_exp.params_[0],4)}\",\"Neural Network Normal\",\"Neural Network Robust\"])\n",
    "plt.title(\"Train Population Survival Curves\")\n",
    "plt.ylim([0,1.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_individual_lambda_histograms(clf_fragile,clf_robust,dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test = load_dataframe(ds_name=args.dataset,drop_first=True)\n",
    "from lifelines import WeibullAFTFitter\n",
    "clf_cph = WeibullAFTFitter()\n",
    "# clf_cph._scipy_fit_method = \"SLSQP\"\n",
    "clf_cph.fit(df=df_train,duration_col=\"time\",event_col=\"event\")\n",
    "kmf.plot()\n",
    "clf_cph.predict_survival_function(df_train).mean(1).plot(label=\"Weibull AFT\",figsize=(10,10))\n",
    "plt.legend()\n",
    "plt.ylim([0,1.05])\n",
    "plt.show()\n",
    "print(clf_cph.params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lifelines CPH Train CI: {:.3f}\".format(clf_cph.score(df_train,scoring_method=\"concordance_index\")))\n",
    "print(\"Lifelines CPH Test CI: {:.3f}\".format(clf_cph.score(df_test,scoring_method=\"concordance_index\")))\n",
    "\n",
    "# F_tr = 1-clf_exp.survival_function_at_times(times=T_train.ravel().numpy())\n",
    "# exp_ci_tr = concordance_index(event_times=T_train.ravel(),predicted_scores=F_tr,event_observed=E_train.ravel())\n",
    "# exp_ci_te = concordance_index(event_times=T_train.ravel(),predicted_scores=F_tr,event_observed=E_train.ravel())\n",
    "\n",
    "# print(\"Lifelines EXP Train CI: {:.3f}\".format(exp_ci_tr))\n",
    "# print(\"Lifelines EXP Test CI: {:.3f}\".format(exp_ci_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epsilons = [10,5,2,1,.8,0.7,.6,0.5,0.1,0.07,0.05,0]\n",
    "print(\"ROBUST CLF\")\n",
    "eps_,ci_ = concordance(clf_robust,dataloader_train,epsilons)\n",
    "plt.figure()\n",
    "plt.plot(eps_,ci_)\n",
    "print(\"NONROBUST CLF\")\n",
    "eps_,ci_ = concordance(clf_fragile,dataloader_train,epsilons)\n",
    "plt.plot(eps_,ci_)\n",
    "plt.legend([\"Robust\",\"Non Robust\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [10,5,2,1,.8,0.7,.6,0.5,0.1,0.07,0.05,0]\n",
    "print(\"ROBUST CLF\")\n",
    "eps_,ci_ = concordance(clf_robust,dataloader_test,epsilons)\n",
    "plt.figure()\n",
    "plt.plot(eps_,ci_)\n",
    "print(\"NONROBUST CLF\")\n",
    "eps_,ci_ = concordance(clf_fragile,dataloader_test,epsilons)\n",
    "plt.plot(eps_,ci_)\n",
    "plt.legend([\"Robust\",\"Non Robust\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [1,.8,0.7,.6,0.5,0.1,0.07,0.05]\n",
    "visualize_population_curves_attacked(clf_fragile,clf_robust,dataloader_train,epsilons=epsilons)\n",
    "visualize_population_curves_attacked(clf_fragile,clf_robust,dataloader_test,epsilons=epsilons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_individual_curves_changes(clf_robust,clf_fragile,dataloader_train,order=\"ascending\",test_cases=10)\n",
    "visualize_individual_curves_changes(clf_robust,clf_fragile,dataloader_train,order=\"descending\",test_cases=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_individual_curves_changes(clf_robust,clf_fragile,dataloader_train,order=\"ascending\",test_cases=10)\n",
    "visualize_individual_curves_changes(clf_robust,clf_fragile,dataloader_test,order=\"descending\",test_cases=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.3\n",
    "visualize_individual_curves_attacked(clf_robust,dataloader_train,epsilon=eps,order=\"descending\")\n",
    "visualize_individual_curves_attacked(clf_robust,dataloader_train,epsilon=eps,order=\"ascending\",test_cases=10)\n",
    "\n",
    "visualize_individual_curves_attacked(clf_robust,dataloader_test,epsilon=eps,order=\"descending\")\n",
    "visualize_individual_curves_attacked(clf_robust,dataloader_test,epsilon=eps,order=\"ascending\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "survival"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
